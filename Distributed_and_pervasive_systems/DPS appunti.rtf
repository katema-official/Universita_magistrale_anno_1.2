{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1040{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.22000}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\qc\f0\fs44\lang16 Distributed and Pervasive Systems - Appunti\par

\pard\sa200\sl276\slmult1\qj\fs22 Lezione 1 - 02/03/2022\par
Claudio Bettini. He worked for IBM in New York for one year. He also has been affiliated with idk University. Processes and Threads will return. Networking too (ISO-OSI stack, TCP-IP...).\par
Friday we have the "apply part" of the course, where we will work and follow what the tutors teach us. The goal is to experiment technologies related to the theory. We will develop and design a project of a distributed system with a pervasive part. \par
We will start at 8.50 (!!!). We will have a break in the middle. Let's motivate this course. Modern applications are Distributed Systems. All the applications that we run (or at least, a lot of them) like youtube, instagram, gmail... run as DS. They have an infrastructure organized as a DS. It would be good to know the concepts behind them. The course anyway is not only about Distributed Systems, because they will naturally include nodes (definition later). The nodes are phones, smartwatch, IOT devices, etc.., not just network and computers. Also, blockchain could not exist if it was not for an algorithm of DS. We will talk a whole morning of the blockchain. Anyawy, DS expertise is required in a lot of contexts.\par
Course objectives? Understanding the foundations of modern distributed systems. but we will not learn about about GRPC (in the lab yes though) or technologies in general: we will grasp the problems and the solutions, so that we can understand the future DS. So, foundations. Also, we will learn about transparency, synchronization, fault-tolerance consensus and blockchain, sensor data management and context-awareness. And also, we will be guided to design and program a distributed system. There is also a risk that someone copies our projects from Github and uses them. The project will be done singularly: the code must be self-made. the teacher doesn't want us to publish our project until february, when the last session of this academic year ends (the risk is that someone copies it, and it happened).\par
What will be talking about in this course?\par
First, what is a DS. Then, the architectures, like client-server and peer-to-peer systems and their organizations. Then, communication in DS. A DS is a collection of pcs that communicate together after all. This is the most interesting part of the theoretic part of the course: if we have different computers that need to communicate, how do we coordinate them to achieve a goal? An idea can be that the pcs share and idea of time, a clock, so that they can coordiante their actions. There are problems, of course, that we will study. We'll also see about mutual exclusion and election algorithms. In single programs, locks and semaphores are used. But in DS, we need communication. We'll see algorithms to do mutual exclusion in this way. The election algorithm instead tries to find a pc among all the ones that must be elected to act as a moderator.\par
The second part is about Pervasive Systems and their applications.\par
The third one is a guidance to project development. We will develop something that deals with, idk, election algorithms. We use Java, and concurrency & multi-threading. We'll learn about gRPC and MQTT. This last one is a technology used to hadle data coming from sensors.\par
TEXTBOOK: Distributed Systems: Concepts and Design, 5e, Coulouris, Dollimore, Kindberg & Blair, Addison-Wesley, 2012. Isbn-10: 0132143011. Occhi per\'f2 che the book does not cover everything.\par
Distributed Systems, third edition, version 3.03 (2020), Maarten van Steen and Andrew S. Tanenbaum.\par
Multiple choice questions + 2 open questions, one is usually an exercise about an algorithm idk. 50-50 theory part and project. Ricevimento on appointment.\par
\par
End of introduction. Tanenbaum's book will have a lot of things of this class. What is a Distributed System? \i A collection of independent computers that appears to its users as a single coherent systems\i0 . This means that the single units don't have multiple CPUs, or at least, they are not required to have more than one. The fact is that each PC has its own memory, there is no shared memory. They are phisically independent, but they should appear as a single entity. The user should not know how many computer there are. And how do we build such a system? Of course we need a network that connects them. Each PC has a local OS, that can be different from machine to machine, and each PC may run a certain application. A given application can run on multiple computers! How? Well, thanks to specific softwares, that use TCP-IP technology. Usually, we have to install a middleware, a Distributed System Layer, that hides some details from the layer: in particular, it hides the fact that the application is running on different PCs. For example:\par
imagine opening a folder in your PC that shows you contents that are not really on the machine that you are using. The resources are shared. Another thing that can happen is that, when i start a process, it really starts on another computer, even if it looks like it started on my local computer. Also in Multiplayer Online Games we can have different components distributed on different machines. And of course the WWW, even if this is not a precise example. But somehow, the WWW has distributed resources (it has different independent computers that are connected and work together). But why it is not exactly a Distributed System? Because, if i look at the website of the course (for example), i do because i know what i'm looking for. There is no location transparency: i know where my resource is locate, i have its IP address! But in a DS i don't need to know where it is.\par
An ironic definition of Leslie Lampor is: "\i you know you have one (DS) when the crash of a computer you've never heard of stops you from getting any work done\i0 ". This is a problem we'll have when developing, since debugging will be difficult. We will have to coordinate different machines, but we'll see some tricks to have an easier life.\par
What are the goeals of a distributed system, its properties? Well, we have some resources that we want to make accessible. And we would also like Distribution transparency. We have different levels of transparency. In ALL EXAMS there is a question of transparency. There are different kinds of transparency that we'd like DS to have. I.e, location trasparency is the ability of the DS to hide where a resource is located. Or, access transparency is the ability of the system to hide different data representation and how a resource is accessed. The component of a DS are independent computers, that can have different architectures, file systems, and so the way we access to the resources in those File Systems is different. A DS should have a layer that hides how the user accesses those datas. A network File System (NFS) is an abstraction layer that standardizes the access to the different FSs.\par
Migration: the user doesn't have to know that a resource is located in some PC. And doesn't need to know if that is moved from a PC to another. Relocation, instead, is moving a resource while it is used (is migration but more complex).\par
Oh, n.b.: not all DS have ALL those properties.\par
Replication transparency: the user should not be aware of the fact that a resource is replicated in different places of the world. I'd like to replicate a reource in roder to have fault tolerance and better performances (if i have a resource closer to me, i'll acces it faster). Concurrency transparency is that a resource may be shared by several competitive users, but this is none of their problem: for the users, the resource they accessed is just accessed by them. Failure transparency is that if something fails, the system should be able to recover by itself, and the users shouldn't note that a failure happened. For example, in an FSM, if a machine is trying to access a resource but it doesn't have that resource, that it asks to another machine to pass it. If this fails, it asks to someone else, and so on until a timeout or the resource if found.\par
A DS also needs to be open. But what does it mean? An open DS should offer Interoperability, Portability and Extensibility. Interoperate means being able to connect and work with another system. Portability means that an application can run on different DS that have minimal or no differences. Extensibility means it can be extended. How do we achieve opnenness? A general rule to guarantee this property is to not design our own protocol and keep them secret. if we do so, we'll be the only one to know how our program works. We should use standard protocols. We should also publish key interfaces with specific languages, like Java. But there are a lot of frameworks (like corba) that allows different processes to run on different computers and in different languages, but they can still communicate one another. So, we need standard interfaces, or API, to make different processes communicate one another. And last, we need testing and verifying the conformance of components to published standars.\par
The last property we'd like a DS to have is scalability: what is scalability? We want to be scalable in terms of what? One example is size scalability: if is increased the number of nodes/users, i have to scale the resources that support them. Can the system do that? Other scalabilities are the geographical one, that require the computers to stay "close" geographically speaking, and understanding if the administration is centralized or distributed. Here are some problems related to scalability, or implementation choices that can make scalability an issue:\par
-Centralized services: a single server for all users. If a single mahine handles all the users, and they grow too much, the machine fails. In this case, a load balancing technique in a DS context would have been better. We need replication and things like this.\par
-Centralized data: of example, a single on-line telephone book. Or, the old way to do the name resolution in internet (symbolic addresses to numeric addresses). The Domain Name Service is nowdays distributed, but it was not always like this. There was once a file on a machine that was queried when a resolution had to be done. This caused problems of cause.\par
-Centralized algorithms: example, doing routing based on complete information. Routing is decing in which way a message has to go. An algorithm that decides the whole path, it is not efficient. If each router handles local informations, btw, the load balance is better handled. In decentralized algorithms, we have that: No machine has complete information about the system state. So, a router knows the links to neighbor routers, no more. Machines make decisions based only on local information. Ok, this is self-explanatory. Failure of one machine does not ruin the algorithm. Of course, if a machine fails, the whole system still stays alive. If a "moderator" fails, the system should be able to elect someone else to moderate the system. Finally, there is non implicit assumtpion that a global clock exists. In a DS, the computers are not always perfectly syncronized. But we usually need perfect syncronization. Well, we'll see how to deal with this problem, and how the machines don't assume a perfecto shared clock. We'll see centralized and decentralized versions. \par
Imagine that a server has to handle the checking of forms as they are being filled, or a client that has to do the same thing. In this last case, i take computations away from the server.\par
Also the DNS nowdays uses a more scalable way to do name resolution. The DNS is organized in domains, zones, and it has multiple servers on the edge of the internet. The name resoultion is performend through communication and collaboration of different nodes spread in the world. This way of distributing the name resolution is a way to make the WWW scalable.\par
When dealing with DS, we should not keep some thing in mind. The following are false assumptions in DS:\par
-The network is reliable. False.\par
-The network is secure. Of course not.\par
-The network is homogeneous. No.\par
-The topology does not change. of course it does.\par
-Latency is zero.\par
-Bandwidth is infinite.\par
-Transport cost is zero.\par
-There is one administrator.\par
-Debugging distributed applications is analogous to standard applications.\par
Now we have an idea about what a Distributed System is, its properties and problems. Now, what kind of DS exist? Well, for example, Distributed computing systems and information systems and pervasive systems.\par
Let's focus on Distributed Computing Systems. What are clusters? A collection of similar servers closely connected by high-speed local-area network and usually running the same operating system. The idea was: having a high-performance single computer is difficult. What if we have hundreds or thousands of PCs connected together that simulate a high-performance computer? The goal was high performance and availability. There are two kind of clusters: symmetric and asymmetric.The symmetric is: all the nodes are the same. There is no master, all nodes have the same software installed, and run the same program. All of them run just one code, the same one, but it is executed in different processes. One of them is elected as coordinator anyway. This is kind of challenging, and has not been the most used type of cluster. In the asymmetric approach, we have a master node instead. We have a lot of compute node with their own local OS and a component of a parallel application. On the mster node, however, we have the local OS, a management application and parallel libraries (?). The case of Google Borg: Google designed this system in 2003, and it had a master, a certain number of compute nodes, and all of them communicated. The strange thing is that there were multiple BorgMasters. Why? For fault tolerance and performance reasons, in order to not have pitfalls or bottlenecks. The five masters elect one of them (the one that is doing the job), and that is the working one. the other fourt though follow the work of the first one (they clone the state). In this way, if the super-master fails, the other can take its place. So, each of them stores the state, but we need a way to ensure their state is the same. So we run the Paxos protocol that makes this possible. After 10 years of google running boh.\par
Nowdays we have Kubernetis, that Google made open source (?). It is widely used because it was donated by the Linus Foundation and everybody can use. Kubernetis is a platform for managing containerized computer applications. It can be seen as an evolution of Borg, and is an example of Asymmetric cluster. [Compito per casa: leggi articoli riguardo borg. Lui ha pullicato un articolo su Borg con le slides].\par
So, cluster computing is a kind of Distributed System. Cloud computing is another kind of DS, that allow users to access resources without knowing exactly where they are. In cloud, we have that the nodes are etherogeneous in hardware and operating systems (not the same as clusters i think). In this case, different cloud clusters can have different technologies, OS, different networks, and so on. The Cloud Service Models are Iaas, PaaS and SaaS.\par
IaaS: I just want this much of memory, computer power, and so on.\par
PaaS: I want more things. If the cloud has a distributed database, i'd like to use it. So i use some higher-level tools of the cloud. An example is Google App engine.\par
SaaS: google docs, Gmail, Youtube... all applications that we use as services.\par
There are also different deployment models, like private cloud (exclusive of a single organization comprising multiple consumers), Community Cloud, fuck. Other but i didn't read.\par
Edge or Fog Computing: introduced to take processing closer to where data is really produced. Like, we have a lot of IOT devices nowdays, and we want to process their datas. The important part is: behind all those systems, we have DSs. \par
Pervasive Computing: \i "the most profound technologies are those that disappear. They weave themselves into the fabric of everyday life until they are indistinnguishable from it"\i0 . The idea is to make everyday life things "intelligent" by putting a limited computationl power inside of them, and let them be part of a DS. If we do so, we can retrive and share a lot of datas. In general, a system is "smart" if it can adapt the way it behaves depending on the context. if the system can understand the context and change the behavior, it can be considered as smart. But doing so is not easy of course. \par
\par
\par
\par
\par
\par
}
 