{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1040{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.22000}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\qc\f0\fs44\lang16 Distributed and Pervasive Systems - Appunti\par

\pard\sa200\sl276\slmult1\qj\fs22 Lezione 1 - 02/03/2022\par
Claudio Bettini. He worked for IBM in New York for one year. He also has been affiliated with idk University. Processes and Threads will return. Networking too (ISO-OSI stack, TCP-IP...).\par
Friday we have the "apply part" of the course, where we will work and follow what the tutors teach us. The goal is to experiment technologies related to the theory. We will develop and design a project of a distributed system with a pervasive part. \par
We will start at 8.50 (!!!). We will have a break in the middle. Let's motivate this course. Modern applications are Distributed Systems. All the applications that we run (or at least, a lot of them) like youtube, instagram, gmail... run as DS. They have an infrastructure organized as a DS. It would be good to know the concepts behind them. The course anyway is not only about Distributed Systems, because they will naturally include nodes (definition later). The nodes are phones, smartwatch, IOT devices, etc.., not just network and computers. Also, blockchain could not exist if it was not for an algorithm of DS. We will talk a whole morning of the blockchain. Anyawy, DS expertise is required in a lot of contexts.\par
Course objectives? Understanding the foundations of modern distributed systems. but we will not learn about about GRPC (in the lab yes though) or technologies in general: we will grasp the problems and the solutions, so that we can understand the future DS. So, foundations. Also, we will learn about transparency, synchronization, fault-tolerance consensus and blockchain, sensor data management and context-awareness. And also, we will be guided to design and program a distributed system. There is also a risk that someone copies our projects from Github and uses them. The project will be done singularly: the code must be self-made. the teacher doesn't want us to publish our project until february, when the last session of this academic year ends (the risk is that someone copies it, and it happened).\par
What will be talking about in this course?\par
First, what is a DS. Then, the architectures, like client-server and peer-to-peer systems and their organizations. Then, communication in DS. A DS is a collection of pcs that communicate together after all. This is the most interesting part of the theoretic part of the course: if we have different computers that need to communicate, how do we coordinate them to achieve a goal? An idea can be that the pcs share and idea of time, a clock, so that they can coordiante their actions. There are problems, of course, that we will study. We'll also see about mutual exclusion and election algorithms. In single programs, locks and semaphores are used. But in DS, we need communication. We'll see algorithms to do mutual exclusion in this way. The election algorithm instead tries to find a pc among all the ones that must be elected to act as a moderator.\par
The second part is about Pervasive Systems and their applications.\par
The third one is a guidance to project development. We will develop something that deals with, idk, election algorithms. We use Java, and concurrency & multi-threading. We'll learn about gRPC and MQTT. This last one is a technology used to hadle data coming from sensors.\par
TEXTBOOK: Distributed Systems: Concepts and Design, 5e, Coulouris, Dollimore, Kindberg & Blair, Addison-Wesley, 2012. Isbn-10: 0132143011. Occhi per\'f2 che the book does not cover everything.\par
Distributed Systems, third edition, version 3.03 (2020), Maarten van Steen and Andrew S. Tanenbaum.\par
Multiple choice questions + 2 open questions, one is usually an exercise about an algorithm idk. 50-50 theory part and project. Ricevimento on appointment.\par
\par
End of introduction. Tanenbaum's book will have a lot of things of this class. What is a Distributed System? \i A collection of independent computers that appears to its users as a single coherent systems\i0 . This means that the single units don't have multiple CPUs, or at least, they are not required to have more than one. The fact is that each PC has its own memory, there is no shared memory. They are phisically independent, but they should appear as a single entity. The user should not know how many computer there are. And how do we build such a system? Of course we need a network that connects them. Each PC has a local OS, that can be different from machine to machine, and each PC may run a certain application. A given application can run on multiple computers! How? Well, thanks to specific softwares, that use TCP-IP technology. Usually, we have to install a middleware, a Distributed System Layer, that hides some details from the layer: in particular, it hides the fact that the application is running on different PCs. For example:\par
imagine opening a folder in your PC that shows you contents that are not really on the machine that you are using. The resources are shared. Another thing that can happen is that, when i start a process, it really starts on another computer, even if it looks like it started on my local computer. Also in Multiplayer Online Games we can have different components distributed on different machines. And of course the WWW, even if this is not a precise example. But somehow, the WWW has distributed resources (it has different independent computers that are connected and work together). But why it is not exactly a Distributed System? Because, if i look at the website of the course (for example), i do because i know what i'm looking for. There is no location transparency: i know where my resource is locate, i have its IP address! But in a DS i don't need to know where it is.\par
An ironic definition of Leslie Lampor is: "\i you know you have one (DS) when the crash of a computer you've never heard of stops you from getting any work done\i0 ". This is a problem we'll have when developing, since debugging will be difficult. We will have to coordinate different machines, but we'll see some tricks to have an easier life.\par
What are the goeals of a distributed system, its properties? Well, we have some resources that we want to make accessible. And we would also like Distribution transparency. We have different levels of transparency. In ALL EXAMS there is a question of transparency. There are different kinds of transparency that we'd like DS to have. I.e, location trasparency is the ability of the DS to hide where a resource is located. Or, access transparency is the ability of the system to hide different data representation and how a resource is accessed. The component of a DS are independent computers, that can have different architectures, file systems, and so the way we access to the resources in those File Systems is different. A DS should have a layer that hides how the user accesses those datas. A network File System (NFS) is an abstraction layer that standardizes the access to the different FSs.\par
Migration: the user doesn't have to know that a resource is located in some PC. And doesn't need to know if that is moved from a PC to another. Relocation, instead, is moving a resource while it is used (is migration but more complex).\par
Oh, n.b.: not all DS have ALL those properties.\par
Replication transparency: the user should not be aware of the fact that a resource is replicated in different places of the world. I'd like to replicate a reource in roder to have fault tolerance and better performances (if i have a resource closer to me, i'll acces it faster). Concurrency transparency is that a resource may be shared by several competitive users, but this is none of their problem: for the users, the resource they accessed is just accessed by them. Failure transparency is that if something fails, the system should be able to recover by itself, and the users shouldn't note that a failure happened. For example, in an FSM, if a machine is trying to access a resource but it doesn't have that resource, that it asks to another machine to pass it. If this fails, it asks to someone else, and so on until a timeout or the resource if found.\par
A DS also needs to be open. But what does it mean? An open DS should offer Interoperability, Portability and Extensibility. Interoperate means being able to connect and work with another system. Portability means that an application can run on different DS that have minimal or no differences. Extensibility means it can be extended. How do we achieve opnenness? A general rule to guarantee this property is to not design our own protocol and keep them secret. if we do so, we'll be the only one to know how our program works. We should use standard protocols. We should also publish key interfaces with specific languages, like Java. But there are a lot of frameworks (like corba) that allows different processes to run on different computers and in different languages, but they can still communicate one another. So, we need standard interfaces, or API, to make different processes communicate one another. And last, we need testing and verifying the conformance of components to published standars.\par
The last property we'd like a DS to have is scalability: what is scalability? We want to be scalable in terms of what? One example is size scalability: if is increased the number of nodes/users, i have to scale the resources that support them. Can the system do that? Other scalabilities are the geographical one, that require the computers to stay "close" geographically speaking, and understanding if the administration is centralized or distributed. Here are some problems related to scalability, or implementation choices that can make scalability an issue:\par
-Centralized services: a single server for all users. If a single mahine handles all the users, and they grow too much, the machine fails. In this case, a load balancing technique in a DS context would have been better. We need replication and things like this.\par
-Centralized data: of example, a single on-line telephone book. Or, the old way to do the name resolution in internet (symbolic addresses to numeric addresses). The Domain Name Service is nowdays distributed, but it was not always like this. There was once a file on a machine that was queried when a resolution had to be done. This caused problems of cause.\par
-Centralized algorithms: example, doing routing based on complete information. Routing is decing in which way a message has to go. An algorithm that decides the whole path, it is not efficient. If each router handles local informations, btw, the load balance is better handled. In decentralized algorithms, we have that: No machine has complete information about the system state. So, a router knows the links to neighbor routers, no more. Machines make decisions based only on local information. Ok, this is self-explanatory. Failure of one machine does not ruin the algorithm. Of course, if a machine fails, the whole system still stays alive. If a "moderator" fails, the system should be able to elect someone else to moderate the system. Finally, there is non implicit assumtpion that a global clock exists. In a DS, the computers are not always perfectly syncronized. But we usually need perfect syncronization. Well, we'll see how to deal with this problem, and how the machines don't assume a perfecto shared clock. We'll see centralized and decentralized versions. \par
Imagine that a server has to handle the checking of forms as they are being filled, or a client that has to do the same thing. In this last case, i take computations away from the server.\par
Also the DNS nowdays uses a more scalable way to do name resolution. The DNS is organized in domains, zones, and it has multiple servers on the edge of the internet. The name resoultion is performend through communication and collaboration of different nodes spread in the world. This way of distributing the name resolution is a way to make the WWW scalable.\par
When dealing with DS, we should not keep some thing in mind. The following are false assumptions in DS:\par
-The network is reliable. False.\par
-The network is secure. Of course not.\par
-The network is homogeneous. No.\par
-The topology does not change. of course it does.\par
-Latency is zero.\par
-Bandwidth is infinite.\par
-Transport cost is zero.\par
-There is one administrator.\par
-Debugging distributed applications is analogous to standard applications.\par
Now we have an idea about what a Distributed System is, its properties and problems. Now, what kind of DS exist? Well, for example, Distributed computing systems and information systems and pervasive systems.\par
Let's focus on Distributed Computing Systems. What are clusters? A collection of similar servers closely connected by high-speed local-area network and usually running the same operating system. The idea was: having a high-performance single computer is difficult. What if we have hundreds or thousands of PCs connected together that simulate a high-performance computer? The goal was high performance and availability. There are two kind of clusters: symmetric and asymmetric.The symmetric is: all the nodes are the same. There is no master, all nodes have the same software installed, and run the same program. All of them run just one code, the same one, but it is executed in different processes. One of them is elected as coordinator anyway. This is kind of challenging, and has not been the most used type of cluster. In the asymmetric approach, we have a master node instead. We have a lot of compute node with their own local OS and a component of a parallel application. On the mster node, however, we have the local OS, a management application and parallel libraries (?). The case of Google Borg: Google designed this system in 2003, and it had a master, a certain number of compute nodes, and all of them communicated. The strange thing is that there were multiple BorgMasters. Why? For fault tolerance and performance reasons, in order to not have pitfalls or bottlenecks. The five masters elect one of them (the one that is doing the job), and that is the working one. the other fourt though follow the work of the first one (they clone the state). In this way, if the super-master fails, the other can take its place. So, each of them stores the state, but we need a way to ensure their state is the same. So we run the Paxos protocol that makes this possible. After 10 years of google running boh.\par
Nowdays we have Kubernetis, that Google made open source (?). It is widely used because it was donated by the Linus Foundation and everybody can use. Kubernetis is a platform for managing containerized computer applications. It can be seen as an evolution of Borg, and is an example of Asymmetric cluster. [Compito per casa: leggi articoli riguardo borg. Lui ha pullicato un articolo su Borg con le slides].\par
So, cluster computing is a kind of Distributed System. Cloud computing is another kind of DS, that allow users to access resources without knowing exactly where they are. In cloud, we have that the nodes are etherogeneous in hardware and operating systems (not the same as clusters i think). In this case, different cloud clusters can have different technologies, OS, different networks, and so on. The Cloud Service Models are Iaas, PaaS and SaaS.\par
IaaS: I just want this much of memory, computer power, and so on.\par
PaaS: I want more things. If the cloud has a distributed database, i'd like to use it. So i use some higher-level tools of the cloud. An example is Google App engine.\par
SaaS: google docs, Gmail, Youtube... all applications that we use as services.\par
There are also different deployment models, like private cloud (exclusive of a single organization comprising multiple consumers), Community Cloud, fuck. Other but i didn't read.\par
Edge or Fog Computing: introduced to take processing closer to where data is really produced. Like, we have a lot of IOT devices nowdays, and we want to process their datas. The important part is: behind all those systems, we have DSs. \par
Pervasive Computing: \i "the most profound technologies are those that disappear. They weave themselves into the fabric of everyday life until they are indistinnguishable from it"\i0 . The idea is to make everyday life things "intelligent" by putting a limited computationl power inside of them, and let them be part of a DS. If we do so, we can retrive and share a lot of datas. In general, a system is "smart" if it can adapt the way it behaves depending on the context. if the system can understand the context and change the behavior, it can be considered as smart. But doing so is not easy of course. \par
\par
Lezione 2 - 09/03/2022\par
Distributed Systems Architecture. A DS architecture defines the main entities of the system (sensor nodes, processes, threads, object, components, services). It describes also the pattern of communication between those single components. It describes also how do they communicate and the role of the entities, for centralized and decentralized architectures and also hybrid once (server, client), and other things. Some architectures in clud computing are also absed on containers and microarchitectures.\par
In centralized architectures, we have client and servers architectures and the event bus architectures. In the client-server model, we have a central component, a server computer, that waits for requests from clients, and we can also have servers that act as clients. In this patter of invocation and service answered from a temporal perspective, we have that the client requests something to the server, it provides the service and replies to the client. There is also network delay, that is, the moment the client requests a service is different from the moment in which the server starts working on it. In this case, the client stops until the server has fully processed the request and answered to the client. There are many variants of the client-server model. We, as designers of the system, can make decisions that have an impact on scalability and other things. In a case, the client is very dumb, it just has a UI, and gets all the informations from the server. We can have also models in which the UI is completely client-side, the database ont server-side, but the application is split between the two. This is the case of a web-based application, where we could use javascript for the client side both for UI and the application. In other cases, the application is completely client-side and forwards SQL queries to the server! The server just has the database and is of course capable to fulfill the requests provided by the queries. The right-est extreme is that portion of the database is cached within the client. There are, in all this, a lot of caching systems. How does caching work in those c-s architectures? The web servers communicate with a proxy server, that is in the middle between the server and the client (closer to this one, so faster client-side). This is used to cache some requests in those proxies, so that future accesses are faster. In the DNS, it's a good idea to store some conversions even locally on the laptop. Another common way to organize software components in a client-server architecture is to not have only two tiers (client and server), but on more tiers, like three. A way is to divide the application logic in a server and the database (intended as data management) in another server. This is because, in this way, datas are divided from applications. In this way, application can change while the datas just increase, but can stay without problems. In those case, we are not only talking of different processes, but different hardwares. This is even an old architecture! In the VERTICAL DISTRIBUTION, we have a different server or node for each functionality of the system. in this case: data management to a node, application logic to another node and presentatio (UI) to another one. It's up to us to decide wheter it's better to distribute an application on more machines or viceversa. In fact, in HORIZONTAL DISTRIBUTION, the same functionality is distributed on multiple servers or nodes with load balancing. For example, we can have a server that accepts the requests and forwards them to other nodes. in this case, the same functionality is distributed on multiple nodes. This helps with fault tolerance, node balancing, etc.. the Round Robin technique can be used for the distribution of the tasks. The VERTICAL and HORIZONTAL distributions can also be combined. In those cases, each functionality is duplicated on a separate group of servers with load balancing. So, some servers are devoted to a specific functionality (vertical), but each functionality is divided in multiple machines (horizontal). So, multiple levels for different functionalities, but at each level we have multiple machines.\par
Let's talk a bit about the Microservice architecture. We can take to extreme the idea of separating functionalities (as in the vertical distribution). In this case, we run each component/functionality of the application logic in a separate process and possibly a different machine. The idea is: if i have a functionality that is independent from others, i'd like to isolate it in a single process or machine. This of course adds network overhead. But nowdays, with low latencies, it is possible to also follow this approach. microservices must be independently replaceable and upgradable, packaged with all they need to be deployed. Microservices communicate with lightweight mechanisms, like REST API or google RPC (invoking a function that is in another machine). Focused scalability: it means that the system MUST be scalable (a lot of users), and to achieve that i find out what's the functionality that is the most critical and i duplicate it, so that i don't have bottlenecks. And also, microservices can be written in different programming languages. [Java RMI is similar to RPC. The problem was that everything had to be written in Java anyway!]\par
A container is the abstraction at the application layer that packagesa code and dependencies together. Basically, the idea is: cool the microservices! But how are they run? To increase portability, we want to run those functionalities in an enviroment which has all that is needed to run a software. We wrap, in a \i container\i0 , the software that implements the functionality and all the libraries needed to run this piece of code. But where do I run this container? Well, the idea is similar to VMs. In a VM, we have a Guest OS that contains bins and libraries and one ore more applications. What's the different thing in containers? Well, we are at ahigher level. We don't care about OS. We have a basic, Host OS, and on top of it we have Docker. On top of docker, we have a container with the app and its bins and libraries. Docker can be thought ad a container engine. It does some optimizations: for examples, three instances of the same program (MySQL DB) can share the same libraries.\par
HE WILL NEVER ASK US ABOUT CONTAINERS (they are just a way to run microservices). But the idea of an architecture where each functioanlity is run in a single, separated environment, and the microservice concept, is important. This was the centralized architecture: we always have a server(s) that serve clients.\par
Another centralized architecture is the Event Bus Architecture. It works following a pattern called publish-subscribe. In this case, we have a certain number of nodes (subscriber), that are interested in getting some informations. And there is another number of nodes (publisher) that publish those informations. The informations, the "news", are published inside an Event Bus, and the subscriber takes them from the Event Bus. The publishers can be seen as source of events. This model is centralized because of the presence of a Broker in the middle, that is similar to a server. We can duplicate it oc.\par
\par
Now, decentralized model. Many years ago, distributed systems investigated a different way to distribute their nodes. In this architecture, we have different nodes that act both as clients and servers. All nodes have the same functional capabilities. Napster is a well-known ?2? system that started in 1999. It was proposed by two college students, but was not totally decentralized. It was used to exchange audio files. Some years ago, infact, it was shut down. The idea was: hey, friend, you have a file i'd like. Can you share it with me? To do this, a server was reached (this is why it was not fully decentralized). The server gives and index, that is a computer that has the file (it can be a list of indexes). At this point, the client requests the file to the given node (infact, the index is really an IP address and a port). The most important part of the protocol starts now: the computer that got the file adds its IP and port to the index server, to notice that now he also has that file. So, this was Napster.\par
Problems in developing those systems: how to place data objects across many hosts in order to achieve load balancing while accessing data and ensuring availability avoiding overheads. \par
Three generations of P2P systems (like in the book): we already talked about the first one, Napster, used for file sharing. Then, protocols were refined to improve scalability, fault tolerance and anonymity, and people started working on other systems with a similar decentralized architecture. Gnutella, for example, that said: let's design a system like napster but without servers. FreeNet said: we want to be anonymous while sharing those files. BitTorrent is kind of doing the same thing, but the idea, the key idea, is that a single file was downloaded from different nodes. Different part of a single file, called chunks, are asked to different nodes, and used to construct the whole file. The last generation is P2P middleware: can we design middleware that achive the objectives said before in a way that is abstract form the data that we are looking for/sharing?\par
So, once those systems reached a good degree of scalability, we hade P2P middleware. Their goals were to enable clients to transparently locate and communicate with any resource, add and remove resources and add and remove peers (those goals are a QUESTION IN THE EXAM!). So, the objectives are that we can add new partecipants to the system easily and delete nodes also easily. A way to optimize those operations is to use an overlay network, like internet. An overlay network is a logical network, or virtual network, in which we impose a pattern of communication on the nodes that don't correspond to the actual connections of the nodes. The optimization criterias are global scalability, load balancing, locality of interactions, etc.. So, we have an overlay network, like a ring composed of nodes connected as a ring.  If we have this kind of overlay, we can think about overlay network as another virtual network.\par
Ooook, so, the Routing Overlay is a distributed algorithm used to locate nodes and objects in an overlay network. Routing requests fro clients to hosts holding objects of interest at the application level instead of the network level IP, etc..\par
We can divide P2P systems in two families: structured and unstructured systems. Most of them will be using a structured P2P system, that is deterministically built in order to obtain efficient routing towards the node containing the required data. In the unstructured, the overlay network is built with randomized algorithms. That is, each peer only knows about its neighbors. Let's see structured overlays first: the idea is to have a structured overlay, like the ring overlay. In this way, we can also optimize the search in the system. For example, we have a certain number of bits for identifying a single node. If we have 16 nodes, we just need 4 bits. A function is used to associate an address to each node. Some nodes are called \i actual nodes\i0  and are the nodes that are present in the system. Also, some data keys are associated to each actual node, following a certain algorithm (this is the Chord structured overlay). (the resources have an address). Distributed Has Tabels are used for having a faster search. What are they and how do they work? The idea is: we have a data structure in each peer, that is a Hash Table or Finger Table. It's an extremely simple table in which in the first column we just have an index. The number of rows is the number of bits used for the address. In the other column, we have succ(p + 2^(i-1)). p represents the index of the current index, while i is the value of the first column. succ(x) means the next available node which index is higher than x. \par
This kind of structured overlay, the ring, and this algorithm, is just an approach. CAN, content addressable network, uses a different kind of separation of ndoes. CAN uses a bi-dimensional address space, where we have different points, and to each of them a certain space is associated. Each node manages all the points in tis region. In this case, the insertion is more complex: we have to divide a rectangle and give half of the resources to a node and other resources to another one.\par
Limitations of structured overlay? I mean, why somebody came up with the idea of unstructured overlay networks? Well, the finger table has to be mantained. Each time a node goes away from the system, we have to change the finger table This maintenance of complex overlay structures can be difficult and costly to achieve. If our target is a dynamic environment, we might not want distributed has tables, that are too costly to mantain. We'd like something that is resilient to fault tolerance and auto-organizes the nodes.\par
In unstructured overlays, the routing is based on randomizing algorithms. Each nodes stores a list of neighbots randomly built. That will be its "view" of the overlay. Also, resources are often randomly assigned to nodes. How is the search handled? It starts from a node and propagates according to local views. Also, the search is often limited to a number of hops or timeout, and the resource replication helps improving search success rate. The search must be limited because else I'd ask to a lot of peers for a file, and that is time consuming. That is why we replicate the resources. How do those randomized algorithms work? one of them is the gossiping algorithm (?): the nodes, periodically, exchange, with the nodes with which they are connected, some informations about other nodes. Basically, a peer communicates a fraction of its knowledge (its neighbor peers) to another peer, randomly (circa). But since this is a dynamic system, we can have that a node goes down at some point, and i cannot communicate with it. For this, the nodes will tend to share the last contacted nodes to other peers. \par
Structured: we are guaranteed to locate objects, if they exist, and provide  time and complexity bounds on this operation (hash table -> logarithmic time). Also, relative low message overhead. BUT. the manteinance of this structure is compex.\par
Unstructured: they self-organize, they are probabilistic and so cannot offer absolute guarantee on sh-\par
Hybrid architectures: for example Gnutella, that has some superpeers connected one another, and they are the reference for a P2P network of their own. \par
BitTorrent: another Hybrid architecture, in the sense that a part is centralized (when looking for a torrent file). In a .torrent file, we have a list of trackers. Inside of them, we find the IP address of ?. The point is that getting the address is centralized (like Napster), while the single nodes part is decentralized. [he won't ask exactly how BitTorrent works, but hybrid systems and hash tables yes].\par
\par
Lezione 4 - 16/03/2022\par
We'll go back to the chord algorithm. We were talking about structured and unstructured overlay network. Chord is an algorithm that maps any kind of resource to a node. How to quickly find resources? how to remove or add nodes? One idea that was used in chord but not only (also in other structured overlays) was to organize the overlay as a ring, so each node only knows about two neighbours. Each node had a Distributed Hash Table, called finger table. How many entries does each table has, at most? 160 bits (?), so each node has at most this number of nodes/addresses that he knows. The time that the algorithms takes to map a resource to a node is at maximum Olog(n), that in this case is almost constant. How does a new peer get an address in the space? Using a hashing function. [A way to get a unique name for a peer is to get the ID number that identifies the machine on the internet and the port that identifies the process in that machine.] Since we have 160 bits, the address space will never be filled (too large number), and so each node will handle multiple resources. The nodes are much less than the resources. A file with key k (obtained through the hashing funciton) is managed by a node, that is the first node (going along the ring clockwise) with id >= k, called succ(k). The calculus is done % 2^m, where m is the number of bits used. \line Chord provides a function LOOKUP(l) to efficiently find the address. Infact, this algorithm provides a middleware that gives us this function, used to know the address of the peer that will handle this resource. [Sta mettendo molta enfasi sul modulo, non so perch\'e9. Comunque id >= k ma tieni sempre presente il modulo].\line So, chord has this algorithm behind, but the programmer doesn't need to know all this stuff (but if he wants to program new algorithms, he has to know them oc). Now, imagine that we have the finger (hash) table of a certain node with id = p. Its number of entries is m, where m is the number of bits used for addressing all the possible peers in the ring. FT(i) is a function that we use to refer to the content of of the Finger Table, row i (FT(3) = 9 in the image). The value of the function, basically, is the address of a peer. But, as said, we need to know, to communicate with a peer, its IP and PORT. So, together with 9, we have an IP and a Port. The value of the function is calculated in a clever way: succ(p + 2^(i-1)). All the operations are % 2^m. succ(j) means: if j is not a node, take its successor node (the next available peer in the ring). [In tannenbauns there is the same example of the algorithm of Chord].\line The formula says, for resolving a key k: find FT(j) such that FT(j) <= k < FT(j+1). After that, the search is forwarded to the node FT(j). If there is no FT(j+1) in the table, then take the largest address number. \line We take FT(j) and not FT(j+1) because between those two there may be a lot of nodes. However, there is a case in which i can go directly to FT(j+1): when k = FT(j+1).\par
Benefits of having unstructured peer 2 peer network with superpeers? Keeping an index of data for a subnetwork and optimizations I didn't read.\par
Let's see some questions we might get in the final exam.\par
Saw them lol.\par
\par
COMMUNICATION MODELS IN DISTRIBUTED SYSTEMS.\par
Communication is the core of distributed system. We have hetereogeneous machines, nodes etc.. in those systems, and we need some general mechanisms that abstract those differences. What will we cover? Types of communication (persisntend vs transient, synchronous vs asynchronous), transient message oriented communication like sockets, persistent asynch message oriented communication, that is queuing systems and message brokers, and remote procedure call. In the case of the algorithm seen below, the "arrows" inside the ring were really RPC.\line The model for communication in the network is composed of muddlware protocols, at different levels: Physical, Data link, Network, Transport, Middleware and Application. Each of them has its own protocol. The sockets are completely above all of this, they are an abstraction. Remember that this is not a networking course. Http is the most commpon kind of application protocol. We'll never go below transport anyway.\par
Types of communication. We'll see, on the slides, a lot of drawings with bold and dashed lines representing the fact that a node is doing something (or not) during a certain moment in time. Usually, a client, after having requested something, wants to know, before the response, if the request was sent or not. A client may also want to know if the request was delivered succesfully or not.\par
What is persistent communication? We have it when there is some infrastructure between the sender and the receiver that conservs the messages. For example, a mail server: it keeps our mails until we read and delete them. This is a persistent system. We can have persistent and asynchronous systems. For example, a client may be sending a message to a server that is not up and running in that moment. So, we have a persistent system in the middle. When the server runs, he fetches the message from the intermediate level. In a persistent synchronous communication, the receiver gets the message, but doesn't handle it immediately. In this case, the receiver waits until this confirmation. In the Transient Asynchronous communication, the sender tries to contact the receiver. Independently on wheter it responds or not, the sender continues processing. Note that there is non intermediate level: the server either recieves the message or it doesn't. In the Receipt-based transient synchronous communication, the client sends a request and waits until the request is received. He doesn't do anything until the server responds. WE ALSO HAVE delivery-based transient synchronous communication at message delivery. In this case, the server receives the request, accepts it, and puts the client in a queue. When he starts processing it, he sends an "accepted" message to the client. Then there is Response.based transient synchronous communication but idk.\par
How can we do Message oriented Communication? Let's see the mechanisms. How can we classify sockets? They are generally not used in systems that use persistency. In fact, with sockets, theorically both computers are present during the communication (even if there are ways to use them asynchronously). How the sockets work, however, is hidden from our eyes. Let's see Berkeley Sockets. Suppose that a client wants to communicate with a server. What does it do? he calls a system call, that is a socket. [A socket it's like the plug of a cable. You need two of them for the communication to actually work.] When we call the socket primitive, the same thing as "opening a file" happens: the system returns us a socket descriptor for that specific socket. Each process in unix has three file descriptor, always: stdin, stout and stderror. When asking the system to open a socket, three file descriptors are returned. They are used to read and write on the "cable". Something else has to be done in the system: the descriptors are numbers really, so we can't use them for communication. So, after opening a socket, we perform a \b bind\b0 . It associated to the number of the socket a port, a free one. We are literally "binding on a port", a free one. In general, the bind operation will take as argument a specific port, if we want to specify it. We have then the \b listen\b0  function. It allows to open the connection by communicating with the TCP protocol (for example). It basically says: I'm a server, and i can receive many requests from different clients. So, please, OS, organize your memory so that I can receive different requests and process them (put them in a queue if necessary). This is NOT a persistent mechanism: remember that anyway the server needs to be up and running to do this. So, now the client calls a function called \b connect\b0 , that takes as arguments the IP and the port of the server. The connect, through the stack TCP-IP, is able to connect with the TCP of the receiving machine, that delivers the request to the server (if there is space to do it). If there is space from the listen, the request is queued. If there is an error, anyway, the connect fails. If it doesn't fail, anyway, the connect is accepted (\b accept\b0 ), and \b read \b0 and \b write\b0  operations can be performed. Note: in the client, there is still a bind, but it's hidden. The application, the programmer even, doesn0t need to know the port used by the client for communicating with the server. We just want a free port. In fact, we just need the IP and Port of both parts of the communication to fully describe it. At this point, it's all about our choice, that is, we can define the format of the messages and similar things. This is a mechanism that doesn't work if the server is not up and running anyway. Sometimes, we would like to have asynchronous mechanisms or some kind of persistent system. In those cases, we have a queuing layer. The queuing layer is between the sender and the local OS, both in the client and server side. It receives messages and keeps them until they can be processed. But what happens if the whoule receiver node is down? That the server can't get the message, so, we have no service. So, what those systems do usually is to use different nodes on the network to conserve the queues. In the message-queuing model, we have some primitives and their meaning in the slide. Put, Get, Poll (check in a queue if there is a message and remove it, but don't block) and another one. \par
The intermediate nodes that implement the queuing system, have also another role. In some intermediate processes, we have a so-called Broker program. He not only has to keep the messages when the destination is down. He also needs to convert some informations: if the two machines "talk in a different way", the broker needs to convert a message into another language when needed. These kind of systems, with a queue, are useful when we want asynchronous communication, and scalability can be increased by admitting delays in requests and responses. But also when the procucer is faster than the consumer, and when implementing a publish-subscribe communication pattern. The messaging protocols we will be seeing are XMPP (maybe?), MQTT, that is a very lightweight/efficient protocol for machine-to-machine communication (also used for IOT) and AMQP, that is more complete and belongs to the ISO standard. This is suitable for a lot of more different infrastructures. Some popular message-broker softwares are Eclipse mosquitto (supports the MQTT protocol, is opensource and lightweight), RabbitMQ (it's very popular and partially opensource, and also supports AMQP and other protocols), Google Cloud Pub/Sub, Amazon MQ and Apache ActiveMQ and Kafka. [All those things like RabbitMQ implement middleware for queue messaging].\par
In RabbitMQ, there are different modalities of use. In the Topic modality, that is the most commonly used, the messages are put in a queue, and then the consumer receives them. The receiver is just one.\par
The role of the broker is the important thing: we want to implement persistent communication, and without the broker we can't do this.\par
REMOTE PROCEDURE CALL.\par
What is difficult in making this remote? When we work with data structures in a program, sometimens we use functions, that take arguments. When we pass the arguments to a function call, we usually pass some variables that hold a value. The same goes for constant values. In those cases, the RPC is easy: i just forward to the remote machine the function name and the parameters. The problem is that sometimes we pass a memory address to a function. We can't do this directly in RPC. The remote machine doesn't know my local addresses.\par
\par
Lezione 5 - 30/03/2022\par
Synchronization and coordination among processes/nodes in a distributed systems. BUT, before that, let's go back to the topics of the previous class.\line We saw transinet, persistent, sync, async communication, sockets and queuing systems. The third topic about communication is RPC that works at a more abstract level. All P2P systems we'll discuss, like chord, have a communciation implemented as RPC. A critical point in RPC is the packing and unpacking of parameters. A DS that supprots a RPC needs to have both on the client and server side some technology that hides to the programmer the fact thath the objects are not all on the same node. We have a Stub both on the client and on the server. They just take care of the parameters when passed to the other side. The sender serializes the data, while the receiver deserializes them in order to understand what the sender wanted to communicate. This process is also called marshalling and unmarshalling. Marshalling requires serialization of objects, using a string-based system like JSON or binary format. Since this is a heterogeneous environment, we have to be careful about how the data is interpreted. The sender and the receiver must agree how to interpret them. This is why we need a standard way of coding this. RPC is sync or async? Well for sure is transient, because when calling a procedure on another nodoe that node must be ready to answer immediately. The client calls the remote procedure, the request is delivered to the server, and a reply is fired to the client. So, sync or async? Usually, both are supported. The standard one is synchronous, but in general can be also asynchronous. We can have also a different point of synchronization, for example, the client waits until the server has recieved the request, he doesn't have to wait for the result. \line Deferred synchronous RPC: as soon as the client is sure the server has recieved the task request, it proceeds with his computations. But at a certain points he will recieve the answer from the client, and at that point the client will be interrupted by a call from the server. How do we perform this interrupt? We can have a thread in the client that waits messages from the server (RPC calls more than messages actually). There are also Sys calls to avoid generating a second thread, but we won't cover that. \line Parameters: if they are passed by value, no problem. For example two integers. But if I'm using a complex data structure and I pass it by reference to my memory, what do I do? The remote machine doesn't have that reference to my memory. So we can't pass by reference.\line Marshalling and Unmarshalling: who writes the code to do that? Some stubs on the client and the server. How do we generate those stubs? Usually they are automatically generated: the system generates it in a C-like manner (?). To generate the stub, we have to provide: the name of the procedure, the kind of parameters it wants. We must also think about the fact that the server and the client might be written in differen languages. Anyway, once we have our Interface definition file, that tells how to format the stubs in different languages, we can use a IDL compiler and use all the languages available to generate the stub (the stub is a reference to the true object on the machine).\line A modern RPC example is gRPC. It is largely used to build low latency, scalable distributed systems, and supports multiple languages. It's also open source.\line esample of IDL and stub generation in gRPC:\line 1) Define: define the function name and the parameters.\line 2) Compile: in the language you prefer\line 3) Generate the stub. \line How do we bind a client to a server? If we have a server, we'd like to make a RPC available to everyone. How can a client in the world know about my server and use my funciton? Well, first of all, the server process might be a daemon process, that is always up and running. What a server can do is "Register an endpoint" in a endpoint talbe present on a DCE daemon process. So, this daemon has a table: a list of all the processes in that machine. So, 1\'b0, register endpoint: tell the daemon that I have this service. But how do I advertice my service? 2\'b0: we have a Directory server, on a different Directory machine, which is notified about the fact that I have my service up and running on this machine. If a client machine is looking for a certain function, but is not interested on the specific machine/process that gives that function, but instead on the function itself, he asks: how do I call this function? 3\'b0: the client contanct a look up server to discover who has the service I'm looking for. 4\'b0 He then asks for endpoint, contacting my machine, in particular my daemon process. This process checks if the service server is up and running. If it's not running, that process starts running. And now, 5\'b0, the client can perform the RPC. The daemon in fact communicated to the client the port on which he can find the the server process that now is running. \par
So,\par
Synchronization Problems in Distributed Systems.\par
We saw the architecture of the DS (how the nodes are connected), we saw the communication part, and now we have to see how do they synchronize to do a more complex task. We synchonize because we are here in class at 9.00. The computers need to do something similar. Another problem we have is mutual exclusions: we have problems even on a single machine, imagine on a DS! But if we have mutual exclusion, how do we decide who can access to them in a certain moment? We need some algorithms to do that. We can also have an algorithm that chooses a node that has to perform those kind of decisions.\line Simplest solution about clock synchronization: we use the clock (the physical clock) to synchronize. Example from Tannenbaum (probably). [he's doing a makefile example to explain how a unix systems deals with file modifications, usinig timestamps, to avoid recompiling the whole project]. In a makefile example, what if the compilation is performed through a RPC? The two machines might have a different clock. Unless we have a way to synchronize them, the system won't work. In the image on the slide, basically, even if the output.o was created BEFORE the output.c file in ABSOLUTE TIME, the fact that the two clocks are not synchronized makes the makefile think that output.c (2143) doesn't need to be recompiled (output.o is seen as created at 2144 instant). This is a problem. \line S*it we have to understand how physical clocks work.\line In computers, we should specify the reference time, UTC time (Coordinated Universal Time). it's a standard time. The time is related to a astronomical phenomena. There are several institutes in the world that study this topic. There is someone that, once in a while, takes away some time (strong powers) to adjust the time. They just observe the universe and take some time away when needed. What kind of clocks do we have anyway? How do they work? with crystal of quarts that vibrate at a constant frequency, so we kind of compute the vibrations that occour in a second and we measure them. But a more precise way to measure time is by using atomic time. TAI (International Atomic Time) seconds are of constant length, unlike solar seconds. Leap seconds are introduced (once in a while) to keep our time in phase with the sun. What is a second? A bilion of transitions of a cheesion (???). A SECOND is just an observation made by looking at the universe. The time that passes from when the sun is precisely above the earth and blablabla. If we keep the atomic time, measured by the TAI, we have something very periodic. But really, seconds are not always the same. That is why leap seconds are introduced once in a while to keep the time, our time, in phase with the sun. To get the UTC (TAI + leap seconds), we have to contact one of those institutes.\line In reality, we can have a node that perfectly measures time. It doesn't exist really: there are some nodes that go slower, some go a bit faster (it depends on the quartz), and if we let them do so, bye bye, our clock goes in the wrong way. We have to make sure our nodes synchronize with UTC, and the synchronization should be performed frequently. Depending on how imprecise is our quartz, we have to calculate the frequency at which we have to synchronize.\line So, how do we synchronize?\line Method 1: Using GNSS. \line GPS is the most used Global Navigation Satellite System (GNSS), and is used mostly in smartphones. Since this is a collaboration between multiple satellites, those satellites whould synchronize with each other. To do so, they have atomic clocks on board. If we can get the signal from the satellite and get their position, we can also obtain our time. ERROR: GPS sends signals to satellites. This is a MISTAKE. GPS, that is in our phones, is a GPS \i receiver\i0 . It recieves signals, but doesn't send them anywhere. The satellites sends signals, and our phone GPS recieve the signals. What's in those signals? What are those satellites saying? Timestamps. They are telling us what time it is. They also have an ID that identifies them. The GPS is actually measuring the time that the signal took to reach our phone (the phone knows its position and the one of the satellite), and tries to build the right time. There are some approximations used of course. But the time obtained by GNSS is very accurate, in the order of micro/nanoseconds. But it really depends on the accuracy of the GNSS. This is one way in which we can synchronize. This is one possibility, if we have GPS in a DS, we can use satellites. Keep in mind that in DS we can't have the same exact time in all the nodes, we need approximations. \line Trilateration idea (slide 10): supposte that 14,14 is the satellite. -6,6 is another satellite. My phone can tell the distance from himself and the satellite. If he queries the distance also from another satellite, he can close the search to just two points. With a third satellite (trilateration), he can know his true position.\line Method 2: Christian's algorithm and Network Time Protocol.\line There are servers, on the earth, that are connected with atomic clocks, and know the right time. Some computers are directly connected with atomic clocks, other are connected at high speed to them. \line When we communicate with another computer, some time passes. All nodes would like to have a precise time. All computers connect to an NTP server to know the right time. If a client asks to one of those precise servers the exact time, some time passes before the response is given, of course. Somehow, we need to estimate the delay, also to understand the true time. How is it done? This communication client-server is done several times. Since we want an estimate of the latency bw client-server to get the right clock time, we make this several time and do an average. When the client communicates with the server, four things, at four points in time, happen:\line T1: the message is send to the server.\line T2: the message arrives to the server.\line T3: the server sends a response to the client.\line T4: the client recieves the message. The precision that this entity reaches is bw one and 30 ms (with the GPS we can be more precise), that is generally good. The frequency at which we should run NTP depends on how we want our approximation in DS. We should also ask ourselves which is the worst synchronization we can get, and accordingly decide the number of NTP request we do in a time unit (frequency). \line Method 3: (Internal Synchronization) the Berkley Algorithm. If our machines interact with the real world, we'd like the time used inside the system to be precise in respect to the one outside the system. Example: let's say we have three machines. I decide that one of them has a role to be a time daemon (it's always active). The time daemon asks all the other machines for their clock values. It is likely that the times provided are a bit different. The daemon will understand how far behind or in front are the clocks of the other nodes. We are NOT considering the latency between the nodes (we suppose the latency is 0, like in a fast cluster). The daemon, what does it do now? It doesn't force the other ones to have its time: he makes an average out of the different times (his and the ones of the other nodes) and forces that clock to all the nodes, himself included. This Berkley Algorithm is used when there is no need to syncronize with UTC, but just synchronize the nodes in the system. But why average? The idea is: let's try to not have too big displacements. I want to minimize them. What happens in reality? The daemon tells other nodes the time they must have. But some nodes might be, in this way, "be sent back in time", so we could have inconsistency. I can't have a file that comes from the future. What is done in practice is: the time will always go forward, but since clocks inside a computer are anyway counters, the time is slowed down until we reach the average time we wanted (that is, until all nodes are synchronized). \line So: UTC, Berkley, and now...\par
Lamport's Logical Clocks: very cool idea.\line Sometimes, the nodes don't need to agree on their physical clock values. Sometimes, it's enough to make the nodes agree on the order on cerain events (fix the before and after, not on absolute time). There are many cases in which we'll see that we have to find a solution for mutual exclusion.\line The logical clocks idea is: decide the order of events. An event is an internal event or a message being sent or received. An integer counter is used at each node as a logical clock. It is incremented every time an interesting event occurs. In this way, the makefile problem is solved: the event of modifying a file happens before making its .o file. So, this counter is incremented every time an interesting event occurs. \line If a and b are events, a --> b means that a happens before b. It is a transitive relation. C(a) is the logical clock value assigned by the process where a occurs. The value of a logical clock can only increase. So, if a --> b, then C(a) < C(b). Some events can be: send a message, receive a message... Anyway, we can't always say that a certain event occurs before another one, it depends on where the events happened. On two different processes, it is not always possible to order the events.\line Idea: each time a process receives a message from another one, the clock instant of that message received is C(message sent) + 1. The logical clock is basically adjusted based on the time of the message received. \line So, LAMPORT'S ALGORITHM is cool, and will be in the exam. Slide 25: it is an example in which Lamport does \b NOT \b0 apply. Applying Lamport means changing some values. 56 becomes 61. 64 becomes 69 (61 + 8, it has to stay 8 events after).\line The algorithm is run in a middleware, that is in the middle (lmao).\line We'd like total order in the timestamps. It may happen that two events have the same timestamps. To reach total ordering, we can modify the algorithm by attaching a process number to the timestamp of an event, or anyway a ID that uniquiely identifies a process in a DS. If we attach this number to the timestamp of the logical clock, we can distinguish different values. The timestamp of event e, P_i, is C_i(e), where i is the process number. In this way, if two events have the same timestamps, the one happened before is the one with the lowest process ID. \line Exercises till slide 30.\par
\par
\par
\par
\par
}
 