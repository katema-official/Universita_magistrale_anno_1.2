{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1040{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.22000}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\qc\f0\fs44\lang16 Distributed and Pervasive Systems - Appunti\par

\pard\sa200\sl276\slmult1\qj\fs22 Lezione 1 - 02/03/2022\par
Claudio Bettini. He worked for IBM in New York for one year. He also has been affiliated with idk University. Processes and Threads will return. Networking too (ISO-OSI stack, TCP-IP...).\par
Friday we have the "apply part" of the course, where we will work and follow what the tutors teach us. The goal is to experiment technologies related to the theory. We will develop and design a project of a distributed system with a pervasive part. \par
We will start at 8.50 (!!!). We will have a break in the middle. Let's motivate this course. Modern applications are Distributed Systems. All the applications that we run (or at least, a lot of them) like youtube, instagram, gmail... run as DS. They have an infrastructure organized as a DS. It would be good to know the concepts behind them. The course anyway is not only about Distributed Systems, because they will naturally include nodes (definition later). The nodes are phones, smartwatch, IOT devices, etc.., not just network and computers. Also, blockchain could not exist if it was not for an algorithm of DS. We will talk a whole morning of the blockchain. Anyawy, DS expertise is required in a lot of contexts.\par
Course objectives? Understanding the foundations of modern distributed systems. but we will not learn about about GRPC (in the lab yes though) or technologies in general: we will grasp the problems and the solutions, so that we can understand the future DS. So, foundations. Also, we will learn about transparency, synchronization, fault-tolerance consensus and blockchain, sensor data management and context-awareness. And also, we will be guided to design and program a distributed system. There is also a risk that someone copies our projects from Github and uses them. The project will be done singularly: the code must be self-made. the teacher doesn't want us to publish our project until february, when the last session of this academic year ends (the risk is that someone copies it, and it happened).\par
What will be talking about in this course?\par
First, what is a DS. Then, the architectures, like client-server and peer-to-peer systems and their organizations. Then, communication in DS. A DS is a collection of pcs that communicate together after all. This is the most interesting part of the theoretic part of the course: if we have different computers that need to communicate, how do we coordinate them to achieve a goal? An idea can be that the pcs share and idea of time, a clock, so that they can coordiante their actions. There are problems, of course, that we will study. We'll also see about mutual exclusion and election algorithms. In single programs, locks and semaphores are used. But in DS, we need communication. We'll see algorithms to do mutual exclusion in this way. The election algorithm instead tries to find a pc among all the ones that must be elected to act as a moderator.\par
The second part is about Pervasive Systems and their applications.\par
The third one is a guidance to project development. We will develop something that deals with, idk, election algorithms. We use Java, and concurrency & multi-threading. We'll learn about gRPC and MQTT. This last one is a technology used to hadle data coming from sensors.\par
TEXTBOOK: Distributed Systems: Concepts and Design, 5e, Coulouris, Dollimore, Kindberg & Blair, Addison-Wesley, 2012. Isbn-10: 0132143011. Occhi per\'f2 che the book does not cover everything.\par
Distributed Systems, third edition, version 3.03 (2020), Maarten van Steen and Andrew S. Tanenbaum.\par
Multiple choice questions + 2 open questions, one is usually an exercise about an algorithm idk. 50-50 theory part and project. Ricevimento on appointment.\par
\par
End of introduction. Tanenbaum's book will have a lot of things of this class. What is a Distributed System? \i A collection of independent computers that appears to its users as a single coherent systems\i0 . This means that the single units don't have multiple CPUs, or at least, they are not required to have more than one. The fact is that each PC has its own memory, there is no shared memory. They are phisically independent, but they should appear as a single entity. The user should not know how many computer there are. And how do we build such a system? Of course we need a network that connects them. Each PC has a local OS, that can be different from machine to machine, and each PC may run a certain application. A given application can run on multiple computers! How? Well, thanks to specific softwares, that use TCP-IP technology. Usually, we have to install a middleware, a Distributed System Layer, that hides some details from the layer: in particular, it hides the fact that the application is running on different PCs. For example:\par
imagine opening a folder in your PC that shows you contents that are not really on the machine that you are using. The resources are shared. Another thing that can happen is that, when i start a process, it really starts on another computer, even if it looks like it started on my local computer. Also in Multiplayer Online Games we can have different components distributed on different machines. And of course the WWW, even if this is not a precise example. But somehow, the WWW has distributed resources (it has different independent computers that are connected and work together). But why it is not exactly a Distributed System? Because, if i look at the website of the course (for example), i do because i know what i'm looking for. There is no location transparency: i know where my resource is locate, i have its IP address! But in a DS i don't need to know where it is.\par
An ironic definition of Leslie Lampor is: "\i you know you have one (DS) when the crash of a computer you've never heard of stops you from getting any work done\i0 ". This is a problem we'll have when developing, since debugging will be difficult. We will have to coordinate different machines, but we'll see some tricks to have an easier life.\par
What are the goeals of a distributed system, its properties? Well, we have some resources that we want to make accessible. And we would also like Distribution transparency. We have different levels of transparency. In ALL EXAMS there is a question of transparency. There are different kinds of transparency that we'd like DS to have. I.e, location trasparency is the ability of the DS to hide where a resource is located. Or, access transparency is the ability of the system to hide different data representation and how a resource is accessed. The component of a DS are independent computers, that can have different architectures, file systems, and so the way we access to the resources in those File Systems is different. A DS should have a layer that hides how the user accesses those datas. A network File System (NFS) is an abstraction layer that standardizes the access to the different FSs.\par
Migration: the user doesn't have to know that a resource is located in some PC. And doesn't need to know if that is moved from a PC to another. Relocation, instead, is moving a resource while it is used (is migration but more complex).\par
Oh, n.b.: not all DS have ALL those properties.\par
Replication transparency: the user should not be aware of the fact that a resource is replicated in different places of the world. I'd like to replicate a reource in roder to have fault tolerance and better performances (if i have a resource closer to me, i'll acces it faster). Concurrency transparency is that a resource may be shared by several competitive users, but this is none of their problem: for the users, the resource they accessed is just accessed by them. Failure transparency is that if something fails, the system should be able to recover by itself, and the users shouldn't note that a failure happened. For example, in an FSM, if a machine is trying to access a resource but it doesn't have that resource, that it asks to another machine to pass it. If this fails, it asks to someone else, and so on until a timeout or the resource if found.\par
A DS also needs to be open. But what does it mean? An open DS should offer Interoperability, Portability and Extensibility. Interoperate means being able to connect and work with another system. Portability means that an application can run on different DS that have minimal or no differences. Extensibility means it can be extended. How do we achieve opnenness? A general rule to guarantee this property is to not design our own protocol and keep them secret. if we do so, we'll be the only one to know how our program works. We should use standard protocols. We should also publish key interfaces with specific languages, like Java. But there are a lot of frameworks (like corba) that allows different processes to run on different computers and in different languages, but they can still communicate one another. So, we need standard interfaces, or API, to make different processes communicate one another. And last, we need testing and verifying the conformance of components to published standars.\par
The last property we'd like a DS to have is scalability: what is scalability? We want to be scalable in terms of what? One example is size scalability: if is increased the number of nodes/users, i have to scale the resources that support them. Can the system do that? Other scalabilities are the geographical one, that require the computers to stay "close" geographically speaking, and understanding if the administration is centralized or distributed. Here are some problems related to scalability, or implementation choices that can make scalability an issue:\par
-Centralized services: a single server for all users. If a single mahine handles all the users, and they grow too much, the machine fails. In this case, a load balancing technique in a DS context would have been better. We need replication and things like this.\par
-Centralized data: of example, a single on-line telephone book. Or, the old way to do the name resolution in internet (symbolic addresses to numeric addresses). The Domain Name Service is nowdays distributed, but it was not always like this. There was once a file on a machine that was queried when a resolution had to be done. This caused problems of cause.\par
-Centralized algorithms: example, doing routing based on complete information. Routing is decing in which way a message has to go. An algorithm that decides the whole path, it is not efficient. If each router handles local informations, btw, the load balance is better handled. In decentralized algorithms, we have that: No machine has complete information about the system state. So, a router knows the links to neighbor routers, no more. Machines make decisions based only on local information. Ok, this is self-explanatory. Failure of one machine does not ruin the algorithm. Of course, if a machine fails, the whole system still stays alive. If a "moderator" fails, the system should be able to elect someone else to moderate the system. Finally, there is non implicit assumtpion that a global clock exists. In a DS, the computers are not always perfectly syncronized. But we usually need perfect syncronization. Well, we'll see how to deal with this problem, and how the machines don't assume a perfecto shared clock. We'll see centralized and decentralized versions. \par
Imagine that a server has to handle the checking of forms as they are being filled, or a client that has to do the same thing. In this last case, i take computations away from the server.\par
Also the DNS nowdays uses a more scalable way to do name resolution. The DNS is organized in domains, zones, and it has multiple servers on the edge of the internet. The name resoultion is performend through communication and collaboration of different nodes spread in the world. This way of distributing the name resolution is a way to make the WWW scalable.\par
When dealing with DS, we should not keep some thing in mind. The following are false assumptions in DS:\par
-The network is reliable. False.\par
-The network is secure. Of course not.\par
-The network is homogeneous. No.\par
-The topology does not change. of course it does.\par
-Latency is zero.\par
-Bandwidth is infinite.\par
-Transport cost is zero.\par
-There is one administrator.\par
-Debugging distributed applications is analogous to standard applications.\par
Now we have an idea about what a Distributed System is, its properties and problems. Now, what kind of DS exist? Well, for example, Distributed computing systems and information systems and pervasive systems.\par
Let's focus on Distributed Computing Systems. What are clusters? A collection of similar servers closely connected by high-speed local-area network and usually running the same operating system. The idea was: having a high-performance single computer is difficult. What if we have hundreds or thousands of PCs connected together that simulate a high-performance computer? The goal was high performance and availability. There are two kind of clusters: symmetric and asymmetric.The symmetric is: all the nodes are the same. There is no master, all nodes have the same software installed, and run the same program. All of them run just one code, the same one, but it is executed in different processes. One of them is elected as coordinator anyway. This is kind of challenging, and has not been the most used type of cluster. In the asymmetric approach, we have a master node instead. We have a lot of compute node with their own local OS and a component of a parallel application. On the mster node, however, we have the local OS, a management application and parallel libraries (?). The case of Google Borg: Google designed this system in 2003, and it had a master, a certain number of compute nodes, and all of them communicated. The strange thing is that there were multiple BorgMasters. Why? For fault tolerance and performance reasons, in order to not have pitfalls or bottlenecks. The five masters elect one of them (the one that is doing the job), and that is the working one. the other fourt though follow the work of the first one (they clone the state). In this way, if the super-master fails, the other can take its place. So, each of them stores the state, but we need a way to ensure their state is the same. So we run the Paxos protocol that makes this possible. After 10 years of google running boh.\par
Nowdays we have Kubernetis, that Google made open source (?). It is widely used because it was donated by the Linus Foundation and everybody can use. Kubernetis is a platform for managing containerized computer applications. It can be seen as an evolution of Borg, and is an example of Asymmetric cluster. [Compito per casa: leggi articoli riguardo borg. Lui ha pullicato un articolo su Borg con le slides].\par
So, cluster computing is a kind of Distributed System. Cloud computing is another kind of DS, that allow users to access resources without knowing exactly where they are. In cloud, we have that the nodes are etherogeneous in hardware and operating systems (not the same as clusters i think). In this case, different cloud clusters can have different technologies, OS, different networks, and so on. The Cloud Service Models are Iaas, PaaS and SaaS.\par
IaaS: I just want this much of memory, computer power, and so on.\par
PaaS: I want more things. If the cloud has a distributed database, i'd like to use it. So i use some higher-level tools of the cloud. An example is Google App engine.\par
SaaS: google docs, Gmail, Youtube... all applications that we use as services.\par
There are also different deployment models, like private cloud (exclusive of a single organization comprising multiple consumers), Community Cloud, fuck. Other but i didn't read.\par
Edge or Fog Computing: introduced to take processing closer to where data is really produced. Like, we have a lot of IOT devices nowdays, and we want to process their datas. The important part is: behind all those systems, we have DSs. \par
Pervasive Computing: \i "the most profound technologies are those that disappear. They weave themselves into the fabric of everyday life until they are indistinnguishable from it"\i0 . The idea is to make everyday life things "intelligent" by putting a limited computationl power inside of them, and let them be part of a DS. If we do so, we can retrive and share a lot of datas. In general, a system is "smart" if it can adapt the way it behaves depending on the context. if the system can understand the context and change the behavior, it can be considered as smart. But doing so is not easy of course. \par
\par
Lezione 2 - 09/03/2022\par
Distributed Systems Architecture. A DS architecture defines the main entities of the system (sensor nodes, processes, threads, object, components, services). It describes also the pattern of communication between those single components. It describes also how do they communicate and the role of the entities, for centralized and decentralized architectures and also hybrid once (server, client), and other things. Some architectures in clud computing are also absed on containers and microarchitectures.\par
In centralized architectures, we have client and servers architectures and the event bus architectures. In the client-server model, we have a central component, a server computer, that waits for requests from clients, and we can also have servers that act as clients. In this patter of invocation and service answered from a temporal perspective, we have that the client requests something to the server, it provides the service and replies to the client. There is also network delay, that is, the moment the client requests a service is different from the moment in which the server starts working on it. In this case, the client stops until the server has fully processed the request and answered to the client. There are many variants of the client-server model. We, as designers of the system, can make decisions that have an impact on scalability and other things. In a case, the client is very dumb, it just has a UI, and gets all the informations from the server. We can have also models in which the UI is completely client-side, the database ont server-side, but the application is split between the two. This is the case of a web-based application, where we could use javascript for the client side both for UI and the application. In other cases, the application is completely client-side and forwards SQL queries to the server! The server just has the database and is of course capable to fulfill the requests provided by the queries. The right-est extreme is that portion of the database is cached within the client. There are, in all this, a lot of caching systems. How does caching work in those c-s architectures? The web servers communicate with a proxy server, that is in the middle between the server and the client (closer to this one, so faster client-side). This is used to cache some requests in those proxies, so that future accesses are faster. In the DNS, it's a good idea to store some conversions even locally on the laptop. Another common way to organize software components in a client-server architecture is to not have only two tiers (client and server), but on more tiers, like three. A way is to divide the application logic in a server and the database (intended as data management) in another server. This is because, in this way, datas are divided from applications. In this way, application can change while the datas just increase, but can stay without problems. In those case, we are not only talking of different processes, but different hardwares. This is even an old architecture! In the VERTICAL DISTRIBUTION, we have a different server or node for each functionality of the system. in this case: data management to a node, application logic to another node and presentatio (UI) to another one. It's up to us to decide wheter it's better to distribute an application on more machines or viceversa. In fact, in HORIZONTAL DISTRIBUTION, the same functionality is distributed on multiple servers or nodes with load balancing. For example, we can have a server that accepts the requests and forwards them to other nodes. in this case, the same functionality is distributed on multiple nodes. This helps with fault tolerance, node balancing, etc.. the Round Robin technique can be used for the distribution of the tasks. The VERTICAL and HORIZONTAL distributions can also be combined. In those cases, each functionality is duplicated on a separate group of servers with load balancing. So, some servers are devoted to a specific functionality (vertical), but each functionality is divided in multiple machines (horizontal). So, multiple levels for different functionalities, but at each level we have multiple machines.\par
Let's talk a bit about the Microservice architecture. We can take to extreme the idea of separating functionalities (as in the vertical distribution). In this case, we run each component/functionality of the application logic in a separate process and possibly a different machine. The idea is: if i have a functionality that is independent from others, i'd like to isolate it in a single process or machine. This of course adds network overhead. But nowdays, with low latencies, it is possible to also follow this approach. microservices must be independently replaceable and upgradable, packaged with all they need to be deployed. Microservices communicate with lightweight mechanisms, like REST API or google RPC (invoking a function that is in another machine). Focused scalability: it means that the system MUST be scalable (a lot of users), and to achieve that i find out what's the functionality that is the most critical and i duplicate it, so that i don't have bottlenecks. And also, microservices can be written in different programming languages. [Java RMI is similar to RPC. The problem was that everything had to be written in Java anyway!]\par
A container is the abstraction at the application layer that packagesa code and dependencies together. Basically, the idea is: cool the microservices! But how are they run? To increase portability, we want to run those functionalities in an enviroment which has all that is needed to run a software. We wrap, in a \i container\i0 , the software that implements the functionality and all the libraries needed to run this piece of code. But where do I run this container? Well, the idea is similar to VMs. In a VM, we have a Guest OS that contains bins and libraries and one ore more applications. What's the different thing in containers? Well, we are at ahigher level. We don't care about OS. We have a basic, Host OS, and on top of it we have Docker. On top of docker, we have a container with the app and its bins and libraries. Docker can be thought ad a container engine. It does some optimizations: for examples, three instances of the same program (MySQL DB) can share the same libraries.\par
HE WILL NEVER ASK US ABOUT CONTAINERS (they are just a way to run microservices). But the idea of an architecture where each functioanlity is run in a single, separated environment, and the microservice concept, is important. This was the centralized architecture: we always have a server(s) that serve clients.\par
Another centralized architecture is the Event Bus Architecture. It works following a pattern called publish-subscribe. In this case, we have a certain number of nodes (subscriber), that are interested in getting some informations. And there is another number of nodes (publisher) that publish those informations. The informations, the "news", are published inside an Event Bus, and the subscriber takes them from the Event Bus. The publishers can be seen as source of events. This model is centralized because of the presence of a Broker in the middle, that is similar to a server. We can duplicate it oc.\par
\par
Now, decentralized model. Many years ago, distributed systems investigated a different way to distribute their nodes. In this architecture, we have different nodes that act both as clients and servers. All nodes have the same functional capabilities. Napster is a well-known ?2? system that started in 1999. It was proposed by two college students, but was not totally decentralized. It was used to exchange audio files. Some years ago, infact, it was shut down. The idea was: hey, friend, you have a file i'd like. Can you share it with me? To do this, a server was reached (this is why it was not fully decentralized). The server gives and index, that is a computer that has the file (it can be a list of indexes). At this point, the client requests the file to the given node (infact, the index is really an IP address and a port). The most important part of the protocol starts now: the computer that got the file adds its IP and port to the index server, to notice that now he also has that file. So, this was Napster.\par
Problems in developing those systems: how to place data objects across many hosts in order to achieve load balancing while accessing data and ensuring availability avoiding overheads. \par
Three generations of P2P systems (like in the book): we already talked about the first one, Napster, used for file sharing. Then, protocols were refined to improve scalability, fault tolerance and anonymity, and people started working on other systems with a similar decentralized architecture. Gnutella, for example, that said: let's design a system like napster but without servers. FreeNet said: we want to be anonymous while sharing those files. BitTorrent is kind of doing the same thing, but the idea, the key idea, is that a single file was downloaded from different nodes. Different part of a single file, called chunks, are asked to different nodes, and used to construct the whole file. The last generation is P2P middleware: can we design middleware that achive the objectives said before in a way that is abstract form the data that we are looking for/sharing?\par
So, once those systems reached a good degree of scalability, we hade P2P middleware. Their goals were to enable clients to transparently locate and communicate with any resource, add and remove resources and add and remove peers (those goals are a QUESTION IN THE EXAM!). So, the objectives are that we can add new partecipants to the system easily and delete nodes also easily. A way to optimize those operations is to use an overlay network, like internet. An overlay network is a logical network, or virtual network, in which we impose a pattern of communication on the nodes that don't correspond to the actual connections of the nodes. The optimization criterias are global scalability, load balancing, locality of interactions, etc.. So, we have an overlay network, like a ring composed of nodes connected as a ring.  If we have this kind of overlay, we can think about overlay network as another virtual network.\par
Ooook, so, the Routing Overlay is a distributed algorithm used to locate nodes and objects in an overlay network. Routing requests fro clients to hosts holding objects of interest at the application level instead of the network level IP, etc..\par
We can divide P2P systems in two families: structured and unstructured systems. Most of them will be using a structured P2P system, that is deterministically built in order to obtain efficient routing towards the node containing the required data. In the unstructured, the overlay network is built with randomized algorithms. That is, each peer only knows about its neighbors. Let's see structured overlays first: the idea is to have a structured overlay, like the ring overlay. In this way, we can also optimize the search in the system. For example, we have a certain number of bits for identifying a single node. If we have 16 nodes, we just need 4 bits. A function is used to associate an address to each node. Some nodes are called \i actual nodes\i0  and are the nodes that are present in the system. Also, some data keys are associated to each actual node, following a certain algorithm (this is the Chord structured overlay). (the resources have an address). Distributed Has Tabels are used for having a faster search. What are they and how do they work? The idea is: we have a data structure in each peer, that is a Hash Table or Finger Table. It's an extremely simple table in which in the first column we just have an index. The number of rows is the number of bits used for the address. In the other column, we have succ(p + 2^(i-1)). p represents the index of the current index, while i is the value of the first column. succ(x) means the next available node which index is higher than x. \par
This kind of structured overlay, the ring, and this algorithm, is just an approach. CAN, content addressable network, uses a different kind of separation of ndoes. CAN uses a bi-dimensional address space, where we have different points, and to each of them a certain space is associated. Each node manages all the points in tis region. In this case, the insertion is more complex: we have to divide a rectangle and give half of the resources to a node and other resources to another one.\par
Limitations of structured overlay? I mean, why somebody came up with the idea of unstructured overlay networks? Well, the finger table has to be mantained. Each time a node goes away from the system, we have to change the finger table This maintenance of complex overlay structures can be difficult and costly to achieve. If our target is a dynamic environment, we might not want distributed has tables, that are too costly to mantain. We'd like something that is resilient to fault tolerance and auto-organizes the nodes.\par
In unstructured overlays, the routing is based on randomizing algorithms. Each nodes stores a list of neighbots randomly built. That will be its "view" of the overlay. Also, resources are often randomly assigned to nodes. How is the search handled? It starts from a node and propagates according to local views. Also, the search is often limited to a number of hops or timeout, and the resource replication helps improving search success rate. The search must be limited because else I'd ask to a lot of peers for a file, and that is time consuming. That is why we replicate the resources. How do those randomized algorithms work? one of them is the gossiping algorithm (?): the nodes, periodically, exchange, with the nodes with which they are connected, some informations about other nodes. Basically, a peer communicates a fraction of its knowledge (its neighbor peers) to another peer, randomly (circa). But since this is a dynamic system, we can have that a node goes down at some point, and i cannot communicate with it. For this, the nodes will tend to share the last contacted nodes to other peers. \par
Structured: we are guaranteed to locate objects, if they exist, and provide  time and complexity bounds on this operation (hash table -> logarithmic time). Also, relative low message overhead. BUT. the manteinance of this structure is compex.\par
Unstructured: they self-organize, they are probabilistic and so cannot offer absolute guarantee on sh-\par
Hybrid architectures: for example Gnutella, that has some superpeers connected one another, and they are the reference for a P2P network of their own. \par
BitTorrent: another Hybrid architecture, in the sense that a part is centralized (when looking for a torrent file). In a .torrent file, we have a list of trackers. Inside of them, we find the IP address of ?. The point is that getting the address is centralized (like Napster), while the single nodes part is decentralized. [he won't ask exactly how BitTorrent works, but hybrid systems and hash tables yes].\par
\par
Lezione 4 - 16/03/2022\par
We'll go back to the chord algorithm. We were talking about structured and unstructured overlay network. Chord is an algorithm that maps any kind of resource to a node. How to quickly find resources? how to remove or add nodes? One idea that was used in chord but not only (also in other structured overlays) was to organize the overlay as a ring, so each node only knows about two neighbours. Each node had a Distributed Hash Table, called finger table. How many entries does each table has, at most? 160 bits (?), so each node has at most this number of nodes/addresses that he knows. The time that the algorithms takes to map a resource to a node is at maximum Olog(n), that in this case is almost constant. How does a new peer get an address in the space? Using a hashing function. [A way to get a unique name for a peer is to get the ID number that identifies the machine on the internet and the port that identifies the process in that machine.] Since we have 160 bits, the address space will never be filled (too large number), and so each node will handle multiple resources. The nodes are much less than the resources. A file with key k (obtained through the hashing funciton) is managed by a node, that is the first node (going along the ring clockwise) with id >= k, called succ(k). The calculus is done % 2^m, where m is the number of bits used. \line Chord provides a function LOOKUP(l) to efficiently find the address. Infact, this algorithm provides a middleware that gives us this function, used to know the address of the peer that will handle this resource. [Sta mettendo molta enfasi sul modulo, non so perch\'e9. Comunque id >= k ma tieni sempre presente il modulo].\line So, chord has this algorithm behind, but the programmer doesn't need to know all this stuff (but if he wants to program new algorithms, he has to know them oc). Now, imagine that we have the finger (hash) table of a certain node with id = p. Its number of entries is m, where m is the number of bits used for addressing all the possible peers in the ring. FT(i) is a function that we use to refer to the content of of the Finger Table, row i (FT(3) = 9 in the image). The value of the function, basically, is the address of a peer. But, as said, we need to know, to communicate with a peer, its IP and PORT. So, together with 9, we have an IP and a Port. The value of the function is calculated in a clever way: succ(p + 2^(i-1)). All the operations are % 2^m. succ(j) means: if j is not a node, take its successor node (the next available peer in the ring). [In tannenbauns there is the same example of the algorithm of Chord].\line The formula says, for resolving a key k: find FT(j) such that FT(j) <= k < FT(j+1). After that, the search is forwarded to the node FT(j). If there is no FT(j+1) in the table, then take the largest address number. \line We take FT(j) and not FT(j+1) because between those two there may be a lot of nodes. However, there is a case in which i can go directly to FT(j+1): when k = FT(j+1).\par
Benefits of having unstructured peer 2 peer network with superpeers? Keeping an index of data for a subnetwork and optimizations I didn't read.\par
Let's see some questions we might get in the final exam.\par
Saw them lol.\par
\par
COMMUNICATION MODELS IN DISTRIBUTED SYSTEMS.\par
Communication is the core of distributed system. We have hetereogeneous machines, nodes etc.. in those systems, and we need some general mechanisms that abstract those differences. What will we cover? Types of communication (persisntend vs transient, synchronous vs asynchronous), transient message oriented communication like sockets, persistent asynch message oriented communication, that is queuing systems and message brokers, and remote procedure call. In the case of the algorithm seen below, the "arrows" inside the ring were really RPC.\line The model for communication in the network is composed of muddlware protocols, at different levels: Physical, Data link, Network, Transport, Middleware and Application. Each of them has its own protocol. The sockets are completely above all of this, they are an abstraction. Remember that this is not a networking course. Http is the most commpon kind of application protocol. We'll never go below transport anyway.\par
Types of communication. We'll see, on the slides, a lot of drawings with bold and dashed lines representing the fact that a node is doing something (or not) during a certain moment in time. Usually, a client, after having requested something, wants to know, before the response, if the request was sent or not. A client may also want to know if the request was delivered succesfully or not.\par
What is persistent communication? We have it when there is some infrastructure between the sender and the receiver that conservs the messages. For example, a mail server: it keeps our mails until we read and delete them. This is a persistent system. We can have persistent and asynchronous systems. For example, a client may be sending a message to a server that is not up and running in that moment. So, we have a persistent system in the middle. When the server runs, he fetches the message from the intermediate level. In a persistent synchronous communication, the receiver gets the message, but doesn't handle it immediately. In this case, the receiver waits until this confirmation. In the Transient Asynchronous communication, the sender tries to contact the receiver. Independently on wheter it responds or not, the sender continues processing. Note that there is non intermediate level: the server either recieves the message or it doesn't. In the Receipt-based transient synchronous communication, the client sends a request and waits until the request is received. He doesn't do anything until the server responds. WE ALSO HAVE delivery-based transient synchronous communication at message delivery. In this case, the server receives the request, accepts it, and puts the client in a queue. When he starts processing it, he sends an "accepted" message to the client. Then there is Response.based transient synchronous communication but idk.\par
How can we do Message oriented Communication? Let's see the mechanisms. How can we classify sockets? They are generally not used in systems that use persistency. In fact, with sockets, theorically both computers are present during the communication (even if there are ways to use them asynchronously). How the sockets work, however, is hidden from our eyes. Let's see Berkeley Sockets. Suppose that a client wants to communicate with a server. What does it do? he calls a system call, that is a socket. [A socket it's like the plug of a cable. You need two of them for the communication to actually work.] When we call the socket primitive, the same thing as "opening a file" happens: the system returns us a socket descriptor for that specific socket. Each process in unix has three file descriptor, always: stdin, stout and stderror. When asking the system to open a socket, three file descriptors are returned. They are used to read and write on the "cable". Something else has to be done in the system: the descriptors are numbers really, so we can't use them for communication. So, after opening a socket, we perform a \b bind\b0 . It associated to the number of the socket a port, a free one. We are literally "binding on a port", a free one. In general, the bind operation will take as argument a specific port, if we want to specify it. We have then the \b listen\b0  function. It allows to open the connection by communicating with the TCP protocol (for example). It basically says: I'm a server, and i can receive many requests from different clients. So, please, OS, organize your memory so that I can receive different requests and process them (put them in a queue if necessary). This is NOT a persistent mechanism: remember that anyway the server needs to be up and running to do this. So, now the client calls a function called \b connect\b0 , that takes as arguments the IP and the port of the server. The connect, through the stack TCP-IP, is able to connect with the TCP of the receiving machine, that delivers the request to the server (if there is space to do it). If there is space from the listen, the request is queued. If there is an error, anyway, the connect fails. If it doesn't fail, anyway, the connect is accepted (\b accept\b0 ), and \b read \b0 and \b write\b0  operations can be performed. Note: in the client, there is still a bind, but it's hidden. The application, the programmer even, doesn0t need to know the port used by the client for communicating with the server. We just want a free port. In fact, we just need the IP and Port of both parts of the communication to fully describe it. At this point, it's all about our choice, that is, we can define the format of the messages and similar things. This is a mechanism that doesn't work if the server is not up and running anyway. Sometimes, we would like to have asynchronous mechanisms or some kind of persistent system. In those cases, we have a queuing layer. The queuing layer is between the sender and the local OS, both in the client and server side. It receives messages and keeps them until they can be processed. But what happens if the whoule receiver node is down? That the server can't get the message, so, we have no service. So, what those systems do usually is to use different nodes on the network to conserve the queues. In the message-queuing model, we have some primitives and their meaning in the slide. Put, Get, Poll (check in a queue if there is a message and remove it, but don't block) and another one. \par
The intermediate nodes that implement the queuing system, have also another role. In some intermediate processes, we have a so-called Broker program. He not only has to keep the messages when the destination is down. He also needs to convert some informations: if the two machines "talk in a different way", the broker needs to convert a message into another language when needed. These kind of systems, with a queue, are useful when we want asynchronous communication, and scalability can be increased by admitting delays in requests and responses. But also when the procucer is faster than the consumer, and when implementing a publish-subscribe communication pattern. The messaging protocols we will be seeing are XMPP (maybe?), MQTT, that is a very lightweight/efficient protocol for machine-to-machine communication (also used for IOT) and AMQP, that is more complete and belongs to the ISO standard. This is suitable for a lot of more different infrastructures. Some popular message-broker softwares are Eclipse mosquitto (supports the MQTT protocol, is opensource and lightweight), RabbitMQ (it's very popular and partially opensource, and also supports AMQP and other protocols), Google Cloud Pub/Sub, Amazon MQ and Apache ActiveMQ and Kafka. [All those things like RabbitMQ implement middleware for queue messaging].\par
In RabbitMQ, there are different modalities of use. In the Topic modality, that is the most commonly used, the messages are put in a queue, and then the consumer receives them. The receiver is just one.\par
The role of the broker is the important thing: we want to implement persistent communication, and without the broker we can't do this.\par
REMOTE PROCEDURE CALL.\par
What is difficult in making this remote? When we work with data structures in a program, sometimens we use functions, that take arguments. When we pass the arguments to a function call, we usually pass some variables that hold a value. The same goes for constant values. In those cases, the RPC is easy: i just forward to the remote machine the function name and the parameters. The problem is that sometimes we pass a memory address to a function. We can't do this directly in RPC. The remote machine doesn't know my local addresses.\par
\par
\par
\par
\par
\par
\par
\par
}
 