{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1040{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.22000}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\qc\f0\fs44\lang16 GPU Computing - appunti\par

\pard\sa200\sl276\slmult1\qj\fs22 Lezione 1 - 28/02/2022\par
\par
Oggi le GPU sono dappertutto. Quello che vogliamo fare in questo corso \'e8 imparare a programmare su GPU. L'idea \'e8 sviscerare il 90% di quello che serve per usare almeno una famiglia di GPU. [Lui \'e8 un Mentana della GPU]. Le lezioni avranno una prima parte di teoria e una seconda di laboratorio.\par
HPC - Hyper Performance Computing: che cos'\'e8 e perch\'e9 ci serve.\par
Ci focalizzeremo sulle GPU di Nvidia. Hanno il merito di aver fornito un paradigma di programmazione general purpose a tutto il mondo. La cosa figa \'e8 che si basa sul C, che gi\'e0 conosciamo. Vedremo inoltre algoritmi relativi a questo mondo.\par
Kilo Mega Giga Tera Peta Exa: una piccola scala di grandezze per i dati. Il punto \'e8 che esistono dei super computer che fanno Exa- operazioni al secondo (exaflops). Queste enormi quantit\'e0 di informazioni, ad oggi, possono essere processate, ma \'e8 possibile solo con macchine apposite. L'AI \'e8 il pi\'f9 grande utilizzatore di GPU, perch\'e9 devono fae operazioni ad hoc e che richiedono molte operazioni al secondo. Il Parallel Computing \'e8 un termine ombrello con cui si intende l'esecuzione di pi\'f9 task paralleli o gerarchici, ma comunque, sotto qualche forma, indipendenti. Praticamente ogni processore fa qualcosa, dato che i dati sono molti e di grandi dimensioni. Non solo serve capacit\'e0 di calcolo ma anche capacit\'e0 di coordinazione o comunicazione intra-processi/core/unit\'e0 di calcolo di varia natura. \par
E la CPU, in tutto questo, dove va? La GPU \'e8 molto pi\'f9 potente dopotutto, ma si usa ancora. Ma...\par
Alcuni valori critici per la CPU sono la potenza di clock: entro un tot non ci si fa, scalderebbe troppo. Oppure la tensione di alimentazione. \par
All'efficienza insomma ci pensa la GPU. Cos\'ec viene portata avanti la legge di Moore e si introduce l'idea di avere un'unit\'e0 multi-core: ne hanno migliaia di core, le GPU, mentre le CPU ne hanno 2,4,8... Le GPU si usano soprattutto per problemi altamente parallelizzabili. Diciamo che le GPU, dal punto di vista della legge di Moore, sono le nuove CPU.\par
Le GPU si usano anche per il ray-tracing. Per ogni pixel bisogna calcolare da dove arriva la luce, dove riflette, il suo materiale, riflettanza, trasparenza, etc..\par
Il prodotto di due matrici \'e8 la stessa operazione fatta tante volte. Basta prendere una riga, una colonna, e calcolare un elemento della nuova matrice. La provenienza dei dati \'e8 comune, ma i calcoli possono essere parallelizzati. \par
Comunque, noi ci occuperemo di Cuda: Compute Unified Device Architecture, \'e8 un'archtettura general-purpose per il calcolo parallelo per GPU Nvidia.\par
Vedremo insomma il General-purpose GPU. La nostra idea \'e8 usare sinergicamente la GPU e la CPU. Tutto quello che pu\'f2 essere eseguito sequenzialmente lo diamo alla CPU, mentre le cose pi\'f9 pesanti e parallelizzabili le diamo alla GPU. Inoltre, dal punto di vista dell'utente, succederanno le stesse cose che succedevano avando solo la CPU, ma andremo pi\'f9 veloci in quanto abbiamo una GPU. Possiamo immaginare di continuare a lavorare sequenzialmente, ma laddove \'e8 possibile parallelizzare, parallelizziamo. Tuttavia pensiamo sempre in termini sequenziali. \par
La CPU \'e8 chiamata host e la GPU invece device. Avremo inoltre un bus attreverso cui i dati vanno dalla CPU alla GPU e poi viceversa. L'idea \'e8 sempre quella di avere un'applicazione che parte sulla CPU, ma poi una parte dei calcoli vanno a finire nella GPU.\par
Se abbiamo pi\'f9 processori, abbiamo tante unit\'e0 che devono eseguire flussi di istruzioni indipendenti. Quindi, se riusciamo a prendere il problema e a suddividerlo in pi\'f9 parti separate che possono essere parallelizzate, possiamo dare ognuna di queste parti a un processore diverso.\par
\par
Architettura e modelli di GPU (Nvidia)\par
SM = Streaming Progessor. Ci sta facendo vedere le diverse architetture Nvidia.  Scriveremo sempre codice tale per cui un numero di thread multiplo di 32 verr\'e0 eseguito.\par
Lavoreremo con le CUDA runtime API, anche se potremmo usare pure le CUDA driver APi, che per\'f2 sono pi\'f9 complicate.\par
\par
Lezione 2 - 07/03/2022\par
Oggi vediamo alcuni richiami al parallelismo e ai modelli di sistemi di computazione paralleli. Passeremo per il multithreading, come funziona in Unix, come vengono implementati e arriveremo infine nella CUDA zone con le GPU e il suo modello di programmazione.\par
Un modello di programmazione parallela rappresenta un'astrazione per un sistema di calcolo parallelo in cui \'e8 conveniente esprimere algoitmi concorrenti o paralleli. Spesso si lavora ad alto livello con un linguaggio che astrae molto, mentre altre volte conviene scendere a livelli pi\'f9 bassi e stare vicini alla macchina. Si pu\'f2 addirittura programmare in assembly. Come modelli di basso livelli ce ne sono vari, ma noi non li vedremo per il modello CUDA. Possiamo individuare 4 livelli di astrazione:\par
-Livello macchina: livello pi\'f9 basso. Si parla direttamente con l'hardware e il sistema operativo.\par
-Modello architetturale: rete di interconnessioni di piattaforme parallele, organizzazione della memoria e livelli di sincronizzazione tra processi, modalit\'e0 di esecuzione delle istruzioni di tipo SIMD o MIMD. Si parla insomma di data center e work station per esempio.\par
-Modello computazionale: qui si studiano la computabilit\'e0 e la complessit\'e0 di vari algoritmi. Quali problemi ammettono algoritmi in grado di risolverli con risorse polinomiali? Cosa facciamo se abbiamo limitate risorse di tempo e spazio? Alcuni esempi sono PRAM e RAM.\par
-Modello di programmazione parallela: in questo caso ci si occupa di definire e usare strumenti come librerie e tool di profilazione, dove si usa un linguaggio con una chiara semantica operazionale. Ci permette di specificare la tipologia delle computazioni parallele. Permette di dare specifiche implicite o esplicite per il parallelismo. Etc.\par
\par
Vediamo, a livello di GPU, elementi di multi-core e multi-threading. L'idea \'e8 che abbiamo un processo in esecuzione con le sue risorse allocate, come stack, heap, registri, programma sorgente, etc.. Un processo single-threaded pu\'f2 eseguire un'attivit\'e0 alla volta. Tuttavia, un processo pu\'f2 essere composto di pi\'f9 thread, che condividono lo stesso (esclusivo) spazio di indirizzamento. Un solo processore pu\'f2 eseguire una moltitudine di processi in esecuzione, che subiscono un context switch (round-robin). Questa cosa in CUDA non c'\'e8! I thread possono essere generati da un processo, e all'inizio il nuovo thread \'e8 un processo clone del padre, ma poi cambia. Ogni thread ha un suo insieme di registri e un suo statk, mentre il codice sorgente, i dati e i file sono condivisi tra thread. Ogni processo ha il proprio contensto, ovvero process ID, progra counter, stato dei registri, stack, codice, dati, file descriptor etc. I thread che genera hanno un loro flusso di istruzione che verr\'e0 eseguito dallo scheduler, e pi\'f9 thread possono essere eseguiti in parallelo. Ogni thread \'e8 parallelo rispetto agli altri. Il programmatore, essenzialmente, si occupa di costruire i thread, attribuir loro un compito e gestirne la comunicazione. Quando generiamo un thread, questo entra in una macchina a stati finiti, e il suo stato cambia in base ad alcuni eventi. Il thread generato viene fornito a uno scheduler che lo eseguir\'e0 quando possibile. Il primo stato in cui va \'e8 executable: pu\'f2 essere eseguibile. Pu\'f2 poi passare in running, e se si blocca va in stato di waiting. Quando si sveglia, va nello stato executable. Se \'e8 in running, pu\'f2 finire e andare nello stato finale finished. Ci sono pro e contro nell'uso dei thread.Tra i pro abbiamo che la condivisione degli oggetti \'e8 semplificata, ovvero ci sono dati di natura globale. Abbiamo inoltre pi\'f9 flussi di esecuzione, quindi andiamo pi\'f9 veloce. Le comunicazioni sono veloci, lo spazio di indirizzamento \'e8 lo stesso, e il context switch \'e8 piuttosto veloce (l'ambiente, in buona parte, viene mantenuto). Svantaggi: concorrenza, perch\'e9 ci deve essere mutua escluzione, e ci sono difficolt\'e0 legate alle risorse private. \par
\par
Un kernel prende una sola griglia. Un kernel \'e8 la funzione che esegui in parallelo (con <<<grid, block>>>() ). \par
\par
I kernel visti fino ad ora accedono solo alla memoria device, deve restituire un void, non supporta-\par
\par
Lezione 3 - 14/03/2022\par
cudaMalloc, cudaMemcpy, cudaMemset e cudaFree sono usate per allocare memoria sul device. L'allocazione sulla GPU avviene in maniera sincrona, perch\'e9 necessita di una comunicazione con la CPU.\par
L'allocazione su host e device sono due cose distinte. Innanzitutto, su usano puntatori diversi per le diverse allocazioni (fanno riferimento a spazi di indirizzamento diversi, d'altronde sono memorie distinte). \par
Con void*, C ci permette di fare casting da e verso ogni tipo. Alla slide 56 un esempio di corretta allocazione della memoria in wrapper di malloc.\par
cudaMemcpy(destinazione, sorgente, numero di byte, identificatore operazione).\par
\par
Un blocco viene mappato su uno streaming multiprocessor. Un thread va su un core, un blocco su uno streaming multiprocessor e un grid su tutto.\par
Una cosa interessante \'e8 che, in generale, definiamo un warp come un insieme di 32 thread (con ID consecutivi). Idealmente, questi 32 thread sono eseguiti SIMT, ovvero tutti loro eseguono la stessa istruzione.\par
L'architettura per\'f2 tiene i warp uniti in un unico blocco. Ebbene, ogni blocco lo possiamo vedere come un insieme di warp. La corrispondenza fisica sta nel fatto che in ogni Streaming Multiprocessors ci sono 32 core (non \'e8 sempre cos\'ec per\'f2, in alcune architetture \'e8 cos\'ec). Se un blocco ha pi\'f9 warp, questi vengono schedulati (cit. Davide). \par
\par
Lezione 4 - 21/03/2022\par
Ci sono operazioni atomiche in CUDA che eseguono solo operazioni matematiche, ma senza interruzione da parte di altri thread. Nel caso del calcolo di istogrammi di immagini, le operazioni atomiche sono utili.\par
Si pu\'f2 usare python per programmare CUDA usando dei decoratori, che portano il codice dall'interprete python al compilatore CUDA. I kernel vengono scritti in Numba, che si appoggia su NumPy, libreria matematica estremamente efficiente. Gli array numPy vengono trasferiti automaticamente da CPU a GPU. \par
JIT compiler: siamo dotati di uno strumento che fa interpretazione in tempo reale. Nel nostro caso, questo vuol dire che, se l'interprete trova decoratori particolari nel parsing del file, fa cose diverse, ovvero nel nostro caso cambiare ad esempio il compilatore usato. Numba si appoggia a un compilatore C e il compilatore cuda nvcc. Per il codice non relativo alla GPU, si usa LLVM. Numba \'e8 improntato all'uso di librerie come Numpy. \par
Il decoratore @numba.jit praticamente prende la funzione e la trasforma in un eseguibile (binario), che viene cachato e poi usato. Serve a compilare una funzione come se fosse in C (noi per\'f2 non veniamo nulla, \'e8 tutto gestito a runtime da python).\par
A noi interessa il backend per CUDA, dove usiamo semplicemente @guda.jit (sottopacchetto di CUDA) come decoratore. Quando decoro una function in questo modo, sono in grado di interpretarla come kernel. I kernel si lanciano come kernel[nBlocks, nThreads](args). Come al solito:\line I kernel non possono restituire nulla.\line Se vogliamo un risultato, dobbiamo passargli un array che lo conservi\par
La prima analisi delle performance non conta dato che c'\'e8 anche la compilazione.\par
\par
Lezione 5 - 28/03/2022\par
Elementi base dell'architettura GPU, le varie famiglie GPU e l'evoluzione delle (micro)architetture, prestazioni legate alle risorse archiettturali, loop unrolling per incrementare le prestaizoni migliorando il parallelismo sulla base dell'architettura sottostante, parallelismo dinamici e ancora MQDB. Vedremo anche come allocare kernel direttamente dalla GPU e i vantaggi che offre questa capacit\'e0. \par
Architetture:\line La CPU e la GPU (host e device) comunicano mediante il PCI express, che \'e8 una tencologia che evolve nel tempo. Ha ad oggi un'ampia bandwidth. Il PCI expresso 2.0 pu\'f2 in teoria trasmettere fino a 500 MB/s, questo per ogni lane (fili di comunicazione). I fili possono essere 1,4,8 o 16. In pratica, le GPU possono ricevere fino a 8 GB di memoria mediante un PCIe.\line I controller di memoria sono due, uno sull'host e uno sul device, e comunicano mediante il PCI express. Le architetture dei due anche sono diverse, e vedremo anche le architetture di memoria.\line La memoria della GPU viene chiamata Global Memory, ed \'e8 "off-chip": con un bus comunica col device. Nella GPU abbiamo due livelli di memoria: L'L2 cache che si interfaccia con la Global Memory, i Core e l'I/O controller. Tutto questo \'e8 importante perch\'e9 spesso bisogna ragionare anche dei dati che vanno trasmessi da host a device e viceversa. Nel 2016, per i supercomputer, NVidia ha sviluppato bus con una bandwidth da 80 GB/s. Insomma, si cerca di aumentarla a dismisura. La storia delle architetture Nvidia parte nel 2006 con l'architettura hw Tesla, che nel modello consumer \'e8 la GeForce 8800 GTX (G80). noi vedremo pi\'f9 che altro la Kepler, la terza generazione. \line Queste architetture hw sono contraddistinde da una coppia di numeri, in particolare dalla Compute Capability, termine che descrive la versione hardware degli acceleratori GPU. Inoltre, a seconda della generazione, cambia il numero di core in ogni SM, le dimensioni delle cache, etc.. le capability comunque si distinguono principalmente per queste caratteristiche. Major Revision Number: se due device hanno lo stesso, la loro architettura base \'e8 la stessa.\line Gi\'e0 dalla 3.5 (Kepler) abbiamo il dynamic parallelism, e nella prosisma lezione vedremo anche la shared memory. \line Vediamo l'architettura della Fermi. abbiamo 6 SM per un totale di 192 core (6 * 32). C'\'e8 un'interfaccia verso l'host, un thread scheduler e una gerarchia di memoria: una Global Memory con una memoria da 1 GB, una cache L2 e un memory controller. La cache L2 \'e8 quella di ultimo livello, e parla con la memoria Globale e ha lo stesso spazio di indirizzamento della Global Memory. Le cache L1 invece sono locali ai singoli SM. Queste sono molto pi\'f9 piccole, ma sono parallele. Le "scatole rosse" sono le unit\'e0 I/O che accedono a 16 core contemporaneamente per caricarli di dati.\line L'host si interfaccia col bus PCI. \line E il Giga Thread scheduler? Si tratta di un oggetto che gestisce l'arrivo di pi\'f9 blocchi: associa ogni blocco a un Sm tra quelli disponibili, con una politica Round Robin. Ogni SM ha un numero massimo di blocchi che pu\'f2 avere assegnati (\b non tutti in esecuzione nello stesso istante\b0 ). L'esecuzione dei blocchi \'e8 molto pi\'f9 lenta rispetto ad assegnare un blocco a un SM. Ogni core, quando esefue un thread, conosce i suoi indici (glieli dice lo scheduler). Vediamo ora lo scheduler di warp.\line I blocchi, dopo essere stati assegnati, vengono scomposti in warp, lo sappiamo. Esistono una coppia di Warp Scheduelr (talvolta sono 4) che si occupano di schedulare i warp e due instruction dispatch unit. \line Con la GF110, sempre Fermi, sono aumentati gli SM e le dimensioni delle varie memorie. E' l'esponente pi\'f9 forte della famiglia Fermi. Ha 32 core per SM, 128KB di register file che contengono 32K blocchi da 32 bit, 4 Special Function Units (SFU) per SM, 16 unit\'e0 di Load e Store (LD/ST), e 64 KB di cache L1 (shared memory sembra).\line Cosa ha introdotto Fermi di particolare? Beh innanzitutto ha reso la GPU un'entit\'e0 condivisa tra pi\'f9 host in caso di contesto multiutente. Inoltre supporta l'esecuzione concorrente di kernel, ovvero pi\'f9 kernel possono essere eseguiti in parallelo sulla stessa GPU (verranno gestiti da uno scheduler). Il che risulta in meno tempo di esecuzione. \par
Arriviamo all'architettura Kepler: Lo SM ora si chiama SMX (non si sa perch\'e9), e sono cambiati anche i contenuti degli SM. Ci sono proprio anche unit\'e0 funzionali distinte al loro interno, la cache L2 \'e8 raddoppiata. Il Giga Thread Scheduler ora \'e8 chiamato Giga Thread Engine, e soprattutto gli SMX contengono diversi tipi di unit\'e0 di esecuzione, chiamate DPU, Double Precision Unit. Con la doppia precisione, queste GPU entrano nell'et\'e0 adulta. I core sono sestuplicati: ogni SMX ha 192 core. Anche il register file \'e8 raddoppiato. \par
Maxwell: le maxwell includono 6 Graphics Processing Clusters, dove ciascuna di questi hanno 4 SMMs (ovvero SM). Purtroppo le Maxwell non ebbero tutto sto gran successo (sembra).\par
Pascal: ha aumentato il throughput (e bandwidth) di un sacco, e anche qui ci sono unit\'e0 di calcolo Floating Point. Arriva a 16 GB di memoria device, ed ha aumentato di molto la bandwidth grazie a un cambiamento microarchitetturale (questa bandwidth \'e8 relativa alla comunicazione memoria device e memoria host). Anche gli SM sono stati potenziati. Ci\'f2 che accomuna queste architetture \'e8 un principio fondamentale: noi possiamo prendere un kernel cuda scritto in una certa versione e farlo funzionare in tutte le versioni architetturali successive. Funzioner\'e0 sempre. Il nostro codice, e il suo design, \'e8 indipendente dall'architettura sottostante. Questa dovrebbe essere la cosiddetta Transparent Scalability. Vediamo pi\'f9 nello specifico alla doppia precisione, le Double Precision Units. Una DPU \'e8 leggermente pi\'f9 grande di un core, e la dimensione fisica di queste unit\'e0 aumenta quadraticamente col numero di bit (? comunque con la richiesta di precisione).\line Citiamo l'Hyper-Q: se abbiamo una singola coda in cui piazzare le griglie accodate (i kernel really), questi verranno eseguiti sequenzialmente, per forza. \par
Come possiamo sapere la scheda che abbiamo sottomano? Con delle API. cudaGetDeviceCount ci dice quante GPU abbiamo. Va da 0 a n-1 se ho n schede collegate al bus PCI. Quindi posso mandare diversi kernel in esecuzione su pi\'f9 device. Con cudaGetDeviceProperties si possono ottenere informazioni sulla scheda scelta. \par
LOOP UNROLLING: cercare di ottimizzare l'esecuzione di un loop riducento la frequenza di istruzioni di branching e loop maintenance. Insomma, il costo della valutazione della clausola e dell'aumento della variabile (pensa a un for) sono costi gratuiti.\par
La lezione di oggi \'e8 che, facendo le cose ad hoc, perdiamo di generalit\'e0 ma guadagnamo in efficienza.\par
PARALLELISMO DINAMICO: finora eravamo abituati a lanciare kernel dall'host, mai dal device. Da Kepler in poi invece si potevano lanciare altri kernel da un kernel per portare a termine delle computazioni. Questo permette molta pi\'f9 flessibilit\'e0, perch\'e9 magari le nostre operazioni dipendono dai dati che abbiamo appena processato. Insomma, a runtime decidiamo il da farsi. Questo ci permette anche di creare kernel ricorsivi. \par
\par
Lezione 6 - 04/04/2022\par
ArgoVisions (maggio). \par
Oggi cominciamo a parlare del modello di memoria CUDA. Vediamo oggi la memoria shared, poi vedremo quella globale. Prima della shared, faremo per\'f2 un elenco delle memorie, ovvero la collezione di memorie che si possono usare in CUDA. Vedremo, della shared, come sono organizzati fisicamente i dati, come vi si accede, i migliori pattern di accesso, etc.. Vedremo anche parallel reduction + unrolling + shared memory. \line Memorie: le gerarchie di memoria non sono cambiate tanto negli ultimi anni, anche se i dati sono diventati "big". Abbiamo oggi bisogno di muovere dati velocemente con bandwidth e throughput sempre pi\'f9 alti. Ad oggi, tenere strutture dati in memoria centrale non \'e8 difficile, ma un tempo lo era. La limitazione maggiore infatti \'e8 data dalla latenza. Nella gerarchia di memoria troviamo registri, cache, main memory e disk memory. Le primo sono pi\'f9 piccole e veloci, e ultime pi\'f9 lente e grandi. Se troviamo le DRAM maggiormente nelle CPU, nelle GPU abbiamo invece pi\'f9 SRAM. Siccome c'\'e8 un condensatore alla base delle DRAM, quando facciamo una lettura dobbiamo aspettare un po', mentre le SRAM, in termini di memoria centrale, sono migliori. In ogni gerarchia di memoria si usano la localit\'e0 temporale e spaziale per accelerare i processi di accesso alla memoria.\line Il modello di memoria cuda prevede ci\'f2 che segue. Quando parlaimo di un blocco, questo si trova dentro un SM (nella slide 7 vediamo uno spaccato di uno SM). La shared memory infatti appartiene a un blocco, e c'\'e8 anche una local memory (non \'e8 fisica) relativa a ogni thread. La local memory pu\'f2 essere immaginata come la memoria locale proprio di ogni function, o singolo kernel (le variabili temporanee insomma). Ci sono poi altre tre memorie "esterne", ovvero la Global, la Constant e la Texture. Nell'immagine, il block pu\'f2 essere paragonato a una SM. Ah, e per ogni thread, abbiamo anche dei registri.\par
Registri: memorie pi\'f9 veloci in assoluto e sono legati ai thread. int i = 1, i diventa una variabile automatica del thread (che va in un registro). Ha delle limitazioni: il numero di registri disponibili per thread \'e8 limitato. E' una risorsa piccola e condivisa, quindi non va abusata. Ogni variabile che non dichiariamo __device__, __shared__ o __constant__ risieder\'e0 in un registro. Tanti meno registri sono unsati da un kernel, tanti pi\'f9 thread posso avere sullo stesso SM. Tante pi\'f9 variabili locali creiamo nel thread, tanti meno thread possiamo creare (se vogliamo essere efficienti) per blocco.\par
Possiamo noi stessi limitare il numero massimo di registri per thread, usando l'opzione -maxrregcount. Il nostro obiettivo sar\'e0 tenere alta l'occupancy. Si pu\'f2 minimizzare lo spilling usando il qualificatore __launch_bounds__ nella definizione del kernel.\par
LOCAL MEMORY: a dispetto del nome, si tratta di una memoria che fisicamente sta nella global memory, in praticamente tutte le architetture. E' quindi lenta come la global memory, o device memory, e quindi ha latenza ampia. Cosa ci si mette dentro? Variabili automatiche di grandi dimensioni. Insomma, o cose troppo grandi, o cose non definite a tempo di compilazione. Insomma, possiamo gi\'e0 sapere se una variabile andr\'e0 a finire da una parte o dall'altra. La local memory sfrutta i dati posti nelle cache L1 e L2. \par
CONSTANT MEMORY: si tratta di una memoria di 64K per ogni famiglia di GPU, ed ha una cache dedicata in ogni SM. Si tratta di una memoria asimmetrica: \'e8 a sola lettura. la scriviamo una sola volta e poi i thread la leggono. A cosa serve? Pensiamo a un meccanismo multicast: tutti i thread di un warp accedono a dati utili a tutti i thread. Tipo, se abbiamo un polinomio, \'e8 utile tenere i coefficienti in questa memoria. Come si inizializza la memoria? dall'host. Va pre-caricata prima di invocare un qualsiasi kernel. cudaError_t cudaMemcpyToSymbol. Come detto, la constant memory lavora bene quando tutti i thread di un warp devono leggere dallo stesso indirizzo di memoria. Se invece ogni thread accede a una locazione diversa di questa memoria, di fatto si sequenzializza l'accesso.\par
TEXTURE MEMORY: memoria con hardware dedicato per fare efficientemente filtraggio e interpolazione lineare. Di solito queste operazioni si fanno su immagini e simili. Sono molto efficaci per la gestione di array 2D (immagini). Non ne parleremo, ma possiamo informarci.\par
La global memory la vedremo la prossima volta.\par
CACHE GPU: pi\'f9 logiche che fisiche. Si tratta delle cache L1 e L2, la memoria costante e quella texture. Da Maxwell in poi la texture era condivisa con la L1.\par
Abbiamo gi\'e0 parlato di occupancy, ma ora la decliniamo sulle memorie. L'occupancy \'e8 definita come il rapporto tra warp attivi e massimo numero di warp. Quindi, tutto \'e8 rapportato a questo denominatore. Dobbiamo cercare di avvicinarci a lui, in termini di warp attivi. Cio\'e8 che limita tuttavia il numero di warp attivi sono la shared memori o numero di thread (la memoria shared \'e8 condivisa tra blocchi), i registri (quantit\'e0 divisa tra i thread), E infine numero di slot: numero cio\'e8 di block e warp. Alcuni limiti insomma sono imposti dall'architettura, altri invece sono imposti da noi se non lavoriamo bene.\par
Arriviao a un in-depth della shared memory. Vediamo come impatta. La shared memory (SMEM) \'e8 una soft cach, in contrapposizione alle hard cache L1 e L2. Insomma, nella SMEM, accadono delle cose di cui l'hardware non \'e8 conscio. Si tratta di una memoria per elaborare dati on-chip e migliorare i pattern di accesso alla blobal memory. Se noi dobbiamo accedere pi\'f9 volte ad un dato, se lo mettiamo in memoria globale e vi accediamo pi\'f9 volte, non siamo tanto efficienti. Se mettiamo quel dato in SMEM, invece, abbiamo un dato disponibile a tutti i thread di un blocco, e vi accediamo pi\'f9 velocemente. Il vantaggio \'e8 che riduciamo di 30 volte la latenza e di 10 volte la Bandwidth. Questa SMEM \'e8 on-chip (piccola, circa 64K) e in alcune architetture condivisa con la L1 (quindi ANCORA pi\'f9 piccola). E' suddivisa in moduli della stassa ampiezza, chiamati bank (sono 32). L'ideale \'e8 prendere 32 dati dai 32 bank, un dato per bank. Se succede, bom, siamo super efficienti. La memoria shared \'e8 intesa come condivisa tra i thread di un blocco, essendo condivisa c'\'e8 bisogno di sincronizzazione tra i dati in memoria, e quindi syncthread. La SMEM dell'architettura Fermi si suddivide in blocchi da 4 byte (chiamate word), che possono contenere vari dati. Se si vuole accedere un byte, bisogna per forza "pupparsi" l'intera word. \par
Molteplici accessi allo stesso bank vengono serializzati. L'accesso ad un singolo dato avviene invece in una sola transazione.\par
\par
Lezione 7 - 11/04/2022\par
Convoluzione (dalla precedente lezione, in verit\'e0). Si tratta dela somma tra segnali di durata finita, x ed h. Il problema \'e8 che \'e8 una sommatoria pesante. Il problema del bordo a quanto pare \'e8 molto importante. \par
GLOBAL MEMORY\par
\par
Lezione 8 - 02/05/2022\par
Stream e concorrenza: ulteriore grado di concorrenza. La prossima la dedichiamo alle librerie, quella dopo ancora pattern paralleli.\line Gradi di concorrenza in CUDA: spesso le operazioni parallele coinvolgono anche il trasferimento di dati, che possono essere pi\'f9 costose delle operazioni vere e proprie. Cercheremo quindi di rendere simultanei calcoli e trasporto dei dati. Gli stream in realt\'e0 li abbiamo gi\'e0 usati in piccolo, ma ora li vediamo meglio. Assieme agli stream vediamo anche gli "eventi", che si gestiscono tramite API e che permettono di mettere marker negli stream. Questi marker permettono, tra le altre cose, sincronizzazioni nei flussi.\line Nel flip orizzontale di un'immagine, la maggior parte del costo risiede nel trasferimento dati, ad esempio. Misureremo infatti sia i tempi di trasferimento che quelli di esecuzione. Vediamo i gradi di concorrenza in CUDA.\line La concorrenza CPU e GPU consiste nel fatto che la CPU e la GPU possono operare indipendentemente tra loro. Tramite DMA, e il protocollo di gestione del bus PCI, il trasferimento dati pu\'f2 essere parallelo con l'esecuzione degli SMs. Inoltre, dalla compute capability 3.x si ha che l'hardwARE PU\'f2 ESEGUIRE FINO A 32 KERNEL IN PARALLELO ANCHE DA THREAD DISTINTI. La concorrenza a livello di grid risiede invece nell'uso di stream multipli per operazioni indipendenti. C'\'e8 infine una concorrenza aggiuntiva data dalla presenza di pi\'f9 GPU che operano in parallelo. \par
Rivediamo il sincronismo nel codice GPU e CPU.\line Nella GPU, a livello di warp, l'esecuzione \'e8 effettivamente sincrona. Diversi warp vengono eseguiti con sovrapposizione arbitraria, anche se con syncthreads si pu\'f2 assicurare il giusto comportamento di un blocco di thread. Inoltre, i blocchi di thread sono distinti, e vengono eseguiti arbitrariamente. \par
All'interno di uno stream, mettiamo una serie di operazioni CUDA (usando le API) che vengono eseguite in modo FIFO, in modo parallelo. \par
\par
Lezione 9 - 09/05/2022\par
The Turn.\par
Vediamo le librerie CUDA. cuBLAS, cuRAND e cuFFT. \par
cudaSetMatrix: il terzultimo e il penultimo argomento sono il numero di righe delle due matrici (A e B, sorgente e destinazione). \par
\par
\par
\par
\ul\par
\ulnone\par
\par
\par
}
 