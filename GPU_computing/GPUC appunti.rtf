{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1040{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.22000}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\qc\f0\fs44\lang16 GPU Computing - appunti\par

\pard\sa200\sl276\slmult1\qj\fs22 Lezione 1 - 28/02/2022\par
\par
Oggi le GPU sono dappertutto. Quello che vogliamo fare in questo corso \'e8 imparare a programmare su GPU. L'idea \'e8 sviscerare il 90% di quello che serve per usare almeno una famiglia di GPU. [Lui \'e8 un Mentana della GPU]. Le lezioni avranno una prima parte di teoria e una seconda di laboratorio.\par
HPC - Hyper Performance Computing: che cos'\'e8 e perch\'e9 ci serve.\par
Ci focalizzeremo sulle GPU di Nvidia. Hanno il merito di aver fornito un paradigma di programmazione general purpose a tutto il mondo. La cosa figa \'e8 che si basa sul C, che gi\'e0 conosciamo. Vedremo inoltre algoritmi relativi a questo mondo.\par
Kilo Mega Giga Tera Peta Exa: una piccola scala di grandezze per i dati. Il punto \'e8 che esistono dei super computer che fanno Exa- operazioni al secondo (exaflops). Queste enormi quantit\'e0 di informazioni, ad oggi, possono essere processate, ma \'e8 possibile solo con macchine apposite. L'AI \'e8 il pi\'f9 grande utilizzatore di GPU, perch\'e9 devono fae operazioni ad hoc e che richiedono molte operazioni al secondo. Il Parallel Computing \'e8 un termine ombrello con cui si intende l'esecuzione di pi\'f9 task paralleli o gerarchici, ma comunque, sotto qualche forma, indipendenti. Praticamente ogni processore fa qualcosa, dato che i dati sono molti e di grandi dimensioni. Non solo serve capacit\'e0 di calcolo ma anche capacit\'e0 di coordinazione o comunicazione intra-processi/core/unit\'e0 di calcolo di varia natura. \par
E la CPU, in tutto questo, dove va? La GPU \'e8 molto pi\'f9 potente dopotutto, ma si usa ancora. Ma...\par
Alcuni valori critici per la CPU sono la potenza di clock: entro un tot non ci si fa, scalderebbe troppo. Oppure la tensione di alimentazione. \par
All'efficienza insomma ci pensa la GPU. Cos\'ec viene portata avanti la legge di Moore e si introduce l'idea di avere un'unit\'e0 multi-core: ne hanno migliaia di core, le GPU, mentre le CPU ne hanno 2,4,8... Le GPU si usano soprattutto per problemi altamente parallelizzabili. Diciamo che le GPU, dal punto di vista della legge di Moore, sono le nuove CPU.\par
Le GPU si usano anche per il ray-tracing. Per ogni pixel bisogna calcolare da dove arriva la luce, dove riflette, il suo materiale, riflettanza, trasparenza, etc..\par
Il prodotto di due matrici \'e8 la stessa operazione fatta tante volte. Basta prendere una riga, una colonna, e calcolare un elemento della nuova matrice. La provenienza dei dati \'e8 comune, ma i calcoli possono essere parallelizzati. \par
Comunque, noi ci occuperemo di Cuda: Compute Unified Device Architecture, \'e8 un'archtettura general-purpose per il calcolo parallelo per GPU Nvidia.\par
Vedremo insomma il General-purpose GPU. La nostra idea \'e8 usare sinergicamente la GPU e la CPU. Tutto quello che pu\'f2 essere eseguito sequenzialmente lo diamo alla CPU, mentre le cose pi\'f9 pesanti e parallelizzabili le diamo alla GPU. Inoltre, dal punto di vista dell'utente, succederanno le stesse cose che succedevano avando solo la CPU, ma andremo pi\'f9 veloci in quanto abbiamo una GPU. Possiamo immaginare di continuare a lavorare sequenzialmente, ma laddove \'e8 possibile parallelizzare, parallelizziamo. Tuttavia pensiamo sempre in termini sequenziali. \par
La CPU \'e8 chiamata host e la GPU invece device. Avremo inoltre un bus attreverso cui i dati vanno dalla CPU alla GPU e poi viceversa. L'idea \'e8 sempre quella di avere un'applicazione che parte sulla CPU, ma poi una parte dei calcoli vanno a finire nella GPU.\par
Se abbiamo pi\'f9 processori, abbiamo tante unit\'e0 che devono eseguire flussi di istruzioni indipendenti. Quindi, se riusciamo a prendere il problema e a suddividerlo in pi\'f9 parti separate che possono essere parallelizzate, possiamo dare ognuna di queste parti a un processore diverso.\par
\par
Architettura e modelli di GPU (Nvidia)\par
SM = Streaming Progessor. Ci sta facendo vedere le diverse architetture Nvidia.  Scriveremo sempre codice tale per cui un numero di thread multiplo di 32 verr\'e0 eseguito.\par
Lavoreremo con le CUDA runtime API, anche se potremmo usare pure le CUDA driver APi, che per\'f2 sono pi\'f9 complicate.\par
\par
Lezione 2 - 07/03/2022\par
Oggi vediamo alcuni richiami al parallelismo e ai modelli di sistemi di computazione paralleli. Passeremo per il multithreading, come funziona in Unix, come vengono implementati e arriveremo infine nella CUDA zone con le GPU e il suo modello di programmazione.\par
Un modello di programmazione parallela rappresenta un'astrazione per un sistema di calcolo parallelo in cui \'e8 conveniente esprimere algoitmi concorrenti o paralleli. Spesso si lavora ad alto livello con un linguaggio che astrae molto, mentre altre volte conviene scendere a livelli pi\'f9 bassi e stare vicini alla macchina. Si pu\'f2 addirittura programmare in assembly. Come modelli di basso livelli ce ne sono vari, ma noi non li vedremo per il modello CUDA. Possiamo individuare 4 livelli di astrazione:\par
-Livello macchina: livello pi\'f9 basso. Si parla direttamente con l'hardware e il sistema operativo.\par
-Modello architetturale: rete di interconnessioni di piattaforme parallele, organizzazione della memoria e livelli di sincronizzazione tra processi, modalit\'e0 di esecuzione delle istruzioni di tipo SIMD o MIMD. Si parla insomma di data center e work station per esempio.\par
-Modello computazionale: qui si studiano la computabilit\'e0 e la complessit\'e0 di vari algoritmi. Quali problemi ammettono algoritmi in grado di risolverli con risorse polinomiali? Cosa facciamo se abbiamo limitate risorse di tempo e spazio? Alcuni esempi sono PRAM e RAM.\par
-Modello di programmazione parallela: in questo caso ci si occupa di definire e usare strumenti come librerie e tool di profilazione, dove si usa un linguaggio con una chiara semantica operazionale. Ci permette di specificare la tipologia delle computazioni parallele. Permette di dare specifiche implicite o esplicite per il parallelismo. Etc.\par
\par
Vediamo, a livello di GPU, elementi di multi-core e multi-threading. L'idea \'e8 che abbiamo un processo in esecuzione con le sue risorse allocate, come stack, heap, registri, programma sorgente, etc.. Un processo single-threaded pu\'f2 eseguire un'attivit\'e0 alla volta. Tuttavia, un processo pu\'f2 essere composto di pi\'f9 thread, che condividono lo stesso (esclusivo) spazio di indirizzamento. Un solo processore pu\'f2 eseguire una moltitudine di processi in esecuzione, che subiscono un context switch (round-robin). Questa cosa in CUDA non c'\'e8! I thread possono essere generati da un processo, e all'inizio il nuovo thread \'e8 un processo clone del padre, ma poi cambia. Ogni thread ha un suo insieme di registri e un suo statk, mentre il codice sorgente, i dati e i file sono condivisi tra thread. Ogni processo ha il proprio contensto, ovvero process ID, progra counter, stato dei registri, stack, codice, dati, file descriptor etc. I thread che genera hanno un loro flusso di istruzione che verr\'e0 eseguito dallo scheduler, e pi\'f9 thread possono essere eseguiti in parallelo. Ogni thread \'e8 parallelo rispetto agli altri. Il programmatore, essenzialmente, si occupa di costruire i thread, attribuir loro un compito e gestirne la comunicazione. Quando generiamo un thread, questo entra in una macchina a stati finiti, e il suo stato cambia in base ad alcuni eventi. Il thread generato viene fornito a uno scheduler che lo eseguir\'e0 quando possibile. Il primo stato in cui va \'e8 executable: pu\'f2 essere eseguibile. Pu\'f2 poi passare in running, e se si blocca va in stato di waiting. Quando si sveglia, va nello stato executable. Se \'e8 in running, pu\'f2 finire e andare nello stato finale finished. Ci sono pro e contro nell'uso dei thread.Tra i pro abbiamo che la condivisione degli oggetti \'e8 semplificata, ovvero ci sono dati di natura globale. Abbiamo inoltre pi\'f9 flussi di esecuzione, quindi andiamo pi\'f9 veloce. Le comunicazioni sono veloci, lo spazio di indirizzamento \'e8 lo stesso, e il context switch \'e8 piuttosto veloce (l'ambiente, in buona parte, viene mantenuto). Svantaggi: concorrenza, perch\'e9 ci deve essere mutua escluzione, e ci sono difficolt\'e0 legate alle risorse private. \par
\par
Un kernel prende una sola griglia. Un kernel \'e8 la funzione che esegui in parallelo (con <<<grid, block>>>() ). \par
\par
I kernel visti fino ad ora accedono solo alla memoria device, deve restituire un void, non supporta-\par
\par
Lezione 3 - 14/03/2022\par
cudaMalloc, cudaMemcpy, cudaMemset e cudaFree sono usate per allocare memoria sul device. L'allocazione sulla GPU avviene in maniera sincrona, perch\'e9 necessita di una comunicazione con la CPU.\par
L'allocazione su host e device sono due cose distinte. Innanzitutto, su usano puntatori diversi per le diverse allocazioni (fanno riferimento a spazi di indirizzamento diversi, d'altronde sono memorie distinte). \par
Con void*, C ci permette di fare casting da e verso ogni tipo. Alla slide 56 un esempio di corretta allocazione della memoria in wrapper di malloc.\par
cudaMemcpy(destinazione, sorgente, numero di byte, identificatore operazione).\par
\par
Un blocco viene mappato su uno streaming multiprocessor. Un thread va su un core, un blocco su uno streaming multiprocessor e un grid su tutto.\par
Una cosa interessante \'e8 che, in generale, definiamo un warp come un insieme di 32 thread (con ID consecutivi). Idealmente, questi 32 thread sono eseguiti SIMT, ovvero tutti loro eseguono la stessa istruzione.\par
L'architettura per\'f2 tiene i warp uniti in un unico blocco. Ebbene, ogni blocco lo possiamo vedere come un insieme di warp. La corrispondenza fisica sta nel fatto che in ogni Streaming Multiprocessors ci sono 32 core (non \'e8 sempre cos\'ec per\'f2, in alcune architetture \'e8 cos\'ec). Se un blocco ha pi\'f9 warp, questi vengono schedulati (cit. Davide). \par
\par
Lezione 4 - 21/03/2022\par
Ci sono operazioni atomiche in CUDA che eseguono solo operazioni matematiche, ma senza interruzione da parte di altri thread. Nel caso del calcolo di istogrammi di immagini, le operazioni atomiche sono utili.\par
Si pu\'f2 usare python per programmare CUDA usando dei decoratori, che portano il codice dall'interprete python al compilatore CUDA. I kernel vengono scritti in Numba, che si appoggia su NumPy, libreria matematica estremamente efficiente. Gli array numPy vengono trasferiti automaticamente da CPU a GPU. \par
JIT compiler: siamo dotati di uno strumento che fa interpretazione in tempo reale. Nel nostro caso, questo vuol dire che, se l'interprete trova decoratori particolari nel parsing del file, fa cose diverse, ovvero nel nostro caso cambiare ad esempio il compilatore usato. Numba si appoggia a un compilatore C e il compilatore cuda nvcc. Per il codice non relativo alla GPU, si usa LLVM. Numba \'e8 improntato all'uso di librerie come Numpy. \par
Il decoratore @numba.jit praticamente prende la funzione e la trasforma in un eseguibile (binario), che viene cachato e poi usato. Serve a compilare una funzione come se fosse in C (noi per\'f2 non veniamo nulla, \'e8 tutto gestito a runtime da python).\par
A noi interessa il backend per CUDA, dove usiamo semplicemente @guda.jit (sottopacchetto di CUDA) come decoratore. Quando decoro una function in questo modo, sono in grado di interpretarla come kernel. I kernel si lanciano come kernel[nBlocks, nThreads](args). Come al solito:\line I kernel non possono restituire nulla.\line Se vogliamo un risultato, dobbiamo passargli un array che lo conservi\par
La prima analisi delle performance non conta dato che c'\'e8 anche la compilazione.\par
\par
Lezione 5 - 28/03/2022\par
Elementi base dell'architettura GPU, le varie famiglie GPU e l'evoluzione delle (micro)architetture, prestazioni legate alle risorse archiettturali, loop unrolling per incrementare le prestaizoni migliorando il parallelismo sulla base dell'architettura sottostante, parallelismo dinamici e ancora MQDB. Vedremo anche come allocare kernel direttamente dalla GPU e i vantaggi che offre questa capacit\'e0. \par
Architetture:\line La CPU e la GPU (host e device) comunicano mediante il PCI express, che \'e8 una tencologia che evolve nel tempo. Ha ad oggi un'ampia bandwidth. Il PCI expresso 2.0 pu\'f2 in teoria trasmettere fino a 500 MB/s, questo per ogni lane (fili di comunicazione). I fili possono essere 1,4,8 o 16. In pratica, le GPU possono ricevere fino a 8 GB di memoria mediante un PCIe.\line I controller di memoria sono due, uno sull'host e uno sul device, e comunicano mediante il PCI express. Le architetture dei due anche sono diverse, e vedremo anche le architetture di memoria.\line La memoria della GPU viene chiamata Global Memory, ed \'e8 "off-chip": con un bus comunica col device. Nella GPU abbiamo due livelli di memoria: L'L2 cache che si interfaccia con la Global Memory, i Core e l'I/O controller. Tutto questo \'e8 importante perch\'e9 spesso bisogna ragionare anche dei dati che vanno trasmessi da host a device e viceversa. Nel 2016, per i supercomputer, NVidia ha sviluppato bus con una bandwidth da 80 GB/s. Insomma, si cerca di aumentarla a dismisura. La storia delle architetture Nvidia parte nel 2006 con l'architettura hw Tesla, che nel modello consumer \'e8 la GeForce 8800 GTX (G80). noi vedremo pi\'f9 che altro la Kepler, la terza generazione. \line Queste architetture hw sono contraddistinde da una coppia di numeri, in particolare dalla Compute Capability, termine che descrive la versione hardware degli acceleratori GPU. Inoltre, a seconda della generazione, cambia il numero di core in ogni SM, le dimensioni delle cache, etc.. le capability comunque si distinguono principalmente per queste caratteristiche. Major Revision Number: se due device hanno lo stesso, la loro architettura base \'e8 la stessa.\line Gi\'e0 dalla 3.5 (Kepler) abbiamo il dynamic parallelism, e nella prosisma lezione vedremo anche la shared memory. \line Vediamo l'architettura della Fermi. abbiamo 6 SM per un totale di 192 core (6 * 32). C'\'e8 un'interfaccia verso l'host, un thread scheduler e una gerarchia di memoria: una Global Memory con una memoria da 1 GB, una cache L2 e un memory controller. La cache L2 \'e8 quella di ultimo livello, e parla con la memoria Globale e ha lo stesso spazio di indirizzamento della Global Memory. Le cache L1 invece sono locali ai singoli SM. Queste sono molto pi\'f9 piccole, ma sono parallele. Le "scatole rosse" sono le unit\'e0 I/O che accedono a 16 core contemporaneamente per caricarli di dati.\line L'host si interfaccia col bus PCI. \line E il Giga Thread scheduler? Si tratta di un oggetto che gestisce l'arrivo di pi\'f9 blocchi: associa ogni blocco a un Sm tra quelli disponibili, con una politica Round Robin. Ogni SM ha un numero massimo di blocchi che pu\'f2 avere assegnati (\b non tutti in esecuzione nello stesso istante\b0 ). L'esecuzione dei blocchi \'e8 molto pi\'f9 lenta rispetto ad assegnare un blocco a un SM. Ogni core, quando esefue un thread, conosce i suoi indici (glieli dice lo scheduler). Vediamo ora lo scheduler di warp.\line I blocchi, dopo essere stati assegnati, vengono scomposti in warp, lo sappiamo. Esistono una coppia di Warp Scheduelr (talvolta sono 4) che si occupano di schedulare i warp e due instruction dispatch unit. \line Con la GF110, sempre Fermi, sono aumentati gli SM e le dimensioni delle varie memorie. E' l'esponente pi\'f9 forte della famiglia Fermi. Ha 32 core per SM, 128KB di register file che contengono 32K blocchi da 32 bit, 4 Special Function Units (SFU) per SM, 16 unit\'e0 di Load e Store (LD/ST), e 64 KB di cache L1 (shared memory sembra).\line Cosa ha introdotto Fermi di particolare? Beh innanzitutto ha reso la GPU un'entit\'e0 condivisa tra pi\'f9 host in caso di contesto multiutente. Inoltre supporta l'esecuzione concorrente di kernel, ovvero pi\'f9 kernel possono essere eseguiti in parallelo sulla stessa GPU (verranno gestiti da uno scheduler). Il che risulta in meno tempo di esecuzione. \par
Arriviamo all'architettura Kepler: Lo SM ora si chiama SMX (non si sa perch\'e9), e sono cambiati anche i contenuti degli SM. Ci sono proprio anche unit\'e0 funzionali distinte al loro interno, la cache L2 \'e8 raddoppiata. Il Giga Thread Scheduler ora \'e8 chiamato Giga Thread Engine, e soprattutto gli SMX contengono diversi tipi di unit\'e0 di esecuzione, chiamate DPU, Double Precision Unit. Con la doppia precisione, queste GPU entrano nell'et\'e0 adulta. I core sono sestuplicati: ogni SMX ha 192 core. Anche il register file \'e8 raddoppiato. \par
Maxwell: le maxwell includono 6 Graphics Processing Clusters, dove ciascuna di questi hanno 4 SMMs (ovvero SM). Purtroppo le Maxwell non ebbero tutto sto gran successo (sembra).\par
Pascal: ha aumentato il throughput (e bandwidth) di un sacco, e anche qui ci sono unit\'e0 di calcolo Floating Point. Arriva a 16 GB di memoria device, ed ha aumentato di molto la bandwidth grazie a un cambiamento microarchitetturale (questa bandwidth \'e8 relativa alla comunicazione memoria device e memoria host). Anche gli SM sono stati potenziati. Ci\'f2 che accomuna queste architetture \'e8 un principio fondamentale: noi possiamo prendere un kernel cuda scritto in una certa versione e farlo funzionare in tutte le versioni architetturali successive. Funzioner\'e0 sempre. Il nostro codice, e il suo design, \'e8 indipendente dall'architettura sottostante. Questa dovrebbe essere la cosiddetta Transparent Scalability. Vediamo pi\'f9 nello specifico alla doppia precisione, le Double Precision Units. Una DPU \'e8 leggermente pi\'f9 grande di un core, e la dimensione fisica di queste unit\'e0 aumenta quadraticamente col numero di bit (? comunque con la richiesta di precisione).\line Citiamo l'Hyper-Q: se abbiamo una singola coda in cui piazzare le griglie accodate (i kernel really), questi verranno eseguiti sequenzialmente, per forza. \par
Come possiamo sapere la scheda che abbiamo sottomano? Con delle API. cudaGetDeviceCount ci dice quante GPU abbiamo. Va da 0 a n-1 se ho n schede collegate al bus PCI. Quindi posso mandare diversi kernel in esecuzione su pi\'f9 device. Con cudaGetDeviceProperties si possono ottenere informazioni sulla scheda scelta. \par
LOOP UNROLLING: cercare di ottimizzare l'esecuzione di un loop riducento la frequenza di istruzioni di branching e loop maintenance. Insomma, il costo della valutazione della clausola e dell'aumento della variabile (pensa a un for) sono costi gratuiti.\par
La lezione di oggi \'e8 che, facendo le cose ad hoc, perdiamo di generalit\'e0 ma guadagnamo in efficienza.\par
PARALLELISMO DINAMICO: finora eravamo abituati a lanciare kernel dall'host, mai dal device. Da Kepler in poi invece si potevano lanciare altri kernel da un kernel per portare a termine delle computazioni. Questo permette molta pi\'f9 flessibilit\'e0, perch\'e9 magari le nostre operazioni dipendono dai dati che abbiamo appena processato. Insomma, a runtime decidiamo il da farsi. Questo ci permette anche di creare kernel ricorsivi. \par
\par
Lezione 6 - 04/04/2022\par
ArgoVisions (maggio). \par
Oggi cominciamo a parlare del modello di memoria CUDA. Vediamo oggi la memoria shared, poi vedremo quella globale. Prima della shared, faremo per\'f2 un elenco delle memorie, ovvero la collezione di memorie che si possono usare in CUDA. Vedremo, della shared, come sono organizzati fisicamente i dati, come vi si accede, i migliori pattern di accesso, etc..\par
\par
\par
\ul\par
\ulnone\par
\par
\par
}
 