{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1040{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.22000}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\qc\f0\fs44\lang16 GPU Computing - appunti\par

\pard\sa200\sl276\slmult1\qj\fs22 Lezione 1 - 28/02/2022\par
\par
Oggi le GPU sono dappertutto. Quello che vogliamo fare in questo corso \'e8 imparare a programmare su GPU. L'idea \'e8 sviscerare il 90% di quello che serve per usare almeno una famiglia di GPU. [Lui \'e8 un Mentana della GPU]. Le lezioni avranno una prima parte di teoria e una seconda di laboratorio.\par
HPC - Hyper Performance Computing: che cos'\'e8 e perch\'e9 ci serve.\par
Ci focalizzeremo sulle GPU di Nvidia. Hanno il merito di aver fornito un paradigma di programmazione general purpose a tutto il mondo. La cosa figa \'e8 che si basa sul C, che gi\'e0 conosciamo. Vedremo inoltre algoritmi relativi a questo mondo.\par
Kilo Mega Giga Tera Peta Exa: una piccola scala di grandezze per i dati. Il punto \'e8 che esistono dei super computer che fanno Exa- operazioni al secondo (exaflops). Queste enormi quantit\'e0 di informazioni, ad oggi, possono essere processate, ma \'e8 possibile solo con macchine apposite. L'AI \'e8 il pi\'f9 grande utilizzatore di GPU, perch\'e9 devono fae operazioni ad hoc e che richiedono molte operazioni al secondo. Il Parallel Computing \'e8 un termine ombrello con cui si intende l'esecuzione di pi\'f9 task paralleli o gerarchici, ma comunque, sotto qualche forma, indipendenti. Praticamente ogni processore fa qualcosa, dato che i dati sono molti e di grandi dimensioni. Non solo serve capacit\'e0 di calcolo ma anche capacit\'e0 di coordinazione o comunicazione intra-processi/core/unit\'e0 di calcolo di varia natura. \par
E la CPU, in tutto questo, dove va? La GPU \'e8 molto pi\'f9 potente dopotutto, ma si usa ancora. Ma...\par
Alcuni valori critici per la CPU sono la potenza di clock: entro un tot non ci si fa, scalderebbe troppo. Oppure la tensione di alimentazione. \par
All'efficienza insomma ci pensa la GPU. Cos\'ec viene portata avanti la legge di Moore e si introduce l'idea di avere un'unit\'e0 multi-core: ne hanno migliaia di core, le GPU, mentre le CPU ne hanno 2,4,8... Le GPU si usano soprattutto per problemi altamente parallelizzabili. Diciamo che le GPU, dal punto di vista della legge di Moore, sono le nuove CPU.\par
Le GPU si usano anche per il ray-tracing. Per ogni pixel bisogna calcolare da dove arriva la luce, dove riflette, il suo materiale, riflettanza, trasparenza, etc..\par
Il prodotto di due matrici \'e8 la stessa operazione fatta tante volte. Basta prendere una riga, una colonna, e calcolare un elemento della nuova matrice. La provenienza dei dati \'e8 comune, ma i calcoli possono essere parallelizzati. \par
Comunque, noi ci occuperemo di Cuda: Compute Unified Device Architecture, \'e8 un'archtettura general-purpose per il calcolo parallelo per GPU Nvidia.\par
Vedremo insomma il General-purpose GPU. La nostra idea \'e8 usare sinergicamente la GPU e la CPU. Tutto quello che pu\'f2 essere eseguito sequenzialmente lo diamo alla CPU, mentre le cose pi\'f9 pesanti e parallelizzabili le diamo alla GPU. Inoltre, dal punto di vista dell'utente, succederanno le stesse cose che succedevano avando solo la CPU, ma andremo pi\'f9 veloci in quanto abbiamo una GPU. Possiamo immaginare di continuare a lavorare sequenzialmente, ma laddove \'e8 possibile parallelizzare, parallelizziamo. Tuttavia pensiamo sempre in termini sequenziali. \par
La CPU \'e8 chiamata host e la GPU invece device. Avremo inoltre un bus attreverso cui i dati vanno dalla CPU alla GPU e poi viceversa. L'idea \'e8 sempre quella di avere un'applicazione che parte sulla CPU, ma poi una parte dei calcoli vanno a finire nella GPU.\par
Se abbiamo pi\'f9 processori, abbiamo tante unit\'e0 che devono eseguire flussi di istruzioni indipendenti. Quindi, se riusciamo a prendere il problema e a suddividerlo in pi\'f9 parti separate che possono essere parallelizzate, possiamo dare ognuna di queste parti a un processore diverso.\par
\par
Architettura e modelli di GPU (Nvidia)\par
SM = Streaming Progessor. Ci sta facendo vedere le diverse architetture Nvidia.  Scriveremo sempre codice tale per cui un numero di thread multiplo di 32 verr\'e0 eseguito.\par
Lavoreremo con le CUDA runtime API, anche se potremmo usare pure le CUDA driver APi, che per\'f2 sono pi\'f9 complicate.\par
\par
Lezione 2 - 07/03/2022\par
Oggi vediamo alcuni richiami al parallelismo e ai modelli di sistemi di computazione paralleli. Passeremo per il multithreading, come funziona in Unix, come vengono implementati e arriveremo infine nella CUDA zone con le GPU e il suo modello di programmazione.\par
Un modello di programmazione parallela rappresenta un'astrazione per un sistema di calcolo parallelo in cui \'e8 conveniente esprimere algoitmi concorrenti o paralleli. Spesso si lavora ad alto livello con un linguaggio che astrae molto, mentre altre volte conviene scendere a livelli pi\'f9 bassi e stare vicini alla macchina. Si pu\'f2 addirittura programmare in assembly. Come modelli di basso livelli ce ne sono vari, ma noi non li vedremo per il modello CUDA. Possiamo individuare 4 livelli di astrazione:\par
-Livello macchina: livello pi\'f9 basso. Si parla direttamente con l'hardware e il sistema operativo.\par
-Modello architetturale: rete di interconnessioni di piattaforme parallele, organizzazione della memoria e livelli di sincronizzazione tra processi, modalit\'e0 di esecuzione delle istruzioni di tipo SIMD o MIMD. Si parla insomma di data center e work station per esempio.\par
-Modello computazionale: qui si studiano la computabilit\'e0 e la complessit\'e0 di vari algoritmi. Quali problemi ammettono algoritmi in grado di risolverli con risorse polinomiali? Cosa facciamo se abbiamo limitate risorse di tempo e spazio? Alcuni esempi sono PRAM e RAM.\par
-Modello di programmazione parallela: in questo caso ci si occupa di definire e usare strumenti come librerie e tool di profilazione, dove si usa un linguaggio con una chiara semantica operazionale. Ci permette di specificare la tipologia delle computazioni parallele. Permette di dare specifiche implicite o esplicite per il parallelismo. Etc.\par
\par
Vediamo, a livello di GPU, elementi di multi-core e multi-threading. L'idea \'e8 che abbiamo un processo in esecuzione con le sue risorse allocate, come stack, heap, registri, programma sorgente, etc.. Un processo single-threaded pu\'f2 eseguire un'attivit\'e0 alla volta. Tuttavia, un processo pu\'f2 essere composto di pi\'f9 thread, che condividono lo stesso (esclusivo) spazio di indirizzamento. Un solo processore pu\'f2 eseguire una moltitudine di processi in esecuzione, che subiscono un context switch (round-robin). Questa cosa in CUDA non c'\'e8! I thread possono essere generati da un processo, e all'inizio il nuovo thread \'e8 un processo clone del padre, ma poi cambia. Ogni thread ha un suo insieme di registri e un suo statk, mentre il codice sorgente, i dati e i file sono condivisi tra thread. Ogni processo ha il proprio contensto, ovvero process ID, progra counter, stato dei registri, stack, codice, dati, file descriptor etc. I thread che genera hanno un loro flusso di istruzione che verr\'e0 eseguito dallo scheduler, e pi\'f9 thread possono essere eseguiti in parallelo. Ogni thread \'e8 parallelo rispetto agli altri. Il programmatore, essenzialmente, si occupa di costruire i thread, attribuir loro un compito e gestirne la comunicazione. Quando generiamo un thread, questo entra in una macchina a stati finiti, e il suo stato cambia in base ad alcuni eventi. Il thread generato viene fornito a uno scheduler che lo eseguir\'e0 quando possibile. Il primo stato in cui va \'e8 executable: pu\'f2 essere eseguibile. Pu\'f2 poi passare in running, e se si blocca va in stato di waiting. Quando si sveglia, va nello stato executable. Se \'e8 in running, pu\'f2 finire e andare nello stato finale finished. Ci sono pro e contro nell'uso dei thread.Tra i pro abbiamo che la condivisione degli oggetti \'e8 semplificata, ovvero ci sono dati di natura globale. Abbiamo inoltre pi\'f9 flussi di esecuzione, quindi andiamo pi\'f9 veloce. Le comunicazioni sono veloci, lo spazio di indirizzamento \'e8 lo stesso, e il context switch \'e8 piuttosto veloce (l'ambiente, in buona parte, viene mantenuto). Svantaggi: concorrenza, perch\'e9 ci deve essere mutua escluzione, e ci sono difficolt\'e0 legate alle risorse private. \par
\par
Un kernel prende una sola griglia. Un kernel \'e8 la funzione che esegui in parallelo (con <<<grid, block>>>() ). \par
\par
I kernel visti fino ad ora accedono solo alla memoria device, deve restituire un void, non supporta-\par
\par
Lezione 3 - 14/03/2022\par
cudaMalloc, cudaMemcpy, cudaMemset e cudaFree sono usate per allocare memoria sul device. L'allocazione sulla GPU avviene in maniera sincrona, perch\'e9 necessita di una comunicazione con la CPU.\par
L'allocazione su host e device sono due cose distinte. Innanzitutto, su usano puntatori diversi per le diverse allocazioni (fanno riferimento a spazi di indirizzamento diversi, d'altronde sono memorie distinte). \par
Con void*, C ci permette di fare casting da e verso ogni tipo. Alla slide 56 un esempio di corretta allocazione della memoria in wrapper di malloc.\par
cudaMemcpy(destinazione, sorgente, numero di byte, identificatore operazione).\par
\par
Un blocco viene mappato su uno streaming multiprocessor. Un thread va su un core, un blocco su uno streaming multiprocessor e un grid su tutto.\par
Una cosa interessante \'e8 che, in generale, definiamo un warp come un insieme di 32 thread (con ID consecutivi). Idealmente, questi 32 thread sono eseguiti SIMT, ovvero tutti loro eseguono la stessa istruzione.\par
L'architettura per\'f2 tiene i warp uniti in un unico blocco. Ebbene, ogni blocco lo possiamo vedere come un insieme di warp. La corrispondenza fisica sta nel fatto che in ogni Streaming Multiprocessors ci sono 32 core (non \'e8 sempre cos\'ec per\'f2, in alcune architetture \'e8 cos\'ec). Se un blocco ha pi\'f9 warp, questi vengono schedulati (cit. Davide). \par
\par
\par
\par
\par
\par
\par
\par
}
 