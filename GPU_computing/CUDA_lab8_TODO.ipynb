{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUDA_lab8_TODO.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "WoJbB3T5Vkw-",
        "zs_a5Vuimily",
        "iUYP4kCJhEIx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **LAB 8 - CUDA Streams**\n",
        "---"
      ],
      "metadata": {
        "id": "fZYqN0UwVLC_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoJbB3T5Vkw-"
      },
      "source": [
        "# ▶️ CUDA setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fht2Wy8wVkxJ"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jP2H_YJVkxJ"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [GPU Compute Capability](https://developer.nvidia.com/cuda-gpus)"
      ],
      "metadata": {
        "id": "VKbaxH9wWosO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cGSqZovVkxK"
      },
      "source": [
        "## NVCC Plugin for Jupyter notebook\n",
        "\n",
        "*Usage*:\n",
        "\n",
        "\n",
        "*   Load Extension `%load_ext nvcc_plugin`\n",
        "*   Mark a cell to be treated as cuda cell\n",
        "`%%cuda --name example.cu --compile false`\n",
        "\n",
        "**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n",
        "\n",
        "*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCVhMkqYVkxK"
      },
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6PDOytTVkxK"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bash and data setup"
      ],
      "metadata": {
        "id": "cReFlD-VRfZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Bash setup\n",
        "%%writefile /root/.bashrc\n",
        "\n",
        "# If not running interactively, don't do anything\n",
        "[ -z \"$PS1\" ] && return\n",
        "\n",
        "# don't put duplicate lines in the history. See bash(1) for more options\n",
        "# ... or force ignoredups and ignorespace\n",
        "HISTCONTROL=ignoredups:ignorespace\n",
        "\n",
        "# append to the history file, don't overwrite it\n",
        "shopt -s histappend\n",
        "\n",
        "# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)\n",
        "HISTSIZE=10000\n",
        "HISTFILESIZE=20000\n",
        "\n",
        "# check the window size after each command and, if necessary,\n",
        "# update the values of LINES and COLUMNS.\n",
        "shopt -s checkwinsize\n",
        "\n",
        "# make less more friendly for non-text input files, see lesspipe(1)\n",
        "[ -x /usr/bin/lesspipe ] && eval \"$(SHELL=/bin/sh lesspipe)\"\n",
        "\n",
        "PS1='\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ '\n",
        "\n",
        "# enable color support of ls and also add handy aliases\n",
        "if [ -x /usr/bin/dircolors ]; then\n",
        "    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\n",
        "    alias ls='ls --color=auto'\n",
        "    #alias dir='dir --color=auto'\n",
        "    #alias vdir='vdir --color=auto'\n",
        "\n",
        "    alias grep='grep --color=auto'\n",
        "    alias fgrep='fgrep --color=auto'\n",
        "    alias egrep='egrep --color=auto'\n",
        "fi\n",
        "\n",
        "# some more ls aliases\n",
        "alias ll='ls -lF'\n",
        "alias la='ls -A'\n",
        "alias l='ls -CF'\n",
        "\n",
        "# path setup\n",
        "export PATH=\"./:/usr/local/cuda/bin:$PATH\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "O8ICSyy8_GEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source /root/.bashrc"
      ],
      "metadata": {
        "id": "QxIfKO3Ghf7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GPUcomputing site on github..."
      ],
      "metadata": {
        "id": "IYG8Cv4bTzyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/giulianogrossi/GPUcomputing.git"
      ],
      "metadata": {
        "id": "E7jZmHjCT0vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some paths..."
      ],
      "metadata": {
        "id": "ZarLje6wR_Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path setup\n",
        "!mkdir -p /content/GPUcomputing/lab8\n",
        "%cd /content/GPUcomputing/lab8\n",
        "!mkdir -p sumArrayStream\n",
        "!mkdir -p tabularStream\n",
        "!mkdir -p MQDB_stream"
      ],
      "metadata": {
        "id": "tC-AaOJlkLOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ▶️ VS Code on Colab"
      ],
      "metadata": {
        "id": "zs_a5Vuimily"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab-ssh tunnel\n",
        "#@markdown Execute this cell to open the ssh tunnel. Check [colab-ssh documentation](https://github.com/WassimBenzarti/colab-ssh) for more details.\n",
        "\n",
        "# Install colab_ssh on google colab\n",
        "!pip install colab_ssh --upgrade\n",
        "\n",
        "from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n",
        "ssh_tunnel_password = \"gpu\" #@param {type: \"string\"}\n",
        "launch_ssh_cloudflared(password=ssh_tunnel_password)\n",
        "\n",
        "# Optional: if you want to clone a Github or Gitlab repository\n",
        "repository_url=\"https://github.com/giulianogrossi/GPUcomputing\" #@param {type: \"string\"}\n",
        "init_git_cloudflared(repository_url)"
      ],
      "metadata": {
        "id": "BCf9JxqphHAp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUYP4kCJhEIx"
      },
      "source": [
        "# ▶️ DeviceQuery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW9b_Yuxi7id"
      },
      "source": [
        "# DeviceQuery dell'attuale device (su Colab!)\n",
        "!nvcc /content/GPUcomputing/utils/deviceQuery.cu -o deviceQuery\n",
        "!./deviceQuery"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check whether the device can transfer in both directions simultaneously"
      ],
      "metadata": {
        "id": "J-EcbVrAYlfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(void) {\n",
        "\n",
        "  cudaDeviceProp dProp;\n",
        "\tcudaGetDeviceProperties(&dProp, 0);\n",
        "\n",
        "  // Shows whether the device can transfer in both directions simultaneously\n",
        "  printf(\"Device %s capable of simultaneous CPU-to-GPU and GPU-to-CPU datatransfers\\n\", dProp.deviceOverlap ? \"IS\": \"NOT\");\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "esYDQTl-Yg3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAVvcKOX_DU0"
      },
      "source": [
        "# ✅ Somma array con stream"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example demonstrates overlapping computation and communication by\n",
        "partitioning a data set and asynchronously launching the memory copies and kernels for each subset. Launching all transfers and kernels for a given subset in the same CUDA stream ensures that computation on the device is not started until the necessary data has been transferred. However, because the work of each subset is independent of all other subsets, the communication and computation of different subsets will overlap.\n",
        "\n",
        "This example launches copies and kernels in breadth-first order."
      ],
      "metadata": {
        "id": "xpJ_WqJ-kJfy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH6TuWnB-_0M"
      },
      "source": [
        "%%writefile sumArrayStream/sumArrayStream.cu\n",
        "\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "#define NSTREAM 4\n",
        "#define BDIM 128\n",
        "\n",
        "void initialData(float *ip, int size) {\n",
        "  int i;\n",
        "\n",
        "  for(i = 0; i < size; i++)\n",
        "    ip[i] = (float)(rand() & 0xFF) / 10.0f;\n",
        "}\n",
        "\n",
        "void sumArraysOnHost(float *A, float *B, float *C, const int N) {\n",
        "  for (int idx = 0; idx < N; idx++)\n",
        "    C[idx] = A[idx] + B[idx];\n",
        "}\n",
        "\n",
        "__global__ void sumArrays(float *A, float *B, float *C, const int N) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < N)\n",
        "      for (int i = 0; i < N; ++i)\n",
        "        C[idx] = A[idx] + B[idx];\n",
        "}\n",
        "\n",
        "void checkResult(float *hostRef, float *gpuRef, const int N) {\n",
        "  double epsilon = 1.0E-8;\n",
        "  bool match = 1;\n",
        "\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n",
        "      match = 0;\n",
        "      printf(\"Arrays do not match!\\n\");\n",
        "      printf(\"host %5.2f gpu %5.2f at %d\\n\", hostRef[i], gpuRef[i], i);\n",
        "      break;\n",
        "    }\n",
        "  }\n",
        "  if (match) \n",
        "    printf(\"Arrays match.\\n\\n\");\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "  printf(\"> %s Starting...\\n\", argv[0]);\n",
        "\n",
        "  int dev = 0;\n",
        "  cudaDeviceProp deviceProp;\n",
        "  CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "  printf(\"> Using Device %d: %s\\n\", dev, deviceProp.name);\n",
        "  CHECK(cudaSetDevice(dev));\n",
        "\n",
        "  // check if device support hyper-q\n",
        "  if (deviceProp.major < 3 || (deviceProp.major == 3 && deviceProp.minor < 5)) {\n",
        "    if (deviceProp.concurrentKernels == 0) {\n",
        "      printf(\"> GPU does not support concurrent kernel execution (SM 3.5 or higher required)\\n\");\n",
        "      printf(\"> CUDA kernel runs will be serialized\\n\");\n",
        "    }\n",
        "    else {\n",
        "      printf(\"> GPU does not support HyperQ\\n\");\n",
        "      printf(\"> CUDA kernel runs will have limited concurrency\\n\");\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printf(\"> Compute Capability %d.%d hardware with %d multi-processors\\n\",\n",
        "          deviceProp.major, deviceProp.minor, deviceProp.multiProcessorCount);\n",
        "\n",
        "  // set up max connectioin\n",
        "  char iname[] = \"CUDA_DEVICE_MAX_CONNECTIONS\";\n",
        "  setenv (iname, \"1\", 1);\n",
        "  char *ivalue =  getenv (iname);\n",
        "  printf (\"> %s = %s\\n\", iname, ivalue);\n",
        "  printf (\"> with streams = %d\\n\", NSTREAM);\n",
        "\n",
        "  // set up data size of vectors\n",
        "  int nElem = 1 << 18;\n",
        "  printf(\"> vector size = %d\\n\", nElem);\n",
        "  size_t nBytes = nElem * sizeof(float);\n",
        "\n",
        "  // malloc pinned host memory for async memcpy\n",
        "  float *h_A, *h_B, *hostRef, *gpuRef;\n",
        "  CHECK(cudaHostAlloc((void**)&h_A, nBytes, cudaHostAllocDefault));\n",
        "  CHECK(cudaHostAlloc((void**)&h_B, nBytes, cudaHostAllocDefault));\n",
        "  CHECK(cudaHostAlloc((void**)&gpuRef, nBytes, cudaHostAllocDefault));\n",
        "  CHECK(cudaHostAlloc((void**)&hostRef, nBytes, cudaHostAllocDefault));\n",
        "\n",
        "  // initialize data at host side\n",
        "  initialData(h_A, nElem);\n",
        "  initialData(h_B, nElem);\n",
        "  memset(hostRef, 0, nBytes);\n",
        "  memset(gpuRef,  0, nBytes);\n",
        "\n",
        "  // add vector at host side for result checks\n",
        "  sumArraysOnHost(h_A, h_B, hostRef, nElem);\n",
        "\n",
        "  // malloc device global memory\n",
        "  float *d_A, *d_B, *d_C;\n",
        "  CHECK(cudaMalloc((float**)&d_A, nBytes));\n",
        "  CHECK(cudaMalloc((float**)&d_B, nBytes));\n",
        "  CHECK(cudaMalloc((float**)&d_C, nBytes));\n",
        "\n",
        "  cudaEvent_t start, stop;\n",
        "  CHECK(cudaEventCreate(&start));\n",
        "  CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "  // invoke kernel at host side\n",
        "  dim3 block (BDIM);\n",
        "  dim3 grid  ((nElem + block.x - 1) / block.x);\n",
        "  printf(\"> grid (%d, %d) block (%d, %d)\\n\", grid.x, grid.y, block.x, block.y);\n",
        "\n",
        "  // sequential operation\n",
        "  CHECK(cudaEventRecord(start, 0));\n",
        "  CHECK(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "  CHECK(cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice));\n",
        "  CHECK(cudaEventRecord(stop, 0));\n",
        "  CHECK(cudaEventSynchronize(stop));\n",
        "  float memcpy_h2d_time;\n",
        "  CHECK(cudaEventElapsedTime(&memcpy_h2d_time, start, stop));\n",
        "\n",
        "  CHECK(cudaEventRecord(start, 0));\n",
        "  sumArrays<<<grid, block>>>(d_A, d_B, d_C, nElem);\n",
        "  CHECK(cudaEventRecord(stop, 0));\n",
        "  CHECK(cudaEventSynchronize(stop));\n",
        "  float kernel_time;\n",
        "  CHECK(cudaEventElapsedTime(&kernel_time, start, stop));\n",
        "\n",
        "  CHECK(cudaEventRecord(start, 0));\n",
        "  CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n",
        "  CHECK(cudaEventRecord(stop, 0));\n",
        "  CHECK(cudaEventSynchronize(stop));\n",
        "  float memcpy_d2h_time;\n",
        "  CHECK(cudaEventElapsedTime(&memcpy_d2h_time, start, stop));\n",
        "  float itotal = kernel_time + memcpy_h2d_time + memcpy_d2h_time;\n",
        "\n",
        "  printf(\"\\n\");\n",
        "  printf(\"Measured timings (throughput):\\n\");\n",
        "  printf(\" Memcpy host to device\\t: %f ms (%f GB/s)\\n\", memcpy_h2d_time, (nBytes * 1e-6) / memcpy_h2d_time);\n",
        "  printf(\" Memcpy device to host\\t: %f ms (%f GB/s)\\n\", memcpy_d2h_time, (nBytes * 1e-6) / memcpy_d2h_time);\n",
        "  printf(\" Kernel\\t\\t\\t: %f ms (%f GB/s)\\n\", kernel_time, (nBytes * 2e-6) / kernel_time);\n",
        "  printf(\" Total\\t\\t\\t: %f ms (%f GB/s)\\n\", itotal, (nBytes * 2e-6) / itotal);\n",
        "\n",
        "  // grid parallel operation\n",
        "  int iElem = nElem / NSTREAM;\n",
        "  size_t iBytes = iElem * sizeof(float);\n",
        "  grid.x = (iElem + block.x - 1) / block.x;\n",
        "\n",
        "  cudaStream_t stream[NSTREAM];\n",
        "\n",
        "  for (int i = 0; i < NSTREAM; ++i)\n",
        "    CHECK(cudaStreamCreate(&stream[i]));\n",
        "\n",
        "  CHECK(cudaEventRecord(start, 0));\n",
        "\n",
        "  // initiate all asynchronous transfers to the device\n",
        "  for (int i = 0; i < NSTREAM; ++i) {\n",
        "    int ioffset = i * iElem;\n",
        "    CHECK(cudaMemcpyAsync(&d_A[ioffset], &h_A[ioffset], iBytes, cudaMemcpyHostToDevice, stream[i]));\n",
        "    CHECK(cudaMemcpyAsync(&d_B[ioffset], &h_B[ioffset], iBytes, cudaMemcpyHostToDevice, stream[i]));\n",
        "  }\n",
        "\n",
        "  // launch a kernel in each stream\n",
        "  for (int i = 0; i < NSTREAM; ++i) {\n",
        "    int ioffset = i * iElem;\n",
        "    sumArrays<<<grid, block, 0, stream[i]>>>(&d_A[ioffset], &d_B[ioffset], &d_C[ioffset], iElem);\n",
        "  }\n",
        "\n",
        "  // enqueue asynchronous transfers from the device\n",
        "  for (int i = 0; i < NSTREAM; ++i) {\n",
        "    int ioffset = i * iElem;\n",
        "    CHECK(cudaMemcpyAsync(&gpuRef[ioffset], &d_C[ioffset], iBytes, cudaMemcpyDeviceToHost, stream[i]));\n",
        "  }\n",
        "\n",
        "  CHECK(cudaEventRecord(stop, 0));\n",
        "  CHECK(cudaEventSynchronize(stop));\n",
        "  float execution_time;\n",
        "  CHECK(cudaEventElapsedTime(&execution_time, start, stop));\n",
        "\n",
        "  printf(\"\\n\");\n",
        "  printf(\"Actual results from overlapped data transfers:\\n\");\n",
        "  printf(\" overlap with %d streams : %f ms (%f GB/s)\\n\", NSTREAM, execution_time, (nBytes * 2e-6) / execution_time );\n",
        "  printf(\" speedup                : %f \\n\", ((itotal - execution_time) * 100.0f) / itotal);\n",
        "\n",
        "  // check kernel error\n",
        "  CHECK(cudaGetLastError());\n",
        "\n",
        "  // check device results\n",
        "  checkResult(hostRef, gpuRef, nElem);\n",
        "\n",
        "  // free device global memory\n",
        "  CHECK(cudaFree(d_A));\n",
        "  CHECK(cudaFree(d_B));\n",
        "  CHECK(cudaFree(d_C));\n",
        "\n",
        "  // free host memory\n",
        "  CHECK(cudaFreeHost(h_A));\n",
        "  CHECK(cudaFreeHost(h_B));\n",
        "  CHECK(cudaFreeHost(hostRef));\n",
        "  CHECK(cudaFreeHost(gpuRef));\n",
        "\n",
        "  // destroy events\n",
        "  CHECK(cudaEventDestroy(start));\n",
        "  CHECK(cudaEventDestroy(stop));\n",
        "\n",
        "  // destroy streams\n",
        "  for (int i = 0; i < NSTREAM; ++i)\n",
        "    CHECK(cudaStreamDestroy(stream[i]));\n",
        "\n",
        "  CHECK(cudaDeviceReset());\n",
        "  return(0);\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G91KROXBScK"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "\n",
        "!nvcc -arch=sm_37  sumArrayStream/sumArrayStream.cu  -o sumArray\n",
        "!./sumArray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXUIQkZLCTcG"
      },
      "source": [
        "# ✅ Tabular\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 TODO"
      ],
      "metadata": {
        "id": "ctZ-l_2hXvDj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y52R0d3CA50"
      },
      "source": [
        "%%writefile tabularStream/tabular.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "#define PI 3.141592f\n",
        "\n",
        "/*\n",
        " * Kernel: tabular function\n",
        " */\n",
        "__global__ void tabular(float *a, int n) {\n",
        "\tint i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\tif (i < n) {\n",
        "\t\tfloat x = PI * (float)i / (float)n;\n",
        "\t\tfloat s = sinf(x);\n",
        "\t\tfloat c = cosf(x);\n",
        "\t\ta[i] = sqrtf(abs(s * s - c * c));\n",
        "\t}\n",
        "}\n",
        "\n",
        "/*\n",
        " * Kernel: tabular function using streams\n",
        " */\n",
        "__global__ void tabular_streams(float *a, int n, int offset) {\n",
        "\tint i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\tint actual_index = i + offset;\n",
        "\tif(actual_index < n){\n",
        "\t\tfloat x = PI * (float)actual_index / (float)n;\n",
        "\t\tfloat s = sinf(x);\n",
        "\t\tfloat c = cosf(x);\n",
        "\t\ta[actual_index] = sqrtf(abs(s * s - c * c));\n",
        "\t}\n",
        "\t\n",
        "}\n",
        "\n",
        "/*\n",
        " * Error measure\n",
        " */\n",
        "float maxError(float *a, int n) {\n",
        "\tfloat maxE = 0;\n",
        "\tfor (int i = 0; i < n; i++) {\n",
        "\t\tfloat error = fabs(a[i] - 1.0f);\n",
        "\t\tif (error > maxE)\n",
        "\t\t\tmaxE = error;\n",
        "\t}\n",
        "\treturn maxE;\n",
        "}\n",
        "\n",
        "/*\n",
        " * Main: tabular function\n",
        " */\n",
        "int main(void) {\n",
        "\t\n",
        "  // main params\n",
        "  uint MB = 1024*1024; \n",
        "  uint n = 256*MB;\n",
        "\tint blockSize = 256;\n",
        "\tint nStreams = 8;\n",
        "\n",
        "\tint streamSize = n / nStreams;\n",
        "\tint streamBytes = streamSize * sizeof(float);\n",
        "\tint bytes = n * sizeof(float);\n",
        "\n",
        "\tint devId = 0;\n",
        "\tcudaDeviceProp prop;\n",
        "\tCHECK(cudaGetDeviceProperties(&prop, devId));\n",
        "\tprintf(\"Device : %s\\n\\n\", prop.name);\n",
        "\tCHECK(cudaSetDevice(devId));\n",
        "  printf(\"Array size   : %d\\n\", n);\n",
        "  printf(\"StreamSize   : %d\\n\", streamSize);\n",
        "  printf(\"Memory bytes : %d (MB)\\n\", bytes/MB);\n",
        "  printf(\"streamBytes  : %d (MB)\\n\", streamBytes/MB);\n",
        "\n",
        "\t// allocate pinned host memory and device memory\n",
        "\tfloat *a, *d_a;\n",
        "\tCHECK(cudaMallocHost((void**) &a, bytes));      // host pinned\n",
        "\tCHECK(cudaMalloc((void**) &d_a, bytes));        // device\n",
        "\n",
        "\tfloat ms; // elapsed time in milliseconds\n",
        "\n",
        "\t// create events and streams\n",
        "\tcudaEvent_t startEvent, stopEvent, dummyEvent;\n",
        "\tcudaStream_t stream[nStreams];\n",
        "\tCHECK(cudaEventCreate(&startEvent));\n",
        "\tCHECK(cudaEventCreate(&stopEvent));\n",
        "\tCHECK(cudaEventCreate(&dummyEvent));\n",
        "\tfor (int i = 0; i < nStreams; ++i)\n",
        "\t\tCHECK(cudaStreamCreate(&stream[i]));\n",
        "\n",
        "\t// baseline case - sequential transfer and execute\n",
        "\tmemset(a, 0, bytes);\n",
        "\n",
        "\t//creiamo i dati giusti\n",
        "\tint x = 0;\n",
        "\tfor(int i = 0; i < n; i++){\n",
        "\t\t\ta[i] = x;\n",
        "\t\t\tx += PI/n;\n",
        "\t}\n",
        "\n",
        "\tCHECK(cudaEventRecord(startEvent, 0));\n",
        "\tCHECK(cudaMemcpy(d_a, a, bytes, cudaMemcpyHostToDevice));\n",
        "\ttabular<<<n / blockSize, blockSize>>>(d_a, n);\n",
        "\tCHECK(cudaMemcpy(a, d_a, bytes, cudaMemcpyDeviceToHost));\n",
        "\tCHECK(cudaEventRecord(stopEvent, 0));\n",
        "\tCHECK(cudaEventSynchronize(stopEvent));\n",
        "\tCHECK(cudaEventElapsedTime(&ms, startEvent, stopEvent));\n",
        "\tprintf(\"\\nTime for sequential transfer and execute (ms): %f\\n\", ms);\n",
        "\tprintf(\"  max error: %e\\n\", maxError(a, n));\n",
        "\n",
        "\t// asynchronous version 1: loop over {copy, kernel, copy}\n",
        "\t\n",
        "\t//rimetto a posto la memoria\n",
        "\tmemset(a, 0, bytes);\n",
        "\tCHECK(cudaMemset(d_a, 0, bytes));\n",
        "\tx = 0;\n",
        "\tfor(int i = 0; i < n; i++){\n",
        "\t\t\ta[i] = x;\n",
        "\t\t\tx += PI/n;\n",
        "\t}\n",
        "\n",
        "\t//mando in esecuzione gli stream\n",
        "\tCHECK(cudaEventRecord(startEvent, 0));\n",
        "\tint offset = 0;\n",
        "\tfor(int i = 0; i < nStreams; i++){\n",
        "\t\t\tCHECK(cudaMemcpyAsync(&d_a[offset], &a[offset], streamBytes, cudaMemcpyHostToDevice, stream[i]));\n",
        "\t\t\ttabular_streams<<<n/blockSize, blockSize, 0, stream[i]>>>(d_a, n, offset);\n",
        "\t\t\tCHECK(cudaMemcpyAsync(&a[offset], &d_a[offset], streamBytes, cudaMemcpyDeviceToHost, stream[i]));\n",
        "\t\t\toffset += streamSize;\n",
        "\t}\n",
        "\tCHECK(cudaEventRecord(stopEvent, 0));\n",
        "\tCHECK(cudaEventSynchronize(stopEvent));\n",
        "\tCHECK(cudaEventElapsedTime(&ms, startEvent, stopEvent));\n",
        "\tprintf(\"\\nTime for sequential transfer and execute (ms): %f\\n\", ms);\n",
        "\tprintf(\"  max error: %e\\n\", maxError(a, n));\n",
        "\n",
        "\t// asynchronous version 2:\n",
        "\t// loop over copy, loop over kernel, loop over copy\n",
        "\t\n",
        "\t//di nuovo, rimetto a posto la memoria\n",
        "\tmemset(a, 0, bytes);\n",
        "\tCHECK(cudaMemset(d_a, 0, bytes));\n",
        "\tx = 0;\n",
        "\tfor(int i = 0; i < n; i++){\n",
        "\t\t\ta[i] = x;\n",
        "\t\t\tx += PI/n;\n",
        "\t}\n",
        "\n",
        "\t\n",
        "\t//mando in esecuzione gli stream\n",
        "\tCHECK(cudaEventRecord(startEvent, 0));\n",
        "\toffset = 0;\n",
        "\tfor(int i = 0; i < nStreams; i++){\n",
        "\t\t\tCHECK(cudaMemcpyAsync(&d_a[offset], &a[offset], streamBytes, cudaMemcpyHostToDevice, stream[i]));\n",
        "\t\t\toffset += streamSize;\n",
        "\t}\n",
        "\n",
        "\toffset = 0;\n",
        "\tfor(int i = 0; i < nStreams; i++){\n",
        "\t\t\ttabular_streams<<<n/blockSize, blockSize, 0, stream[i]>>>(d_a, n, offset);\n",
        "\t\t\toffset += streamSize;\n",
        "\t}\n",
        "\n",
        "\toffset = 0;\n",
        "\tfor(int i = 0; i < nStreams; i++){\n",
        "\t\t\tCHECK(cudaMemcpyAsync(&a[offset], &d_a[offset], streamBytes, cudaMemcpyDeviceToHost, stream[i]));\n",
        "\t\t\toffset += streamSize;\n",
        "\t}\n",
        "\n",
        "\tCHECK(cudaEventRecord(stopEvent, 0));\n",
        "\tCHECK(cudaEventSynchronize(stopEvent));\n",
        "\tCHECK(cudaEventElapsedTime(&ms, startEvent, stopEvent));\n",
        "\tprintf(\"\\nTime for sequential transfer and execute (ms): %f\\n\", ms);\n",
        "\tprintf(\"  max error: %e\\n\", maxError(a, n));\n",
        "\n",
        "\tcudaFree(d_a);\n",
        "\tcudaFreeHost(a);\n",
        "\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PSc9B9PDTWt"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "\n",
        "!nvcc -arch=sm_75 tabularStream/tabular.cu  -o tabular\n",
        "!./tabular"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfR471rze2p-"
      },
      "source": [
        "# profilazione (senza unified memory - dà errore)\n",
        "\n",
        "!nvprof ./tabular"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOFMQZAkjlLW"
      },
      "source": [
        "# ✅ MQDB con stream"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 TODO"
      ],
      "metadata": {
        "id": "d3iIdkudZuiE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9nRkLgeB10A"
      },
      "source": [
        "\n",
        "%%writefile MQDB_stream/MQDB_stream_Unified.cu\n",
        "\n",
        "\n",
        "#include \"../../utils/MQDB/mqdb.h\"\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "#define BLOCK_SIZE 16     // block size\n",
        "#define TEST_CPU 0\n",
        "\n",
        "/*\n",
        " * Kernel for standard (naive) matrix product\n",
        " */\n",
        "__global__ void matProdKernel(mqdb *A, mqdb *B, mqdb *C, int n) {\n",
        "\t// row & col indexes\n",
        "\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\t// each thread computes an entry of the product matrix\n",
        "\tif ((row < n) && (col < n)) {\n",
        "\t\tfloat val = 0;\n",
        "\t\tfor (int k = 0; k < n; k++)\n",
        "\t\t\tval += A->elem[row * n + k] * B->elem[k * n + col];\n",
        "\t\tC->elem[row * n + col] = val;\n",
        "\t}\n",
        "}\n",
        "\n",
        "/*\n",
        " * Kernel for block sub-matrix product of mqdb\n",
        " */\n",
        "__global__ void mqdbBlockProd(mqdb *A, mqdb *B, mqdb *C, uint sdim, uint d, uint n) {\n",
        "\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\t// jump to the right block sub-matrix\n",
        "\tuint  offset = (n+1)*sdim;\n",
        "\n",
        "\t// each thread computes an entry of the product matrix\n",
        "\tif ((row < d) && (col < d)) {\n",
        "\t\tfloat val = 0;\n",
        "\t\tfor (int k = 0; k < d; k++)\n",
        "\t\t\tval += A->elem[row * n + k + offset] * B->elem[k * n + col + offset];\n",
        "\t\tC->elem[row * n + col + offset] = val;\n",
        "\t}\n",
        "}\n",
        "\n",
        "\n",
        "/*\n",
        " * Test on MQDB kernels using Unified Memory\n",
        " */\n",
        "void testKernelsMQDB_unified(uint n, uint k, cudaEvent_t start, cudaEvent_t stop) {\n",
        "\n",
        "\t// matrix instance generation - Unified Memory\n",
        "\tmqdb *A, *B, *C;\n",
        "\t\n",
        "\t// TODO\n",
        "\n",
        "\tulong nBytes = n * n * sizeof(float);\n",
        "\tprintf(\"Memory size required = %3.4f (MB)\\n\",(float)nBytes/(1024.0*1024.0));\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*                    CPU MQDB product                     */\n",
        "\t/***********************************************************/\n",
        "\t\n",
        "  printf(\"CPU MQDB product...\\n\");\n",
        "\tdouble CPUtime = 0.0;\n",
        "\n",
        "  #if TEST_CPU\n",
        "    double startTm = seconds();\n",
        "\t  mqdbProd(A,B,C);\n",
        "\t  CPUtime = seconds() - startTm;\n",
        "  #endif\n",
        "\n",
        "\tprintf(\"   CPU elapsed time: %.5f (sec)\\n\\n\", CPUtime);\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*                     GPU mat product                     */\n",
        "\t/***********************************************************/\n",
        "\t\n",
        "  printf(\"Kernel (naive) mat product...\\n\");\n",
        "\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
        "\tdim3 grid((n + block.x - 1) / block.x, (n + block.y - 1) / block.y);\n",
        "  float milliseconds;\n",
        "\tCHECK(cudaEventRecord(start));\n",
        "\tmatProdKernel<<<grid, block>>>(A, B, C, n);\n",
        "  CHECK(cudaDeviceSynchronize());\n",
        "\tCHECK(cudaEventRecord(stop));\n",
        "\tCHECK(cudaEventSynchronize(stop));\n",
        "\tCHECK(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "\tfloat GPUtime1 = milliseconds / 1000.0;\n",
        "\tprintf(\"   elapsed time               : %.2f (sec)\\n\", GPUtime1);\n",
        "\tprintf(\"   speedup vs CPU MQDB product: %.2f\\n\\n\", CPUtime/GPUtime1);\n",
        "\t//mqdbDisplay(C);\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*                     GPU MQDB product                    */\n",
        "\t/***********************************************************/\n",
        "\t\n",
        "  printf(\"Kernel MQDB product...\\n\");\n",
        "\tuint sdim = 0;\n",
        "\tCHECK(cudaEventRecord(start));\n",
        "\tfor (uint i = 0; i < k; i++ ) {\n",
        "\t\tuint d = A->blkSize[i];\n",
        "\t\t//mqdbBlockProd<<<grid, block>>>(A, B, C, sdim, d, n);\n",
        "\t\tsdim += d;\n",
        "\t}\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tCHECK(cudaEventRecord(stop));\n",
        "\tCHECK(cudaEventSynchronize(stop));\n",
        "\tCHECK(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "\tfloat GPUtime2 = milliseconds / 1000.0;\n",
        "\tprintf(\"   elapsed time                  :  %.2f (sec)\\n\", GPUtime2);\n",
        "\tprintf(\"   speedup vs CPU MQDB product   :  %.2f\\n\", CPUtime/GPUtime2);\n",
        "\tprintf(\"   speedup vs GPU std mat product:  %.2f\\n\\n\", GPUtime1/GPUtime2);\n",
        "\n",
        "  /***********************************************************/\n",
        "\t/*             GPU MQDB product using streams              */\n",
        "\t/***********************************************************/\n",
        "\t\n",
        "  printf(\"GPU MQDB product using streams...\\n\");\n",
        "\n",
        "  // create and use streams\n",
        "\tint nstreams = A->nBlocks;\n",
        "\t//cudaStream_t *streams = A->nBlocks * sizeof(cudaStream_t);\n",
        "\tcudaStream_t streams[nstreams];\n",
        "\n",
        "\tfor(int i = 0; i < nstreams; i++){\n",
        "\t\t\tcudaStreamCreate(&streams[i]);\n",
        "\t}\n",
        "\n",
        "\tsdim = 0;\n",
        "\tCHECK(cudaEventRecord(start));\n",
        "\tfor(int i = 0; i < nstreams; i++){\n",
        "\t\t\tint d = A->blkSize[i];\n",
        "\t\t\tdim3 grid_current(d + block.x - 1, d + block.y - 1);\n",
        "\t\t\tmqdbBlockProd<<<grid_current, block, 0, streams[i]>>>(A, B, C, sdim, d, n);\n",
        "\t\t\tsdim += d;\n",
        "\t}\n",
        "\n",
        "\tCHECK(cudaEventRecord(stop));\n",
        "\tCHECK(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "\n",
        "\n",
        "\n",
        "\tfloat GPUtime3 = milliseconds / 1000.0;\n",
        "\tprintf(\"   elapsed time                  : %.5f (sec)\\n\", GPUtime3);\n",
        "\tprintf(\"   speedup vs CPU MQDB product   : %.2f\\n\", CPUtime/GPUtime3);\n",
        "\tprintf(\"   speedup vs GPU std mat product: %.2f\\n\",GPUtime1/GPUtime3);\n",
        "  printf(\"   speedup vs GPU MQDB product   : %.2f\\n\",GPUtime2/GPUtime3);\n",
        "  //mqdbDisplay(C);\n",
        "\n",
        "\t// clean up streams and events\n",
        "\tfor (int i = 0; i < nstreams; i++)\n",
        "\t\tcudaStreamDestroy(streams[i]);\n",
        "\n",
        "}\n",
        "\n",
        "/*\n",
        " * main function\n",
        " */\n",
        "int main(int argc, char *argv[]) {\n",
        "  \n",
        "  // set up device\n",
        "\tint dev = 0;\n",
        "\tcudaDeviceProp deviceProp;\n",
        "\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "\tprintf(\"%s starting mqdb product at \", argv[0]);\n",
        "\tprintf(\"device %d: %s\\n\", dev, deviceProp.name);\n",
        "\tCHECK(cudaSetDevice(dev));\n",
        "\n",
        "\t// events to measure time\n",
        "\tcudaEvent_t start, stop;\n",
        "\tcudaEventCreate(&start);\n",
        "\tcudaEventCreate(&stop);\n",
        "\n",
        "\tuint n = 4*1024;         // matrix size\n",
        "\tuint min_k = 20;       // max num of blocks\n",
        "\tuint max_k = 30;       // max num of blocks\n",
        "\n",
        "\t// multiple tests for k = # diag blocks\n",
        "\tfor (uint k = min_k; k <= max_k; k+=5) {\n",
        "\t\tprintf(\"\\n*****   k = %d --- (avg block size = %f)\\n\",k,(float)n/k);\n",
        "\t\ttestKernelsMQDB_unified(n, k, start, stop);\n",
        "\t}\n",
        "\n",
        "  cudaEventDestroy(start);\n",
        "\tcudaEventDestroy(stop);\n",
        "\treturn 0;\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLxZjCx8bT3s"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "!nvcc -arch=sm_37  MQDB_stream/MQDB_stream_Unified.cu ../utils/MQDB/mqdb.cpp -o MQDBS\n",
        "!./MQDBS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fYYbj397SdK"
      },
      "source": [
        "%%writefile MQDB_stream/MQDB_stream_manual.cu\n",
        "\n",
        "\n",
        "#include \"../../utils/MQDB/mqdb.h\"\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "#define BLOCK_SIZE 16     // block size\n",
        "\n",
        "/*\n",
        " * Kernel for block sub-matrix product of mqdb\n",
        " */\n",
        "__global__ void mqdbBlockProd(mqdb A, mqdb B, mqdb C, uint sdim, uint d, uint n) {\n",
        "\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\t// jump to the right block sub-matrix\n",
        "\tuint  offset = (n+1)*sdim;\n",
        "\n",
        "\t// each thread computes an entry of the product matrix\n",
        "\tif ((row < d) && (col < d)) {\n",
        "\t\tfloat val = 0;\n",
        "\t\tfor (int k = 0; k < d; k++)\n",
        "\t\t\tval += A.elem[row * n + k + offset] * B.elem[k * n + col + offset];\n",
        "\t\tC.elem[row * n + col + offset] = val;\n",
        "\t}\n",
        "}\n",
        "\n",
        "/*\n",
        " * Test on MQDB kernels using manual async memory\n",
        " */\n",
        "void testKernelsMQDB_manual_mem(uint n, uint k, cudaEvent_t start, cudaEvent_t stop) {\n",
        "\n",
        "\t// matrices\n",
        "\tmqdb *A, *B, *C;         // host \n",
        "\tmqdb d_A, d_B, d_C;      // device\n",
        "\n",
        "  ulong nBytes = n * n * sizeof(float);\n",
        "  int kBytes = k * sizeof(int);\n",
        "\tprintf(\"Memory size required = %3.4f (MB)\\n\",(float)nBytes/(1024.0*1024.0));\n",
        "\n",
        "\n",
        "  // host and device Memory\n",
        "\tCHECK(cudaMallocHost(&A, sizeof(mqdb)));\n",
        "  CHECK(cudaMallocHost(&A->blkSize, kBytes));\n",
        "  CHECK(cudaMallocHost(&A->elem, nBytes));\n",
        "  CHECK(cudaMalloc(&d_A.blkSize, kBytes));\n",
        "  CHECK(cudaMalloc(&d_A.elem, nBytes));\n",
        "\n",
        "  CHECK(cudaMallocHost(&B, sizeof(mqdb)));\n",
        "  CHECK(cudaMallocHost(&B->blkSize, kBytes));\n",
        "  CHECK(cudaMallocHost(&B->elem, nBytes));\n",
        "\tCHECK(cudaMalloc(&d_B.blkSize, kBytes));\n",
        "  CHECK(cudaMalloc(&d_B.elem, nBytes));\n",
        "\t\n",
        "  CHECK(cudaMallocHost(&C, sizeof(mqdb)));\n",
        "  CHECK(cudaMallocHost(&C->blkSize, kBytes));\n",
        "  CHECK(cudaMallocHost(&C->elem, nBytes));\n",
        "\tCHECK(cudaMalloc(&d_C.blkSize, kBytes));\n",
        "  CHECK(cudaMalloc(&d_C.elem, nBytes));\n",
        "\n",
        "  // random fill mat entries\n",
        "  int seed = 1;\n",
        "\tgenRandDims(A, n, k, seed);\n",
        "\tgenRandDims(B, n, k, seed);\n",
        "\tgenRandDims(C, n, k, seed);\n",
        "\tfillBlocks(A, n, k, 'C', 1);\n",
        "\tfillBlocks(B, n, k, 'C', 2);\n",
        "\tfillBlocks(C, n, k, 'C', 0);\n",
        "\n",
        "  /***********************************************************/\n",
        "\t/*       GPU MQDB product using streams & async copy       */\n",
        "\t/***********************************************************/\n",
        "\tprintf(\"GPU MQDB product using streams...\\n\");\n",
        "\n",
        "\t// TODO\n",
        "\n",
        "\tfloat GPUtime3 = milliseconds / 1000.0;\n",
        "\tprintf(\"   elapsed time                  : %.5f (sec)\\n\", GPUtime3);\n",
        "\n",
        "\t// clean up streams and events\n",
        "\tfor (int i = 0; i < nstreams; i++)\n",
        "\t\tcudaStreamDestroy(streams[i]);\n",
        "\n",
        "  CHECK(cudaFreeHost(A->elem));\n",
        "\tCHECK(cudaFreeHost(B->elem));\n",
        "\tCHECK(cudaFreeHost(C->elem));\n",
        "  CHECK(cudaFreeHost(A->blkSize));\n",
        "\tCHECK(cudaFreeHost(B->blkSize));\n",
        "\tCHECK(cudaFreeHost(C->blkSize));\n",
        "  CHECK(cudaFreeHost(A));\n",
        "\tCHECK(cudaFreeHost(B));\n",
        "\tCHECK(cudaFreeHost(C));\n",
        "\tCHECK(cudaFree(d_A.elem));\n",
        "\tCHECK(cudaFree(d_B.elem));\n",
        "\tCHECK(cudaFree(d_C.elem));\n",
        "  CHECK(cudaFree(d_A.blkSize));\n",
        "\tCHECK(cudaFree(d_B.blkSize));\n",
        "\tCHECK(cudaFree(d_C.blkSize));\n",
        "} \n",
        "\n",
        "/*\n",
        " * main function\n",
        " */\n",
        "int main(int argc, char *argv[]) {\n",
        "  \n",
        "  // set up device\n",
        "\tint dev = 0;\n",
        "\tcudaDeviceProp deviceProp;\n",
        "\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "\tprintf(\"%s starting mqdb product at \", argv[0]);\n",
        "\tprintf(\"device %d: %s\\n\", dev, deviceProp.name);\n",
        "\tCHECK(cudaSetDevice(dev));\n",
        "\n",
        "\t// events to measure time\n",
        "\tcudaEvent_t start, stop;\n",
        "\tcudaEventCreate(&start);\n",
        "\tcudaEventCreate(&stop);\n",
        "\n",
        "\tuint n = 16*1024;         // matrix size\n",
        "\tuint min_k = 20;       // max num of blocks\n",
        "\tuint max_k = 30;       // max num of blocks\n",
        "\n",
        "\t// multiple tests for k = # diag blocks\n",
        "\tfor (uint k = min_k; k <= max_k; k+=5) {\n",
        "\t\tprintf(\"\\n*****   k = %d --- (avg block size = %f)\\n\",k,(float)n/k);\n",
        "\t\ttestKernelsMQDB_manual_mem(n, k, start, stop);\n",
        "\t}\n",
        "\n",
        "  cudaEventDestroy(start);\n",
        "\tcudaEventDestroy(stop);\n",
        "\treturn 0;\n",
        "}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmMmj5KDnteq"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "!nvcc -arch=sm_37  MQDB_stream/MQDB_stream_manual.cu ../utils/MQDB/mqdb.cpp -o MQDBS\n",
        "!./MQDBS"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}