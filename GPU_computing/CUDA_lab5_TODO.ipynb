{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUDA_lab5_TODO.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "WoJbB3T5Vkw-",
        "zs_a5Vuimily",
        "SOFMQZAkjlLW",
        "Z2o3XANQf6Zv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **LAB 5 - Unrolling e parallelismo dinamico**\n",
        "---"
      ],
      "metadata": {
        "id": "fZYqN0UwVLC_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoJbB3T5Vkw-"
      },
      "source": [
        "# ▶️ CUDA setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fht2Wy8wVkxJ"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jP2H_YJVkxJ"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [GPU Compute Capability](https://developer.nvidia.com/cuda-gpus)"
      ],
      "metadata": {
        "id": "VKbaxH9wWosO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cGSqZovVkxK"
      },
      "source": [
        "## NVCC Plugin for Jupyter notebook\n",
        "\n",
        "*Usage*:\n",
        "\n",
        "\n",
        "*   Load Extension `%load_ext nvcc_plugin`\n",
        "*   Mark a cell to be treated as cuda cell\n",
        "`%%cuda --name example.cu --compile false`\n",
        "\n",
        "**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n",
        "\n",
        "*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCVhMkqYVkxK"
      },
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6PDOytTVkxK"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bash and data setup"
      ],
      "metadata": {
        "id": "cReFlD-VRfZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Bash setup\n",
        "%%writefile /root/.bashrc\n",
        "\n",
        "# If not running interactively, don't do anything\n",
        "[ -z \"$PS1\" ] && return\n",
        "\n",
        "# don't put duplicate lines in the history. See bash(1) for more options\n",
        "# ... or force ignoredups and ignorespace\n",
        "HISTCONTROL=ignoredups:ignorespace\n",
        "\n",
        "# append to the history file, don't overwrite it\n",
        "shopt -s histappend\n",
        "\n",
        "# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)\n",
        "HISTSIZE=10000\n",
        "HISTFILESIZE=20000\n",
        "\n",
        "# check the window size after each command and, if necessary,\n",
        "# update the values of LINES and COLUMNS.\n",
        "shopt -s checkwinsize\n",
        "\n",
        "# make less more friendly for non-text input files, see lesspipe(1)\n",
        "[ -x /usr/bin/lesspipe ] && eval \"$(SHELL=/bin/sh lesspipe)\"\n",
        "\n",
        "PS1='\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ '\n",
        "\n",
        "# enable color support of ls and also add handy aliases\n",
        "if [ -x /usr/bin/dircolors ]; then\n",
        "    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\n",
        "    alias ls='ls --color=auto'\n",
        "    #alias dir='dir --color=auto'\n",
        "    #alias vdir='vdir --color=auto'\n",
        "\n",
        "    alias grep='grep --color=auto'\n",
        "    alias fgrep='fgrep --color=auto'\n",
        "    alias egrep='egrep --color=auto'\n",
        "fi\n",
        "\n",
        "# some more ls aliases\n",
        "alias ll='ls -lF'\n",
        "alias la='ls -A'\n",
        "alias l='ls -CF'\n",
        "\n",
        "# path setup\n",
        "export PATH=\"./:/usr/local/cuda/bin:$PATH\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "O8ICSyy8_GEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source /root/.bashrc"
      ],
      "metadata": {
        "id": "QxIfKO3Ghf7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GPUcomputing site on github..."
      ],
      "metadata": {
        "id": "IYG8Cv4bTzyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/giulianogrossi/GPUcomputing.git"
      ],
      "metadata": {
        "id": "E7jZmHjCT0vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some paths..."
      ],
      "metadata": {
        "id": "ZarLje6wR_Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path setup\n",
        "!mkdir -p /content/GPUcomputing/lab5\n",
        "%cd /content/GPUcomputing/lab5\n",
        "!mkdir -p device\n",
        "!mkdir -p dynParall\n",
        "!mkdir -p unrolling\n",
        "!mkdir -p MQDB-CUDA-DP"
      ],
      "metadata": {
        "id": "tC-AaOJlkLOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ▶️ VS Code on Colab"
      ],
      "metadata": {
        "id": "zs_a5Vuimily"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab-ssh tunnel\n",
        "#@markdown Execute this cell to open the ssh tunnel. Check [colab-ssh documentation](https://github.com/WassimBenzarti/colab-ssh) for more details.\n",
        "\n",
        "# Install colab_ssh on google colab\n",
        "!pip install colab_ssh --upgrade\n",
        "\n",
        "from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n",
        "ssh_tunnel_password = \"gpu\" #@param {type: \"string\"}\n",
        "launch_ssh_cloudflared(password=ssh_tunnel_password)\n",
        "\n",
        "# Optional: if you want to clone a Github or Gitlab repository\n",
        "repository_url=\"https://github.com/giulianogrossi/GPUcomputing\" #@param {type: \"string\"}\n",
        "init_git_cloudflared(repository_url)"
      ],
      "metadata": {
        "id": "BCf9JxqphHAp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUYP4kCJhEIx"
      },
      "source": [
        "# ✅ DeviceQuery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPaV4DfPh26Q"
      },
      "source": [
        "%%writefile device/helper.h\n",
        "// Beginning of GPU Architecture definitions\n",
        "inline int _ConvertSMVer2Cores(int major, int minor) {\n",
        "\t// Defines for GPU Architecture types (using the SM version to determine\n",
        "\t// the # of cores per SM\n",
        "\ttypedef struct {\n",
        "\t\tint SM;  // 0xMm (hexidecimal notation), M = SM Major version,\n",
        "\t\t// and m = SM minor version\n",
        "\t\tint Cores;\n",
        "\t} sSMtoCores;\n",
        "\n",
        "\tsSMtoCores nGpuArchCoresPerSM[] = {\n",
        "\t\t\t{0x20, 32},\n",
        "\t\t\t{0x30, 192},\n",
        "\t\t\t{0x32, 192},\n",
        "\t\t\t{0x35, 192},\n",
        "\t\t\t{0x37, 192},\n",
        "\t\t\t{0x50, 128},\n",
        "\t\t\t{0x52, 128},\n",
        "\t\t\t{0x53, 128},\n",
        "\t\t\t{0x60,  64},\n",
        "\t\t\t{0x61, 128},\n",
        "\t\t\t{0x62, 128},\n",
        "\t\t\t{0x70,  64},\n",
        "\t\t\t{0x72,  64},\n",
        "\t\t\t{0x75,  64},\n",
        "\t\t\t{0x80,  64},\n",
        "      {0x86, 128},\n",
        "      {0x87, 128},\n",
        "\t\t\t{-1, -1}};\n",
        "\n",
        "\tint index = 0;\n",
        "\n",
        "\twhile (nGpuArchCoresPerSM[index].SM != -1) {\n",
        "\t\tif (nGpuArchCoresPerSM[index].SM == ((major << 4) + minor)) {\n",
        "\t\t\treturn nGpuArchCoresPerSM[index].Cores;\n",
        "\t\t}\n",
        "\n",
        "\t\tindex++;\n",
        "\t}\n",
        "\n",
        "\t//# If we don't find the values, we default use the previous one to run properly\n",
        "\tprintf(\n",
        "\t\t\t\"MapSMtoCores for SM %d.%d is undefined.\"\n",
        "\t\t\t\"  Default to use %d Cores/SM\\n\",\n",
        "\t\t\tmajor, minor, nGpuArchCoresPerSM[index - 1].Cores);\n",
        "\treturn nGpuArchCoresPerSM[index - 1].Cores;\n",
        "}\n",
        "\n",
        "inline const char* _ConvertSMVer2ArchName(int major, int minor) {\n",
        "  // Defines for GPU Architecture types (using the SM version to determine\n",
        "  // the GPU Arch name)\n",
        "  typedef struct {\n",
        "    int SM;  // 0xMm (hexidecimal notation), M = SM Major version,\n",
        "    // and m = SM minor version\n",
        "    const char* name;\n",
        "  } sSMtoArchName;\n",
        "\n",
        "  sSMtoArchName nGpuArchNameSM[] = {\n",
        "      {0x30, \"Kepler\"},\n",
        "      {0x32, \"Kepler\"},\n",
        "      {0x35, \"Kepler\"},\n",
        "      {0x37, \"Kepler\"},\n",
        "      {0x50, \"Maxwell\"},\n",
        "      {0x52, \"Maxwell\"},\n",
        "      {0x53, \"Maxwell\"},\n",
        "      {0x60, \"Pascal\"},\n",
        "      {0x61, \"Pascal\"},\n",
        "      {0x62, \"Pascal\"},\n",
        "      {0x70, \"Volta\"},\n",
        "      {0x72, \"Xavier\"},\n",
        "      {0x75, \"Turing\"},\n",
        "      {0x80, \"Ampere\"},\n",
        "      {0x86, \"Ampere\"},\n",
        "      {-1, \"Graphics Device\"}};\n",
        "\n",
        "  int index = 0;\n",
        "\n",
        "  while (nGpuArchNameSM[index].SM != -1) {\n",
        "    if (nGpuArchNameSM[index].SM == ((major << 4) + minor)) {\n",
        "      return nGpuArchNameSM[index].name;\n",
        "    }\n",
        "    index++;\n",
        "  }\n",
        "\n",
        "  // If we don't find the values, we default use the previous one\n",
        "  // to run properly\n",
        "  printf(\n",
        "      \"MapSMtoArchName for SM %d.%d is undefined.\"\n",
        "      \"  Default to use %s\\n\",\n",
        "      major, minor, nGpuArchNameSM[index - 1].name);\n",
        "  return nGpuArchNameSM[index - 1].name;\n",
        "}\n",
        "// end of GPU Architecture definitions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvl8WXljg0WK"
      },
      "source": [
        "%%writefile device/deviceQuery.cu\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include \"helper.h\"\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "int main(void) {\n",
        "\n",
        "\tprintf(\"\\nCUDA Device Query (Runtime API) version (CUDART static linking)\\n\\n\");\n",
        "\tint deviceCount = 0;\n",
        "\tCHECK(cudaGetDeviceCount(&deviceCount));\n",
        "\n",
        "\t// This function call returns 0 if there are no CUDA capable devices.\n",
        "\tif (deviceCount == 0)\n",
        "\t\tprintf(\"There are no available device(s) that support CUDA\\n\");\n",
        "\telse\n",
        "\t\tprintf(\"Detected %d CUDA Capable device(s)\\n\", deviceCount);\n",
        "\n",
        "\tint dev, driverVersion = 0, runtimeVersion = 0;\n",
        "\n",
        "\tfor (dev = 0; dev < deviceCount; ++dev) {\n",
        "\t\tcudaSetDevice(dev);\n",
        "\t\tcudaDeviceProp deviceProp;\n",
        "\t\tcudaGetDeviceProperties(&deviceProp, dev);\n",
        "\n",
        "\t\tprintf(\"\\nDevice %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n",
        "\n",
        "\t\tcudaDriverGetVersion(&driverVersion);\n",
        "\t\tcudaRuntimeGetVersion(&runtimeVersion);\n",
        "\n",
        "\t\tprintf(\"  CUDA Driver Version / Runtime Version          %d.%d / %d.%d\\n\",\n",
        "\t\t\t\tdriverVersion / 1000, (driverVersion % 100) / 10,\n",
        "\t\t\t\truntimeVersion / 1000, (runtimeVersion % 100) / 10);\n",
        "\n",
        "\t\tprintf(\"  GPU arch name:                                 %s\\n\",\n",
        "\t\t\t\t\t\t_ConvertSMVer2ArchName(deviceProp.major, deviceProp.minor));\n",
        "\n",
        "\t\tprintf(\"  CUDA Capability Major/Minor version number:    %d.%d\\n\",\n",
        "\t\t\t\tdeviceProp.major, deviceProp.minor);\n",
        "\n",
        "\t\tprintf(\"  Total amount of global memory:                 %.0f MBytes (%llu bytes)\\n\",\n",
        "\t\t\t\t(float) deviceProp.totalGlobalMem / 1048576.0f,\n",
        "\t\t\t\t(unsigned long long) deviceProp.totalGlobalMem);\n",
        "\n",
        "\t\tprintf(\"  (%2d) Multiprocessors, (%3d) CUDA Cores/MP:     %d CUDA Cores\\n\",\n",
        "\t\t\t\t\t\tdeviceProp.multiProcessorCount,\n",
        "\t\t\t\t\t\t_ConvertSMVer2Cores(deviceProp.major, deviceProp.minor),\n",
        "\t\t\t\t\t\t_ConvertSMVer2Cores(deviceProp.major, deviceProp.minor) *\n",
        "\t\t\t\t\t\t\t\tdeviceProp.multiProcessorCount);\n",
        "\t\t\n",
        "\t\tprintf(\"  GPU Max Clock rate:                            %.0f MHz (%0.2f GHz)\\n\",\n",
        "\t\t\t\tdeviceProp.clockRate * 1e-3f, deviceProp.clockRate * 1e-6f);\n",
        "\n",
        "\t\tprintf(\"  Memory Clock rate:                             %.0f Mhz\\n\", deviceProp.memoryClockRate * 1e-3f);\n",
        "\t\tprintf(\"  Memory Bus Width:                              %d-bit\\n\", deviceProp.memoryBusWidth);\n",
        "\t\tif (deviceProp.l2CacheSize)\n",
        "\t\t\tprintf(\"  L2 Cache Size:                                 %d bytes\\n\", deviceProp.l2CacheSize);\n",
        "\n",
        "\t\tprintf(\"  Maximum Texture Dimension Size (x,y,z)         1D=(%d), 2D=(%d, %d), 3D=(%d, %d, %d)\\n\",\n",
        "\t\t\t\tdeviceProp.maxTexture1D, deviceProp.maxTexture2D[0],\n",
        "\t\t\t\tdeviceProp.maxTexture2D[1], deviceProp.maxTexture3D[0],\n",
        "\t\t\t\tdeviceProp.maxTexture3D[1], deviceProp.maxTexture3D[2]);\n",
        "\n",
        "\t\tprintf(\"  Maximum Layered 1D Texture Size, (num) layers  1D=(%d), %d layers\\n\",\n",
        "\t\t\t\tdeviceProp.maxTexture1DLayered[0],\n",
        "\t\t\t\tdeviceProp.maxTexture1DLayered[1]);\n",
        "\n",
        "\t\tprintf(\"  Maximum Layered 2D Texture Size, (num) layers  2D=(%d, %d), %d layers\\n\",\n",
        "\t\t\t\tdeviceProp.maxTexture2DLayered[0],\n",
        "\t\t\t\tdeviceProp.maxTexture2DLayered[1],\n",
        "\t\t\t\tdeviceProp.maxTexture2DLayered[2]);\n",
        "\n",
        "\t\tprintf(\"  Total amount of constant memory                %lu bytes\\n\",\n",
        "\t\t\t\tdeviceProp.totalConstMem);\n",
        "\t\tprintf(\"  Total amount of shared memory per block        %lu bytes\\n\",\n",
        "\t\t\t\tdeviceProp.sharedMemPerBlock);\n",
        "\t\tprintf(\"  Total number of registers available per block  %d\\n\",\n",
        "\t\t\t\tdeviceProp.regsPerBlock);\n",
        "\t\tprintf(\"  Warp size                                      %d\\n\",\n",
        "\t\t\t\tdeviceProp.warpSize);\n",
        "\t\tprintf(\"  Maximum number of threads per multiprocessor   %d\\n\",\n",
        "\t\t\t\tdeviceProp.maxThreadsPerMultiProcessor);\n",
        "\t\tprintf(\"  Maximum number of threads per block            %d\\n\",\n",
        "\t\t\t\tdeviceProp.maxThreadsPerBlock);\n",
        "\t\tprintf(\"  Max dimension size of a thread block (x,y,z)  (%d, %d, %d)\\n\",\n",
        "\t\t\t\tdeviceProp.maxThreadsDim[0], deviceProp.maxThreadsDim[1],\n",
        "\t\t\t\tdeviceProp.maxThreadsDim[2]);\n",
        "\t\tprintf(\"  Max dimension size of a grid size    (x,y,z)  (%d, %d, %d)\\n\",\n",
        "\t\t\t\tdeviceProp.maxGridSize[0], deviceProp.maxGridSize[1],\n",
        "\t\t\t\tdeviceProp.maxGridSize[2]);\n",
        "\t\tprintf(\"  Maximum memory pitch                           %lu bytes\\n\",\n",
        "\t\t\t\tdeviceProp.memPitch);\n",
        "\t\tprintf(\"  Texture alignment                              %lu bytes\\n\",\n",
        "\t\t\t\tdeviceProp.textureAlignment);\n",
        "\t\tprintf(\"  Concurrent copy and kernel execution           %s with %d copy engine(s)\\n\",\n",
        "\t\t\t\t(deviceProp.deviceOverlap ? \"Yes\" : \"No\"),\n",
        "\t\t\t\tdeviceProp.asyncEngineCount);\n",
        "\t\tprintf(\"  Run time limit on kernels                      %s\\n\",\n",
        "\t\t\t\tdeviceProp.kernelExecTimeoutEnabled ? \"Yes\" : \"No\");\n",
        "\t\tprintf(\"  Integrated GPU sharing Host Memory             %s\\n\",\n",
        "\t\t\t\tdeviceProp.integrated ? \"Yes\" : \"No\");\n",
        "\t\tprintf(\"  Support host page-locked memory mapping        %s\\n\",\n",
        "\t\t\t\tdeviceProp.canMapHostMemory ? \"Yes\" : \"No\");\n",
        "\t\tprintf(\"  Alignment requirement for Surfaces             %s\\n\",\n",
        "\t\t\t\tdeviceProp.surfaceAlignment ? \"Yes\" : \"No\");\n",
        "\t\tprintf(\"  Device has ECC support                         %s\\n\",\n",
        "\t\t\t\tdeviceProp.ECCEnabled ? \"Enabled\" : \"Disabled\");\n",
        "\n",
        "\t\tprintf(\"  Device supports Unified Addressing (UVA):      %s\\n\",\n",
        "\t\t\t\tdeviceProp.unifiedAddressing ? \"Yes\" : \"No\");\n",
        "\t\tprintf(\"  Device PCI Domain ID / Bus ID / location ID:   %d / %d / %d\\n\",\n",
        "\t\t\t\tdeviceProp.pciDomainID, deviceProp.pciBusID,\n",
        "\t\t\t\tdeviceProp.pciDeviceID);\n",
        "\t}\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW9b_Yuxi7id"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "\n",
        "!nvcc device/deviceQuery.cu -o deviceQuery\n",
        "!./deviceQuery"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXUIQkZLCTcG"
      },
      "source": [
        "# ✅ Unrolling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y52R0d3CA50"
      },
      "source": [
        "%%writefile unrolling/main.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "// the kernels prototype\n",
        "__global__ void blockParReduceUroll(int*, int*, ulong);\n",
        "__global__ void multBlockParReduceUroll8(int*, int*, ulong); \n",
        "__global__ void multBlockParReduceUroll16(int*, int*, ulong);\n",
        "\n",
        "\n",
        "/*\n",
        " *  Block by block parallel implementation with divergence\n",
        " */\n",
        "__global__ void blockParReduce1(int *in, int *out, ulong n) {\n",
        "\n",
        "\tuint tid = threadIdx.x;\n",
        "\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\t// boundary check\n",
        "\tif (idx >= n)\n",
        "\t\treturn;\n",
        "\n",
        "\t// convert global data pointer to the local pointer of this block\n",
        "\tint *thisBlock = in + blockIdx.x * blockDim.x;\n",
        "\n",
        "\t// in-place reduction in global memory\n",
        "\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n",
        "\t\tif ((tid % (2 * stride)) == 0)\n",
        "\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n",
        "\n",
        "\t\t// synchronize within threadblock\n",
        "\t\t__syncthreads();\n",
        "\t}\n",
        "\n",
        "\t// write result for this block to global mem\n",
        "\tif (tid == 0)\n",
        "\t\tout[blockIdx.x] = thisBlock[0];\n",
        "}\n",
        "\n",
        "/*\n",
        " *  Block by block parallel implementation without divergence\n",
        " */\n",
        "__global__ void blockParReduce2(int *in, int *out, ulong n) {\n",
        "\n",
        "\tuint tid = threadIdx.x;\n",
        "\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\t// boundary check\n",
        "\tif (idx >= n)\n",
        "\t\treturn;\n",
        "\n",
        "\t// convert global data pointer to the local pointer of this block\n",
        "\tint *thisBlock = in + blockIdx.x * blockDim.x;\n",
        "\n",
        "\t// in-place reduction in global memory\n",
        "\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1)  {\n",
        "\t\tif (tid < stride)\n",
        "\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n",
        "\n",
        "\t\t// synchronize within threadblock\n",
        "\t\t__syncthreads();\n",
        "\t}\n",
        "\n",
        "\t// write result for this block to global mem\n",
        "\tif (tid == 0)\n",
        "\t\tout[blockIdx.x] = thisBlock[0];\n",
        "}\n",
        "\n",
        "/*\n",
        " * MAIN: test on parallel reduction\n",
        " */\n",
        "int main(void) {\n",
        "\tint *a, *b, *d_a, *d_b;\n",
        "\tint blockSize = 1024;            // block dim 1D\n",
        "\tulong numBlock = 1024*1024;      // grid dim 1D\n",
        "\tulong n = blockSize * numBlock;  // array dim\n",
        "\tlong sum_CPU = 0, sum_GPU;\n",
        "\tlong nByte = n*sizeof(int), mByte = numBlock * sizeof(int);\n",
        "\tdouble start, stopGPU, stopCPU, speedup;\n",
        "\n",
        "\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n",
        "\n",
        "\t// init\n",
        "\ta = (int *) malloc(nByte);\n",
        "\tb = (int *) malloc(mByte);\n",
        "\tCHECK(cudaMalloc((void **) &d_a, nByte));\n",
        "\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n",
        "\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n",
        "\tCHECK(cudaMalloc((void **) &d_b, mByte));\n",
        "\tCHECK(cudaMemset((void *) d_b, 0, mByte));\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*                     CPU reduction                       */\n",
        "\t/***********************************************************/\n",
        "\tprintf(\"  Vector length: %.2f MB\\n\",n/(1024.0*1024.0));\n",
        "\tprintf(\"\\n  CPU procedure...\\n\");\n",
        "\tstart = seconds();\n",
        "\tfor (ulong i = 0; i < n; i++) sum_CPU += a[i];\n",
        "\tstopCPU = seconds() - start;\n",
        "\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n",
        "\tprintf(\"    sum: %lu\\n\",sum_CPU);\n",
        "\n",
        "\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*         KERNEL blockParReduce1 (divergent)              */\n",
        "\t/***********************************************************/\n",
        "\t// block by block parallel implementation with divergence\n",
        "\tprintf(\"\\n  Launch kernel: blockParReduce1...\\n\");\n",
        "\tstart = seconds();\n",
        "\tblockParReduce1<<<numBlock, blockSize>>>(d_a, d_b, n);\n",
        "\tCHECK(cudaGetLastError());\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tstopGPU = seconds() - start;\n",
        "\tspeedup = stopCPU/stopGPU;\n",
        "\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n",
        "\t// memcopy D2H\n",
        "\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n",
        "\t// check result\n",
        "\tsum_GPU = 0;\n",
        "\tfor (uint i = 0; i < numBlock; i++)\n",
        "\t\tsum_GPU += b[i];\n",
        "\tassert(sum_GPU == n);\n",
        "\t// reset input vector on GPU\n",
        "\tfor (ulong i = 0; i < n; i++) a[i]=1;\n",
        "\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*        KERNEL blockParReduce2  (non divergent)          */\n",
        "\t/***********************************************************/\n",
        "\t// block by block parallel implementation without divergence\n",
        "\tprintf(\"\\n  Launch kernel: blockParReduce2...\\n\");\n",
        "\tstart = seconds();\n",
        "\tblockParReduce2<<<numBlock, blockSize>>>(d_a, d_b, n);\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tstopGPU = seconds() - start;\n",
        "\tspeedup = stopCPU/stopGPU;\n",
        "\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n",
        "\tCHECK(cudaGetLastError());\n",
        "\t// memcopy D2H\n",
        "\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n",
        "\t// check result\n",
        "\tsum_GPU = 0;\n",
        "\tfor (uint i = 0; i < numBlock; i++) {\n",
        "\t\tsum_GPU += b[i];\n",
        "//\t\tprintf(\"b[%d] = %d\\n\",i,b[i]);\n",
        "\t}\n",
        "\tassert(sum_GPU == n);\n",
        "\t// reset input vector on GPU\n",
        "\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n",
        "\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*               KERNEL blockParReduceUroll                */\n",
        "\t/***********************************************************/\n",
        "\t// block by block parallel implementation without divergence\n",
        "\tprintf(\"\\n  Launch kernel: blockParReduceUroll...\\n\");\n",
        "\tstart = seconds();\n",
        "\tblockParReduceUroll<<<numBlock, blockSize>>>(d_a, d_b, n);\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tstopGPU = seconds() - start;\n",
        "\tspeedup = stopCPU/stopGPU;\n",
        "\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n",
        "\tCHECK(cudaGetLastError());\n",
        "\t// memcopy D2H\n",
        "\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n",
        "\t// check result\n",
        "\tsum_GPU = 0;\n",
        "\tfor (uint i = 0; i < numBlock; i++)\n",
        "\t\tsum_GPU += b[i];\n",
        "\tassert(sum_GPU == n);\n",
        "\t// reset input vector on GPU\n",
        "\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n",
        "\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*            KERNEL multBlockParReduceUroll8              */\n",
        "\t/***********************************************************/\n",
        "\t// block by block parallel implementation without divergence\n",
        "\tprintf(\"\\n  Launch kernel: multBlockParReduceUroll8...\\n\");\n",
        "\tstart = seconds();\n",
        "\tmultBlockParReduceUroll8<<<numBlock/8, blockSize>>>(d_a, d_b, n);\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tstopGPU = seconds() - start;\n",
        "\tspeedup = stopCPU/stopGPU;\n",
        "\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n",
        "\tCHECK(cudaGetLastError());\n",
        "\t// memcopy D2H\n",
        "\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n",
        "\t// check result\n",
        "\tsum_GPU = 0;\n",
        "\tfor (uint i = 0; i < numBlock/8; i++)\n",
        "\t\tsum_GPU += b[i];\n",
        "\tprintf(\"    sum: %lu\\n\",sum_GPU);\n",
        "\tassert(sum_GPU == n);\n",
        "\t// reset input vector on GPU\n",
        "\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n",
        "\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*            KERNEL multBlockParReduceUroll16             */\n",
        "\t/***********************************************************/\n",
        "\t// block by block parallel implementation without divergence\n",
        "\tprintf(\"\\n  Launch kernel: multBlockParReduceUroll16...\\n\");\n",
        "\tstart = seconds();\n",
        "\tmultBlockParReduceUroll16<<<numBlock/16, blockSize>>>(d_a, d_b, n);\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tstopGPU = seconds() - start;\n",
        "\tspeedup = stopCPU/stopGPU;\n",
        "\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n",
        "\tCHECK(cudaGetLastError());\n",
        "\t// memcopy D2H\n",
        "\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n",
        "\t// check result\n",
        "\tsum_GPU = 0;\n",
        "\tfor (uint i = 0; i < numBlock/16; i++)\n",
        "\t\tsum_GPU += b[i];\n",
        "\tassert(sum_GPU == n);\n",
        "\n",
        "\tcudaFree(d_a);\n",
        "\n",
        "\tCHECK(cudaDeviceReset());\n",
        "\treturn 0;\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 TODO"
      ],
      "metadata": {
        "id": "4RYX2IU4dwaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile unrolling/preduceUnroll.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "/*\n",
        " *  Device function: block parallel reduction based on warp unrolling\n",
        " */\n",
        "__device__ void blockWarpUnroll(int *thisBlock, int blockDim, uint tid) {\n",
        "  // in-place reduction in global memory\n",
        "    for (int stride = blockDim / 2; stride > 32; stride >>= 1)  {\n",
        "      if (tid < stride)\n",
        "        thisBlock[tid] += thisBlock[tid + stride];\n",
        "\n",
        "      // synchronize within threadblock\n",
        "      __syncthreads();\n",
        "    }\n",
        "\n",
        "    // unrolling warp\n",
        "    if (tid < 32) {\n",
        "      volatile int *vmem = thisBlock;\n",
        "      vmem[tid] += vmem[tid + 32];\n",
        "      vmem[tid] += vmem[tid + 16];\n",
        "      vmem[tid] += vmem[tid + 8];\n",
        "      vmem[tid] += vmem[tid + 4];\n",
        "      vmem[tid] += vmem[tid + 2];\n",
        "      vmem[tid] += vmem[tid + 1];\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        " *  Block by block parallel implementation with warp unrolling\n",
        " */\n",
        "__global__ void blockParReduceUroll(int *in, int *out, ulong n) {\n",
        "\n",
        "\tuint tid = threadIdx.x;\n",
        "\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\t// boundary check\n",
        "\tif (idx >= n)\n",
        "\t\treturn;\n",
        "\n",
        "\t// convert global data pointer to the local pointer of this block\n",
        "\tint *thisBlock = in + blockIdx.x * blockDim.x;\n",
        "\n",
        "  // block parall. reduction based on warp unrolling \n",
        "  blockWarpUnroll(thisBlock, blockDim.x, tid);\n",
        "\n",
        "\t// write result for this block to global mem\n",
        "\tif (tid == 0)\n",
        "\t\tout[blockIdx.x] = thisBlock[0];\n",
        "}\n",
        "\n",
        "/*\n",
        " *  Multi block parallel implementation with block and warp unrolling\n",
        " */\n",
        "__global__ void multBlockParReduceUroll8(int *in, int *out, ulong n) {\n",
        "\n",
        "    ulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    //printf(\"blockIdx.x = %d, blockDim.x = %d,  threadIdx.x = %d\\n\", blockIdx.x, blockDim.x, threadIdx.x);\n",
        "    //printf(\"idx = %d\\n\", idx);\n",
        "    if(idx < n){\n",
        "          out[idx] = \n",
        "                                in[(blockIdx.x) * blockDim.x *8 + threadIdx.x] + \n",
        "                                in[(blockIdx.x) * blockDim.x *8+ threadIdx.x + blockDim.x] + \n",
        "                                in[(blockIdx.x) * blockDim.x *8+ threadIdx.x + 2*blockDim.x] + \n",
        "                                in[(blockIdx.x) * blockDim.x *8+ threadIdx.x + 3*blockDim.x] +\n",
        "                                in[(blockIdx.x) * blockDim.x *8+ threadIdx.x + 4*blockDim.x] +\n",
        "                                in[(blockIdx.x) * blockDim.x *8+ threadIdx.x + 5*blockDim.x] +\n",
        "                                in[(blockIdx.x) * blockDim.x *8+ threadIdx.x + 6*blockDim.x] +\n",
        "                                in[(blockIdx.x) * blockDim.x *8+ threadIdx.x + 7*blockDim.x];\n",
        "                \n",
        "            __syncthreads();\n",
        "\n",
        "            if(threadIdx.x == 0){\n",
        "              // convert global data pointer to the local pointer of this block\n",
        "              int *thisBlock = in + blockIdx.x * blockDim.x;\n",
        "\n",
        "              // block parall. reduction based on warp unrolling \n",
        "              blockWarpUnroll(thisBlock, blockDim.x, threadIdx.x);\n",
        "              \n",
        "              // write result for this block to global mem\n",
        "              \n",
        "              out[blockIdx.x] = thisBlock[0];\n",
        "              \n",
        "            }\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        " *  Multi block parallel implementation with block and warp unrolling\n",
        " */\n",
        "__global__ void multBlockParReduceUroll16(int *in, int *out, ulong n) {\n",
        "\n",
        "\t// TODO\n",
        "\t\n",
        "}\n"
      ],
      "metadata": {
        "id": "NAeBUVaScJKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PSc9B9PDTWt"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "\n",
        "!nvcc -arch=sm_37 unrolling/main.cu unrolling/preduceUnroll.cu   -o preduceUnroll\n",
        "!./preduceUnroll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOFMQZAkjlLW"
      },
      "source": [
        "# ✅ Parallelismo dinamico"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVQVpcvKjkIk"
      },
      "source": [
        "%%writefile MQDB-CUDA-DP/main.cu\n",
        "\n",
        "#include \"../../utils/MQDB/mqdb.h\"\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "#define BLOCK_SIZE 16     // block size\n",
        "\n",
        "struct tms {\n",
        "\tdouble GPUtmsMQDB;\n",
        "\tdouble GPUtmsMQDBDynPar1;\n",
        "\tdouble GPUtmsMQDBDynPark;\n",
        "\tfloat density;\n",
        "};\n",
        "\n",
        "// the kernels prototype\n",
        "__global__ void mqdbBlockProd(mqdb, mqdb, mqdb, uint, uint, uint);\n",
        "__global__ void mqdbProdDP1(mqdb, mqdb, mqdb, uint, uint); \n",
        "__global__ void mqdbProdDPk(mqdb, mqdb, mqdb, uint); \n",
        "\n",
        "/*\n",
        " * Test on MQDB kernels\n",
        " */\n",
        "void testKernelsMQDB(uint n, uint k, struct tms* times) {\n",
        "\n",
        "\t// mqdb host matrices\n",
        "\tmqdb A, B, C, C1;\n",
        "\n",
        "\t// mqdb device matrices\n",
        "\tmqdb d_A, d_B, d_C;\n",
        "\n",
        "\t// fill in\n",
        "\tA = mqdbConst(n, k, 10, 1);\n",
        "\tB = mqdbConst(n, k, 10, 1);\n",
        "\tC = mqdbConst(n, k, 10, 1);\n",
        "\tC1 = mqdbConst(n, k, 10, 1);\n",
        "\n",
        "\tulong nBytes = n * n * sizeof(float);\n",
        "\tulong kBytes = k * sizeof(uint);\n",
        "\tprintf(\"Memory size required = %.1f (MB)\\n\",(float)nBytes/(1024.0*1024.0));\n",
        "\n",
        "\t// malloc and copy on device memory\n",
        "\td_A.nBlocks = A.nBlocks;\n",
        "\tCHECK(cudaMalloc((void**)&d_A.blkSize, kBytes));\n",
        "\tCHECK(cudaMemcpy(d_A.blkSize, A.blkSize, kBytes, cudaMemcpyHostToDevice));\n",
        "\tCHECK(cudaMalloc((void**)&d_A.elem, nBytes));\n",
        "\tCHECK(cudaMemcpy(d_A.elem, A.elem, nBytes, cudaMemcpyHostToDevice));\n",
        "\td_B.nBlocks = B.nBlocks;\n",
        "\tCHECK(cudaMalloc((void**)&d_B.blkSize, kBytes));\n",
        "\tCHECK(cudaMemcpy(d_B.blkSize, B.blkSize, kBytes, cudaMemcpyHostToDevice));\n",
        "\tCHECK(cudaMalloc((void**)&d_B.elem, nBytes));\n",
        "\tCHECK(cudaMemcpy(d_B.elem, B.elem, nBytes, cudaMemcpyHostToDevice));\n",
        "\td_C.nBlocks = C.nBlocks;\n",
        "\tCHECK(cudaMalloc((void**)&d_C.blkSize, kBytes));\n",
        "\tCHECK(cudaMemcpy(d_C.blkSize, C.blkSize, kBytes, cudaMemcpyHostToDevice));\n",
        "\tCHECK(cudaMalloc((void**)&d_C.elem, nBytes));\n",
        "\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*                     GPU MQDB product                    */\n",
        "\t/***********************************************************/\n",
        "\tprintf(\"Kernel MQDB product...\\n\");\n",
        "\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
        "\tdim3 grid((n + block.x - 1) / block.x, (n + block.y - 1) / block.y);\n",
        "\tuint sdim = 0;\n",
        "\tdouble start = seconds();\n",
        "\tfor (uint i = 0; i < k; i++ ) {\n",
        "\t\tuint d = A.blkSize[i];\n",
        "\t\tmqdbBlockProd<<<grid, block>>>(d_A, d_B, d_C, sdim, d, n);\n",
        "\t\tsdim += d;\n",
        "\t}\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tdouble GPUtime2 = seconds() - start;\n",
        "\tprintf(\"   elapsed time:                    %.2f (sec)\\n\", GPUtime2);\n",
        "\t// copy the array 'C' back from the GPU to the CPU\n",
        "\tCHECK(cudaMemcpy(C1.elem, d_C.elem, nBytes, cudaMemcpyDeviceToHost));\n",
        "\tcheckResult(C,C1);\n",
        "\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*              GPU MQDB dynamic par. GRID(1)              */\n",
        "\t/***********************************************************/\n",
        "\tstart = seconds();\n",
        "\tprintf(\"Kernel MQDB product with dynamic parall. GRID(1)...\\n\");\n",
        "\tmqdbProdDP1<<< 1, 1 >>>(d_A, d_B, d_C, k, n);\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tdouble GPUtime3 = seconds() - start;\n",
        "\tprintf(\"   elapsed time:                        %.2f (sec)\\n\", GPUtime3);\n",
        "\tprintf(\"   speedup vs GPU MQDB product:         %.2f\\n\", GPUtime2/GPUtime3);\n",
        "\t// copy the array 'C' back from the GPU to the CPU\n",
        "\tCHECK(cudaMemcpy(C1.elem, d_C.elem, nBytes, cudaMemcpyDeviceToHost));\n",
        "\tcheckResult(C,C1);\n",
        "\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*              GPU MQDB dynamic par. GRID(k)              */\n",
        "\t/***********************************************************/\n",
        "\tstart = seconds();\n",
        "\tprintf(\"Kernel MQDB product with dynamic parall. GRID(k)...\\n\");\n",
        "\tmqdbProdDPk<<< 1, k >>>(d_A, d_B, d_C, n);\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tdouble GPUtime4 = seconds() - start;\n",
        "\tprintf(\"   elapsed time:                        %.2f (sec)\\n\", GPUtime4);\n",
        "\tprintf(\"   speedup vs GPU MQDB product:         %.2f\\n\", GPUtime2/GPUtime4);\n",
        "\tprintf(\"   speedup vs GPU MQDB product GRID(1): %.2f\\n\", GPUtime3/GPUtime4);\n",
        "\t// copy the array 'C' back from the GPU to the CPU\n",
        "\tCHECK(cudaMemcpy(C1.elem, d_C.elem, nBytes, cudaMemcpyDeviceToHost));\n",
        "\tcheckResult(C,C1);\n",
        "\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n",
        "\n",
        "\tCHECK(cudaFree(d_A.elem));\n",
        "\tCHECK(cudaFree(d_B.elem));\n",
        "\tCHECK(cudaFree(d_C.elem));\n",
        "\n",
        "\t// collect times\n",
        "\ttimes->GPUtmsMQDB = GPUtime2;\n",
        "\ttimes->GPUtmsMQDBDynPar1 = GPUtime3;\n",
        "\ttimes->GPUtmsMQDBDynPark = GPUtime4;\n",
        "\tfloat den = 0;\n",
        "\tfor (uint j = 0; j < k; j++)\n",
        "\t\tden += A.blkSize[j]*A.blkSize[j];\n",
        "\ttimes->density = den/(n*n);\n",
        "}\n",
        "\n",
        "/*\n",
        " * main function\n",
        " */\n",
        "int main(int argc, char *argv[]) {\n",
        "\tuint n = 16*1024;      // matrix size\n",
        "\tuint min_k = 10;       // max num of blocks\n",
        "\tuint max_k = 20;       // max num of blocks\n",
        "\n",
        "\tstruct tms times[max_k-min_k+1];\n",
        "\n",
        "\t// multiple tests on kernels\n",
        "\tfor (uint k = min_k; k <= max_k; k++) {\n",
        "\t\tprintf(\"\\n*****   k = %d --- (avg block size = %f)\\n\",k,(float)n/k);\n",
        "\t\ttestKernelsMQDB(n, k, &times[k-min_k]);\n",
        "\t}\n",
        "\n",
        "\tFILE *fd;\n",
        "\tfd = fopen(\"res.csv\", \"w\");\n",
        "\tif (fd == NULL) {\n",
        "\t\tperror(\"file error!\\n\");\n",
        "\t\texit(1);\n",
        "\t}\n",
        "\n",
        "\t// write results on file\n",
        "\tfprintf(fd,\"num blocks,\");\n",
        "\t\tfor (uint j = 0; j <= max_k-min_k; j++)\n",
        "\t\t\tfprintf(fd,\"%d,\",j+min_k);\n",
        "\n",
        "\tfprintf(fd,\"\\nKernel MQDB product,\");\n",
        "\tfor (uint j = 0; j <= max_k-min_k; j++)\n",
        "\t\tfprintf(fd,\"%.4f,\",times[j].GPUtmsMQDB);\n",
        "\n",
        "\tfprintf(fd,\"\\nKernel MQDB product with dynamic parall. GRID(1),\");\n",
        "\tfor (uint j = 0; j <= max_k-min_k; j++)\n",
        "\t\tfprintf(fd,\"%.4f,\",times[j].GPUtmsMQDBDynPar1);\n",
        "\n",
        "\tfprintf(fd,\"\\nKernel MQDB product with dynamic parall. GRID(k),\");\n",
        "\tfor (uint j = 0; j <= max_k-min_k; j++)\n",
        "\t\tfprintf(fd,\"%.4f,\",times[j].GPUtmsMQDBDynPark);\n",
        "\n",
        "\tfprintf(fd,\"\\ndensity,\");\n",
        "\tfor (uint j = 0; j <= max_k-min_k; j++)\n",
        "\t\tfprintf(fd,\"%.4f,\",times[j].density);\n",
        "\n",
        "\tfclose(fd);\n",
        "\n",
        "\treturn 0;\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 TODO"
      ],
      "metadata": {
        "id": "Z2o3XANQf6Zv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9nRkLgeB10A"
      },
      "source": [
        "%%writefile MQDB-CUDA-DP/mqdb_DP.cu\n",
        "\n",
        "#include \"../../utils/MQDB/mqdb.h\"\n",
        "\n",
        "#define BLOCK_SIZE 16     // block size\n",
        "\n",
        "\n",
        "/*\n",
        " * Kernel for block sub-matrix product of mqdb\n",
        " */\n",
        "__global__ void mqdbBlockProd(mqdb A, mqdb B, mqdb C, uint sdim, uint d, uint n) {\n",
        "\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\t// jump to the right block sub-matrix\n",
        "\tint  offset = (n+1)*sdim;\n",
        "\n",
        "\t// each thread computes an entry of the product matrix\n",
        "\tif ((row < d) && (col < d)) {\n",
        "\t\tfloat val = 0;\n",
        "\n",
        "\t\tfor (int k = 0; k < d; k++)\n",
        "\t\t\tval += A.elem[row * n + k + offset] * B.elem[k * n + col + offset];\n",
        "\t\tC.elem[row * n + col + offset] = val;\n",
        "\t}\n",
        "}\n",
        "\n",
        "/*\n",
        " * Kernel for block sub-matrix product of mqdb: parent grid(1)\n",
        " */\n",
        "__global__ void mqdbProdDP1(mqdb A, mqdb B, mqdb C, uint k, uint n) {\n",
        "\t\n",
        "\t// TODO\n",
        "}\n",
        "\n",
        "/*\n",
        " * Kernel for block sub-matrix product of mqdb: parent grid(k)\n",
        " */\n",
        "__global__ void mqdbProdDPk(mqdb A, mqdb B, mqdb C, uint n) {\n",
        "\t\n",
        "\t// TODO\n",
        "\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLxZjCx8bT3s"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "!nvcc -arch=sm_37 -dc MQDB-CUDA-DP/mqdb_DP.cu MQDB-CUDA-DP/main.cu ../utils/MQDB/mqdb.cpp \n",
        "!nvcc -arch=sm_37 mqdb_DP.o main.o mqdb.o -o mqdb_prod\n",
        "!rm -rf *.o\n",
        "!./mqdb_prod"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}