{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUDA_lab6_TODO.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "WoJbB3T5Vkw-",
        "_cGSqZovVkxK",
        "zs_a5Vuimily",
        "iUYP4kCJhEIx",
        "OHR7Zs3dNs1N",
        "9yf0EfM4Gb4Z",
        "SOFMQZAkjlLW",
        "3jMZU51M8beJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **LAB 6 - Shared memory (SMEM)**\n",
        "---"
      ],
      "metadata": {
        "id": "fZYqN0UwVLC_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoJbB3T5Vkw-"
      },
      "source": [
        "# ▶️ CUDA setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fht2Wy8wVkxJ"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jP2H_YJVkxJ"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [GPU Compute Capability](https://developer.nvidia.com/cuda-gpus)"
      ],
      "metadata": {
        "id": "VKbaxH9wWosO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cGSqZovVkxK"
      },
      "source": [
        "## NVCC Plugin for Jupyter notebook\n",
        "\n",
        "*Usage*:\n",
        "\n",
        "\n",
        "*   Load Extension `%load_ext nvcc_plugin`\n",
        "*   Mark a cell to be treated as cuda cell\n",
        "`%%cuda --name example.cu --compile false`\n",
        "\n",
        "**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n",
        "\n",
        "*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCVhMkqYVkxK"
      },
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6PDOytTVkxK"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bash and data setup"
      ],
      "metadata": {
        "id": "cReFlD-VRfZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Bash setup\n",
        "%%writefile /root/.bashrc\n",
        "\n",
        "# If not running interactively, don't do anything\n",
        "[ -z \"$PS1\" ] && return\n",
        "\n",
        "# don't put duplicate lines in the history. See bash(1) for more options\n",
        "# ... or force ignoredups and ignorespace\n",
        "HISTCONTROL=ignoredups:ignorespace\n",
        "\n",
        "# append to the history file, don't overwrite it\n",
        "shopt -s histappend\n",
        "\n",
        "# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)\n",
        "HISTSIZE=10000\n",
        "HISTFILESIZE=20000\n",
        "\n",
        "# check the window size after each command and, if necessary,\n",
        "# update the values of LINES and COLUMNS.\n",
        "shopt -s checkwinsize\n",
        "\n",
        "# make less more friendly for non-text input files, see lesspipe(1)\n",
        "[ -x /usr/bin/lesspipe ] && eval \"$(SHELL=/bin/sh lesspipe)\"\n",
        "\n",
        "PS1='\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ '\n",
        "\n",
        "# enable color support of ls and also add handy aliases\n",
        "if [ -x /usr/bin/dircolors ]; then\n",
        "    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\n",
        "    alias ls='ls --color=auto'\n",
        "    #alias dir='dir --color=auto'\n",
        "    #alias vdir='vdir --color=auto'\n",
        "\n",
        "    alias grep='grep --color=auto'\n",
        "    alias fgrep='fgrep --color=auto'\n",
        "    alias egrep='egrep --color=auto'\n",
        "fi\n",
        "\n",
        "# some more ls aliases\n",
        "alias ll='ls -lF'\n",
        "alias la='ls -A'\n",
        "alias l='ls -CF'\n",
        "\n",
        "# path setup\n",
        "export PATH=\"./:/usr/local/cuda/bin:$PATH\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "O8ICSyy8_GEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source /root/.bashrc"
      ],
      "metadata": {
        "id": "QxIfKO3Ghf7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GPUcomputing site on github..."
      ],
      "metadata": {
        "id": "IYG8Cv4bTzyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/giulianogrossi/GPUcomputing.git"
      ],
      "metadata": {
        "id": "E7jZmHjCT0vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some paths..."
      ],
      "metadata": {
        "id": "ZarLje6wR_Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path setup\n",
        "!mkdir -p /content/GPUcomputing/lab6\n",
        "%cd /content/GPUcomputing/lab6\n",
        "!mkdir -p parReduceSMEM\n",
        "!mkdir -p prodMatSMEM\n",
        "!mkdir -p convolutionSMEM"
      ],
      "metadata": {
        "id": "tC-AaOJlkLOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ▶️ VS Code on Colab"
      ],
      "metadata": {
        "id": "zs_a5Vuimily"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab-ssh tunnel\n",
        "#@markdown Execute this cell to open the ssh tunnel. Check [colab-ssh documentation](https://github.com/WassimBenzarti/colab-ssh) for more details.\n",
        "\n",
        "# Install colab_ssh on google colab\n",
        "!pip install colab_ssh --upgrade\n",
        "\n",
        "from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n",
        "ssh_tunnel_password = \"gpu\" #@param {type: \"string\"}\n",
        "launch_ssh_cloudflared(password=ssh_tunnel_password)\n",
        "\n",
        "# Optional: if you want to clone a Github or Gitlab repository\n",
        "repository_url=\"https://github.com/giulianogrossi/GPUcomputing\" #@param {type: \"string\"}\n",
        "init_git_cloudflared(repository_url)"
      ],
      "metadata": {
        "id": "BCf9JxqphHAp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUYP4kCJhEIx"
      },
      "source": [
        "# ▶️ DeviceQuery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW9b_Yuxi7id"
      },
      "source": [
        "# DeviceQuery dell'attuale device (su Colab!)\n",
        "!nvcc /content/GPUcomputing/utils/deviceQuery.cu -o deviceQuery\n",
        "!./deviceQuery"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Parallel reduction con SMEM\n"
      ],
      "metadata": {
        "id": "OHR7Zs3dNs1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parReduceSMEM/preduceSMEM.cu\n",
        "\n",
        "#include \"../../utils/common.h\"\n",
        "#include <stdio.h>\n",
        "#define DIM 1024\n",
        "\n",
        "/*\n",
        " * An example of using shared memory to optimize performance of a parallel\n",
        " * reduction by constructing partial results for a thread block in shared memory\n",
        " * before flushing to global memory.\n",
        " */\n",
        "\n",
        "extern __shared__ int dsmem[];\n",
        "\n",
        "// Recursive Implementation of Interleaved Pair Approach\n",
        "int recursiveReduce(int *data, int const size) {\n",
        "    if (size == 1) return data[0];\n",
        "\n",
        "    int const stride = size / 2;\n",
        "\n",
        "    for (int i = 0; i < stride; i++)\n",
        "        data[i] += data[i + stride];\n",
        "\n",
        "    return recursiveReduce(data, stride);\n",
        "}\n",
        "\n",
        "// unroll4 + complete unroll for loop + gmem\n",
        "__global__ void reduceGmem(int *g_idata, int *g_odata, unsigned int n) {\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // boundary check\n",
        "    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx >= n) return;\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    if (blockDim.x >= 1024 && tid < 512) idata[tid] += idata[tid + 512];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 512 && tid < 256) idata[tid] += idata[tid + 256];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 256 && tid < 128) idata[tid] += idata[tid + 128];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 128 && tid < 64) idata[tid] += idata[tid + 64];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // unrolling warp\n",
        "    if (tid < 32) {\n",
        "        volatile int *vsmem = idata;\n",
        "        vsmem[tid] += vsmem[tid + 32];\n",
        "        vsmem[tid] += vsmem[tid + 16];\n",
        "        vsmem[tid] += vsmem[tid +  8];\n",
        "        vsmem[tid] += vsmem[tid +  4];\n",
        "        vsmem[tid] += vsmem[tid +  2];\n",
        "        vsmem[tid] += vsmem[tid +  1];\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceSmem(int *g_idata, int *g_odata, unsigned int n) {\n",
        "    __shared__ int smem[DIM];\n",
        "\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "\n",
        "    // boundary check\n",
        "    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx >= n) return;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // set to smem by each threads\n",
        "    smem[tid] = idata[tid];\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction in shared memory\n",
        "    if (blockDim.x >= 1024 && tid < 512) smem[tid] += smem[tid + 512];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 512 && tid < 256) smem[tid] += smem[tid + 256];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 256 && tid < 128) smem[tid] += smem[tid + 128];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 128 && tid < 64)  smem[tid] += smem[tid + 64];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // unrolling warp\n",
        "    if (tid < 32)\n",
        "    {\n",
        "        volatile int *vsmem = smem;\n",
        "        vsmem[tid] += vsmem[tid + 32];\n",
        "        vsmem[tid] += vsmem[tid + 16];\n",
        "        vsmem[tid] += vsmem[tid +  8];\n",
        "        vsmem[tid] += vsmem[tid +  4];\n",
        "        vsmem[tid] += vsmem[tid +  2];\n",
        "        vsmem[tid] += vsmem[tid +  1];\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = smem[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceSmemDyn(int *g_idata, int *g_odata, unsigned int n) {\n",
        "    extern __shared__ int smem[];\n",
        "\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // set to smem by each threads\n",
        "    smem[tid] = idata[tid];\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    if (blockDim.x >= 1024 && tid < 512)  smem[tid] += smem[tid + 512];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 512 && tid < 256)  smem[tid] += smem[tid + 256];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 256 && tid < 128) smem[tid] += smem[tid + 128];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 128 && tid < 64) smem[tid] += smem[tid + 64];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // unrolling warp\n",
        "    if (tid < 32)\n",
        "    {\n",
        "        volatile int *vsmem = smem;\n",
        "        vsmem[tid] += vsmem[tid + 32];\n",
        "        vsmem[tid] += vsmem[tid + 16];\n",
        "        vsmem[tid] += vsmem[tid +  8];\n",
        "        vsmem[tid] += vsmem[tid +  4];\n",
        "        vsmem[tid] += vsmem[tid +  2];\n",
        "        vsmem[tid] += vsmem[tid +  1];\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = smem[0];\n",
        "}\n",
        "\n",
        "// unroll4 + complete unroll for loop + gmem\n",
        "__global__ void reduceGmemUnroll(int *g_idata, int *g_odata, unsigned int n) {\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x * 4;\n",
        "\n",
        "    // unrolling 4\n",
        "    if (idx + 3 * blockDim.x < n)\n",
        "    {\n",
        "        int a1 = g_idata[idx];\n",
        "        int a2 = g_idata[idx + blockDim.x];\n",
        "        int a3 = g_idata[idx + 2 * blockDim.x];\n",
        "        int a4 = g_idata[idx + 3 * blockDim.x];\n",
        "        g_idata[idx] = a1 + a2 + a3 + a4;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    if (blockDim.x >= 1024 && tid < 512) idata[tid] += idata[tid + 512];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 512 && tid < 256) idata[tid] += idata[tid + 256];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 256 && tid < 128) idata[tid] += idata[tid + 128];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 128 && tid < 64) idata[tid] += idata[tid + 64];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // unrolling warp\n",
        "    if (tid < 32)\n",
        "    {\n",
        "        volatile int *vsmem = idata;\n",
        "        vsmem[tid] += vsmem[tid + 32];\n",
        "        vsmem[tid] += vsmem[tid + 16];\n",
        "        vsmem[tid] += vsmem[tid +  8];\n",
        "        vsmem[tid] += vsmem[tid +  4];\n",
        "        vsmem[tid] += vsmem[tid +  2];\n",
        "        vsmem[tid] += vsmem[tid +  1];\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceSmemUnroll(int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    // static shared memory\n",
        "    __shared__ int smem[DIM];\n",
        "\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "\n",
        "    // global index, 4 blocks of input data processed at a time\n",
        "    unsigned int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n",
        "\n",
        "    // unrolling 4 blocks\n",
        "    int tmpSum = 0;\n",
        "\n",
        "    // boundary check\n",
        "    if (idx + 4 * blockDim.x <= n)\n",
        "    {\n",
        "        int a1 = g_idata[idx];\n",
        "        int a2 = g_idata[idx + blockDim.x];\n",
        "        int a3 = g_idata[idx + 2 * blockDim.x];\n",
        "        int a4 = g_idata[idx + 3 * blockDim.x];\n",
        "        tmpSum = a1 + a2 + a3 + a4;\n",
        "    }\n",
        "\n",
        "    smem[tid] = tmpSum;\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction in shared memory\n",
        "    if (blockDim.x >= 1024 && tid < 512) smem[tid] += smem[tid + 512];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 512 && tid < 256)  smem[tid] += smem[tid + 256];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 256 && tid < 128)  smem[tid] += smem[tid + 128];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 128 && tid < 64)   smem[tid] += smem[tid + 64];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // unrolling warp\n",
        "    if (tid < 32)\n",
        "    {\n",
        "        volatile int *vsmem = smem;\n",
        "        vsmem[tid] += vsmem[tid + 32];\n",
        "        vsmem[tid] += vsmem[tid + 16];\n",
        "        vsmem[tid] += vsmem[tid +  8];\n",
        "        vsmem[tid] += vsmem[tid +  4];\n",
        "        vsmem[tid] += vsmem[tid +  2];\n",
        "        vsmem[tid] += vsmem[tid +  1];\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = smem[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceSmemUnrollDyn(int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    extern __shared__ int smem[];\n",
        "\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n",
        "\n",
        "    // unrolling 4\n",
        "    int tmpSum = 0;\n",
        "\n",
        "    if (idx + 3 * blockDim.x < n)\n",
        "    {\n",
        "        int a1 = g_idata[idx];\n",
        "        int a2 = g_idata[idx + blockDim.x];\n",
        "        int a3 = g_idata[idx + 2 * blockDim.x];\n",
        "        int a4 = g_idata[idx + 3 * blockDim.x];\n",
        "        tmpSum = a1 + a2 + a3 + a4;\n",
        "    }\n",
        "\n",
        "    smem[tid] = tmpSum;\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    if (blockDim.x >= 1024 && tid < 512)  smem[tid] += smem[tid + 512];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 512 && tid < 256)  smem[tid] += smem[tid + 256];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 256 && tid < 128) smem[tid] += smem[tid + 128];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 128 && tid < 64) smem[tid] += smem[tid + 64];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // unrolling warp\n",
        "    if (tid < 32)\n",
        "    {\n",
        "        volatile int *vsmem = smem;\n",
        "        vsmem[tid] += vsmem[tid + 32];\n",
        "        vsmem[tid] += vsmem[tid + 16];\n",
        "        vsmem[tid] += vsmem[tid +  8];\n",
        "        vsmem[tid] += vsmem[tid +  4];\n",
        "        vsmem[tid] += vsmem[tid +  2];\n",
        "        vsmem[tid] += vsmem[tid +  1];\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = smem[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceNeighboredGmem(int *g_idata, int *g_odata,\n",
        "                                     unsigned int  n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // boundary check\n",
        "    if (idx >= n) return;\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = 1; stride < blockDim.x; stride *= 2)\n",
        "    {\n",
        "        if ((tid % (2 * stride)) == 0)\n",
        "        {\n",
        "            idata[tid] += idata[tid + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceNeighboredSmem(int *g_idata, int *g_odata,\n",
        "                                     unsigned int  n)\n",
        "{\n",
        "    __shared__ int smem[DIM];\n",
        "\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // boundary check\n",
        "    if (idx >= n) return;\n",
        "\n",
        "    smem[tid] = idata[tid];\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = 1; stride < blockDim.x; stride *= 2)\n",
        "    {\n",
        "        if ((tid % (2 * stride)) == 0)\n",
        "        {\n",
        "            smem[tid] += smem[tid + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = smem[0];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"%s starting reduction at \", argv[0]);\n",
        "    printf(\"device %d: %s \", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    bool bResult = false;\n",
        "\n",
        "    // initialization\n",
        "    int power = 10;\n",
        "\n",
        "    // execution configuration\n",
        "    int blocksize = DIM;   // initial block size\n",
        "\n",
        "    if (argc >= 2) {\n",
        "        blocksize = atoi(argv[1]);\n",
        "    }\n",
        "\n",
        "    if (argc >= 3) {\n",
        "        power = atoi(argv[2]);\n",
        "    }\n",
        "\n",
        "    int size = 1 << power; // total number of elements to reduce\n",
        "    printf(\"    with array size %d  \", size);\n",
        "\n",
        "    dim3 block (blocksize, 1);\n",
        "    dim3 grid  ((size + block.x - 1) / block.x, 1);\n",
        "    printf(\"grid %d block %d\\n\", grid.x, block.x);\n",
        "\n",
        "    // allocate host memory\n",
        "    size_t bytes = size * sizeof(int);\n",
        "    int *h_idata = (int *) malloc(bytes);\n",
        "    int *h_odata = (int *) malloc(grid.x * sizeof(int));\n",
        "    int *tmp     = (int *) malloc(bytes);\n",
        "\n",
        "    // initialize the array\n",
        "    for (int i = 0; i < size; i++)\n",
        "        h_idata[i] = (int)( rand() & 0xFF );\n",
        "\n",
        "    memcpy (tmp, h_idata, bytes);\n",
        "\n",
        "    int gpu_sum = 0;\n",
        "\n",
        "    // allocate device memory\n",
        "    int *d_idata = NULL;\n",
        "    int *d_odata = NULL;\n",
        "    CHECK(cudaMalloc((void **) &d_idata, bytes));\n",
        "    CHECK(cudaMalloc((void **) &d_odata, grid.x * sizeof(int)));\n",
        "\n",
        "    // cpu reduction\n",
        "    int cpu_sum = recursiveReduce (tmp, size);\n",
        "    printf(\"cpu reduce          : %d\\n\", cpu_sum);\n",
        "\n",
        "    // reduce gmem\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    reduceNeighboredGmem<<<grid.x, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaGetLastError());\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"reduceNeighboredGmem: %d <<<grid %d block %d>>>\\n\", gpu_sum, grid.x,\n",
        "           block.x);\n",
        "\n",
        "    // reduce gmem\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    reduceNeighboredSmem<<<grid.x, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaGetLastError());\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"reduceNeighboredSmem: %d <<<grid %d block %d>>>\\n\", gpu_sum, grid.x,\n",
        "           block.x);\n",
        "\n",
        "    // reduce gmem\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    reduceGmem<<<grid.x, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaGetLastError());\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"reduceGmem          : %d <<<grid %d block %d>>>\\n\", gpu_sum, grid.x,\n",
        "           block.x);\n",
        "\n",
        "    // reduce smem\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    reduceSmem<<<grid.x, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaGetLastError());\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"reduceSmem          : %d <<<grid %d block %d>>>\\n\", gpu_sum, grid.x,\n",
        "           block.x);\n",
        "\n",
        "    // reduce smem\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    reduceSmemDyn<<<grid.x, block, blocksize*sizeof(int)>>>(d_idata, d_odata,\n",
        "            size);\n",
        "    CHECK(cudaGetLastError());\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"reduceSmemDyn       : %d <<<grid %d block %d>>>\\n\", gpu_sum, grid.x,\n",
        "           block.x);\n",
        "\n",
        "    // reduce gmem\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    reduceGmemUnroll<<<grid.x / 4, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaGetLastError());\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 4 * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x / 4; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"reduceGmemUnroll4   : %d <<<grid %d block %d>>>\\n\", gpu_sum,\n",
        "            grid.x / 4, block.x);\n",
        "\n",
        "    // reduce smem\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    reduceSmemUnroll<<<grid.x / 4, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaGetLastError());\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 4 * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x / 4; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"reduceSmemUnroll4   : %d <<<grid %d block %d>>>\\n\", gpu_sum,\n",
        "            grid.x / 4, block.x);\n",
        "\n",
        "    // reduce smem\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    reduceSmemUnrollDyn<<<grid.x / 4, block, DIM*sizeof(int)>>>(d_idata,\n",
        "            d_odata, size);\n",
        "    CHECK(cudaGetLastError());\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 4 * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x / 4; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"reduceSmemDynUnroll4: %d <<<grid %d block %d>>>\\n\", gpu_sum,\n",
        "            grid.x / 4, block.x);\n",
        "\n",
        "    // free host memory\n",
        "    free(h_idata);\n",
        "    free(h_odata);\n",
        "\n",
        "    // free device memory\n",
        "    CHECK(cudaFree(d_idata));\n",
        "    CHECK(cudaFree(d_odata));\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    // check the results\n",
        "    bResult = (gpu_sum == cpu_sum);\n",
        "\n",
        "    if(!bResult) printf(\"Test failed!\\n\");\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "}\n"
      ],
      "metadata": {
        "id": "aF_XWzGyNmBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_37 parReduceSMEM/preduceSMEM.cu -o preduce\n",
        "!./preduce 512 12"
      ],
      "metadata": {
        "id": "sCBstFF3U1Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 TODO"
      ],
      "metadata": {
        "id": "9yf0EfM4Gb4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confrontare tra loro le versioni con e senza SMEM (dinamica e statica)"
      ],
      "metadata": {
        "id": "7fMQzsShGidW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXUIQkZLCTcG"
      },
      "source": [
        "# ✅ Moltiplicazione matriciale con SMEM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 TODO"
      ],
      "metadata": {
        "id": "dfx5xovl8DYv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y52R0d3CA50"
      },
      "source": [
        "%%writefile prodMatSMEM/prodMatSMEM.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "#define IDX(i,j,n) (i*n+j)\n",
        "#define ABS(x,y) (x-y>=0?x-y:y-x)\n",
        "#define MAX(x,y) (x>y?x:y)\n",
        "#define N 789\n",
        "#define P 1029\n",
        "#define M 342\n",
        "#define DEBUG 0\n",
        "\n",
        "#define BLOCK_SIZE 16\n",
        "\n",
        "void printMatrix(float* A, int rows, int cols){\n",
        "\t\tfor (int i = 0; i < rows; i++){\n",
        "\t\t\tprintf(\"%d) \", i);\n",
        "\t\t\tfor (int j = 0; j < cols; j++) {\n",
        "\t\t\t\tprintf(\"%f \", A[i * cols + j]);\n",
        "\t\t\t}\n",
        "\t\t\tprintf(\"\\n\");\n",
        "\t\t}\n",
        "}\n",
        "\n",
        "\n",
        "/*\n",
        " * Kernel for matrix product with static SMEM\n",
        " *      C  =  A  *  B\n",
        " *    (NxM) (MxP) (PxM)\n",
        " */\n",
        "\n",
        "__global__ void matProdSMEMstatic(float* A, float* B, float* C) {\n",
        "\n",
        "\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\n",
        "\t__shared__ float A_shared[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\t__shared__ float B_shared[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "\tfloat sum = 0;\n",
        "\tint numOfBlocks = (P + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\t\n",
        "\tfor(int i = 0; i < numOfBlocks; i++){\n",
        "\t\t\tif((row*P) + (i*BLOCK_SIZE) + threadIdx.x < N*P) A_shared[threadIdx.y][threadIdx.x] = A[(row*P) + (i*BLOCK_SIZE) + threadIdx.x];\n",
        "\t\t\tif((M*i*BLOCK_SIZE) + (col) + (M*threadIdx.y) < P*M) B_shared[threadIdx.y][threadIdx.x] = B[(M*i*BLOCK_SIZE) + (col) + (M*threadIdx.y)];\t//mi fa strano che non mi dica \"illegal memory access\" ogni tanto\n",
        "\t\t\t__syncthreads();\n",
        "\n",
        "\t\t\tif(row < N && col < M){\n",
        "\t\t\t\t\tfor(int k = 0; k < BLOCK_SIZE; k++){\t\t\n",
        "\t\t\t\t\t\t\tif(k + i*BLOCK_SIZE < P){\t\t\t//threadIdx.y + i*BLOCK_SIZE < P && threadIdx.x + i*BLOCK_SIZE < P\n",
        "\t\t\t\t\t\t\t\t\tsum += A_shared[threadIdx.y][k] * B_shared[k][threadIdx.x];\n",
        "\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t}\n",
        "\n",
        "\t\t\t\t\tif(DEBUG){\n",
        "\t\t\t\t\t\t\tif(row == 32 && col == 0){\n",
        "\t\t\t\t\t\t\t\t\tprintf(\"sum = %f\\n\", sum);\n",
        "\t\t\t\t\t\t\t\t\tprintf(\"A a questa iterazione = \\n\");\n",
        "\t\t\t\t\t\t\t\t\tfor (int ii = 0; ii < BLOCK_SIZE; ii++){\n",
        "\t\t\t\t\t\t\t\t\t\t\tfor (int j = 0; j < BLOCK_SIZE; j++) {\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tprintf(\"%f \", A_shared[ii][j]);\n",
        "\t\t\t\t\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\t\t\t\t\tprintf(\"\\n\");\n",
        "\t\t\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\t\t\tprintf(\"B a questa iterazione = \\n\");\n",
        "\t\t\t\t\t\t\t\t\tfor (int ii = 0; ii < BLOCK_SIZE; ii++){\n",
        "\t\t\t\t\t\t\t\t\t\t\tfor (int j = 0; j < BLOCK_SIZE; j++) {\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tprintf(\"%f \", B_shared[ii][j]);\n",
        "\t\t\t\t\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\t\t\t\t\tprintf(\"\\n\");\n",
        "\t\t\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\n",
        "\t\t\t\t\t\n",
        "\t\t\t}\n",
        "\t\t\t__syncthreads();\n",
        "\t}\n",
        "\t\n",
        "\tif(row < N && col < M){\n",
        "\t\t\tC[row * M + col] = sum;\n",
        "\t}\n",
        "\t\n",
        "}\n",
        "\n",
        "\n",
        "/*\n",
        " * Kernel for matrix product using dynamic SMEM\n",
        " */\n",
        "__global__ void matProdSMEMdynamic(float* A, float* B, float* C, const uint SMEMsize) {\n",
        "\t\n",
        "\t// TODO\n",
        "\t\n",
        "}\n",
        "\n",
        "/*\n",
        " * Kernel for naive matrix product\n",
        " */\n",
        "__global__ void matProd(float* A, float* B, float* C) {\n",
        "\t// indexes\n",
        "\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\t// each thread computes an entry of the product matrix\n",
        "\tif ((row < N) && (col < M)) {\n",
        "\t\tfloat sum = 0;\n",
        "\t\tfor (int k = 0; k < P; k++)\n",
        "\t\t\tsum += A[row * P + k] * B[k * M + col];\n",
        "\t\tC[row * M + col] = sum;\n",
        "\t}\n",
        "}\n",
        "\n",
        "/*\n",
        " *  matrix product on CPU\n",
        " */\n",
        "void matProdCPU(float* A, float* B, float* C) {\n",
        "\n",
        "\tfor (int i = 0; i < N; i++)\n",
        "\t\tfor (int j = 0; j < M; j++) {\n",
        "\t\t\tfloat sum = 0;\n",
        "\t\t\tfor (int k = 0; k < P; k++)\n",
        "\t\t\t\tsum += A[i * P + k] * B[k * M + j];\n",
        "\t\t\tC[i * M + j] = sum;\n",
        "\t\t}\n",
        "}\n",
        "\n",
        "/*\n",
        " * Test the device\n",
        " */\n",
        "unsigned long testCUDADevice(void) {\n",
        "\tint dev = 0;\n",
        "\n",
        "\tcudaDeviceSetCacheConfig (cudaFuncCachePreferEqual);\n",
        "\tcudaDeviceProp deviceProp;\n",
        "\tcudaSetDevice(dev);\n",
        "\tcudaGetDeviceProperties(&deviceProp, dev);\n",
        "\tprintf(\"Device %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n",
        "\tprintf(\"Total amount of shared memory available per block: %lu KB\\n\",\n",
        "\t\t\tdeviceProp.sharedMemPerBlock / 1024);\n",
        "\treturn deviceProp.sharedMemPerBlock;\n",
        "}\n",
        "\n",
        "\n",
        "/*\n",
        " * elementwise comparison between two mqdb\n",
        " */\n",
        "void checkResult(float *A, float *B) {\n",
        "\tdouble epsilon = 1.0E-8;\n",
        "\tbool match = 1;\n",
        "\tfor (int i = 0; i < N*M; i++)\n",
        "\t\tif (ABS(A[i], B[i]) > epsilon) {\n",
        "\t\t\tmatch = 0;\n",
        "\t\t\tprintf(\"   * Arrays do not match!\\n\");\n",
        "\t\t\tbreak;\n",
        "\t\t}\n",
        "\tif (match)\n",
        "\t\tprintf(\"   Arrays match\\n\\n\");\n",
        "}\n",
        "\n",
        "/*\n",
        " * MAIN\n",
        " */\n",
        "int main(void) {\n",
        "\t // Kernels for matrix product\n",
        "\t //      C  =  A  *  B\n",
        "\t //    (NxM) (MxP) (PxM)\n",
        "\tuint rowA = N, rowB = P;\n",
        "\tuint colA = P, colB = M;\n",
        "\tuint rowC = N, colC = M;\n",
        "\tfloat *A, *B, *C, *C1;\n",
        "\tfloat *dev_A, *dev_B, *dev_C;\n",
        "\n",
        "\t// dims\n",
        "\tunsigned long Asize = rowA * colA * sizeof(float);\n",
        "\tunsigned long Bsize = rowB * colB * sizeof(float);\n",
        "\tunsigned long Csize = rowC * colC * sizeof(float);\n",
        "\tunsigned long maxSMEMbytes;\n",
        "\tuint nByteSMEM = 2 * BLOCK_SIZE * BLOCK_SIZE * sizeof(float);\n",
        "\tprintf(\"N = %d, M = %d, P = %d\\n\",N,M,P);\n",
        "\n",
        "\t// test device shared memory\n",
        "\tmaxSMEMbytes = testCUDADevice();\n",
        "\tif (maxSMEMbytes < nByteSMEM)\n",
        "\t\tprintf(\"Shared memory usage WARNING: available: %lu, required: %d bytes\\n\",\n",
        "\t\t\t\tmaxSMEMbytes, nByteSMEM);\n",
        "\telse\n",
        "\t\tprintf(\"Total amount of shared memory required per block %.1f KB\\n\",\n",
        "\t\t\t\t(float) nByteSMEM / (float) 1024);\n",
        "\n",
        "\t// malloc host memory\n",
        "\tA = (float*) malloc(Asize);\n",
        "\tB = (float*) malloc(Bsize);\n",
        "\tC = (float*) malloc(Csize);\n",
        "\tC1 = (float*) malloc(Csize);\n",
        "\n",
        "\t// malloc device memory\n",
        "\tCHECK(cudaMalloc((void** )&dev_A, Asize));\n",
        "\tCHECK(cudaMalloc((void** )&dev_B, Bsize));\n",
        "\tCHECK(cudaMalloc((void** )&dev_C, Csize));\n",
        "\tprintf(\"Total amount of allocated memory on GPU %lu bytes\\n\\n\",\n",
        "\t\t\tAsize + Bsize + Csize);\n",
        "\n",
        "\t// fill the matrices A and B\n",
        "\tfor (int i = 0; i < N * P; i++)\n",
        "\t\tA[i] = rand() % 10;\n",
        "\tfor (int i = 0; i < P * M; i++)\n",
        "\t\tB[i] = rand() % 10;\n",
        "\n",
        "\tdouble start = seconds();\n",
        "\tmatProdCPU(A, B, C);\n",
        "\tprintf(\"    elapsed time CPU = %f\\n\", seconds() - start);\n",
        "\n",
        "\t// copy matrices A and B to the GPU\n",
        "\tCHECK(cudaMemcpy(dev_A, A, Asize, cudaMemcpyHostToDevice));\n",
        "\tCHECK(cudaMemcpy(dev_B, B, Bsize, cudaMemcpyHostToDevice));\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*              GPU matProdSMEM static SMEM               */\n",
        "\t/***********************************************************/\n",
        "\t// grid block dims = shared mem dims = BLOCK_SIZE\n",
        "\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
        "\tdim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n",
        "\tstart = seconds();\n",
        "\tprintf(\"%d %d\\n\", grid.x, grid.y);\n",
        "\tprintf(\"%d %d\\n\", block.x, block.y);\n",
        "\tmatProdSMEMstatic<<<grid, block>>>(dev_A, dev_B, dev_C);\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tprintf(\"   Kernel matProdSMEM static elapsed time GPU = %f\\n\", seconds() - start);\n",
        "\n",
        "\t// copy the array 'C' back from the GPU to the CPU\n",
        "\tCHECK(cudaMemcpy(C1, dev_C, Csize, cudaMemcpyDeviceToHost));\n",
        "\tcheckResult(C,C1);\n",
        "\n",
        "\tif(DEBUG){\n",
        "\t\t\tprintf(\"A =\\n\");\n",
        "\t\t\t//printMatrix(A, N, P);\n",
        "\t\t\tprintf(\"B =\\n\");\n",
        "\t\t\t//printMatrix(B, P, M);\n",
        "\t\t\t//printMatrix(C, N, M);\n",
        "\t\t\tprintf(\"Eccome se c'è un pattern -cit\\n\");\n",
        "\t\t\t//printMatrix(C1, N, M);\n",
        "\t}\n",
        "\n",
        "\t/***********************************************************/\n",
        "\t/*            GPU matProdSMEMD dynamic SMEM                */\n",
        "\t/***********************************************************/\n",
        "\t// set cache size\n",
        "\tcudaDeviceSetCacheConfig (cudaFuncCachePreferShared);\n",
        "\n",
        "\t// try with various SMEM sizes\n",
        "\tuint sizes[] = {8,16,32};\n",
        "\tfor (int i = 0; i < 3; i++) {\n",
        "\t\tuint blockSize = sizes[i];\n",
        "\t\tblock.x = blockSize;\n",
        "\t\tblock.y = blockSize;\n",
        "\t\tgrid.x = (M + block.x - 1) / block.x;\n",
        "\t\tgrid.y = (N + block.y - 1) / block.y;\n",
        "\t\tuint SMEMsize = blockSize * blockSize;\n",
        "\t\tuint SMEMbyte = 2 * SMEMsize * sizeof(float);\n",
        "\t\tstart = seconds();\n",
        "\t\tmatProdSMEMdynamic<<< grid, block, SMEMbyte >>>(dev_A, dev_B, dev_C, SMEMsize);\n",
        "\t\tCHECK(cudaDeviceSynchronize());\n",
        "\t\tprintf(\"   Kernel matProdSMEM dynamic (SMEM size %d) elapsed time GPU = %f\\n\", blockSize, seconds() - start);\n",
        "\n",
        "\t\t// copy the array 'C' back from the GPU to the CPU\n",
        "\t\tCHECK(cudaMemcpy(C1, dev_C, Csize, cudaMemcpyDeviceToHost));\n",
        "\t\tcheckResult(C,C1);\n",
        "\t}\n",
        "\n",
        "\t// free the memory allocated on the GPU\n",
        "\tcudaFree(dev_A);\n",
        "\tcudaFree(dev_B);\n",
        "\tcudaFree(dev_C);\n",
        "\n",
        "\tcudaDeviceReset();\n",
        "\treturn EXIT_SUCCESS;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PSc9B9PDTWt"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "\n",
        "!nvcc -arch=sm_37 ./prodMatSMEM/prodMatSMEM.cu -o prodMatSmem\n",
        "!./prodMatSmem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfR471rze2p-"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOFMQZAkjlLW"
      },
      "source": [
        "# ✅ Convoluzione con SMEM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9nRkLgeB10A"
      },
      "source": [
        "%%writefile convolutionSMEM/conv1D.cu\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "#define MASK_RADIUS  5\n",
        "#define MASK_SIZE    2 * MASK_RADIUS + 1\n",
        "#define BLOCK_SIZE   128\n",
        "#define TILE_WIDTH   BLOCK_SIZE + MASK_SIZE - 1\n",
        "\n",
        "__device__ __constant__ float d_mask[MASK_SIZE];\n",
        "\n",
        "void initialData(float*, int);\n",
        "void movingAverage(float*, int n);\n",
        "void printData(float*, const int);\n",
        "void convolutionHost(float*, float*, float*, const int);\n",
        "void checkResult(float*, float*, int);\n",
        "\n",
        "/*\n",
        " * kernel for 1D convolution: it holds only if MASK_RADIUS < BLOCK_SIZE\n",
        " */\n",
        "__global__ void convolution1D(float *result, float *data, int n) {\n",
        "\tunsigned int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "\n",
        "\t// shared memory size = BLOCK_SIZE + MASK\n",
        "\t__shared__ float tile[TILE_WIDTH];\n",
        "\n",
        "\t// boundary\n",
        "\tint left = blockIdx.x * blockDim.x - MASK_RADIUS;\n",
        "\tint right = (blockIdx.x + 1) * blockDim.x;\n",
        "\n",
        "  // left halo\n",
        "\tif (threadIdx.x < MASK_RADIUS)                      \n",
        "\t\ttile[threadIdx.x] = left < 0 ? 0 : data[left + threadIdx.x];\n",
        "\n",
        "  // center\n",
        "\ttile[threadIdx.x + MASK_RADIUS] = data[i];\n",
        "\n",
        "  // right halo  \n",
        "\tif (threadIdx.x >= blockDim.x - MASK_RADIUS)  \n",
        "\t\ttile[threadIdx.x + MASK_SIZE - 1] = right >= n ? 0 :\n",
        "\t\t\t\tdata[right + threadIdx.x - blockDim.x + MASK_RADIUS];\n",
        "\n",
        "\t__syncthreads();\n",
        "\n",
        "\t// convolution: tile * mask\n",
        "\tfloat sum = 0;\n",
        "\tfor (int i = -MASK_RADIUS; i <= MASK_RADIUS; i++)\n",
        "\t\tsum += tile[threadIdx.x + MASK_RADIUS + i] * d_mask[i + MASK_RADIUS];\n",
        "\n",
        "\t// final result\n",
        "\tresult[i] = sum;\n",
        "}\n",
        "\n",
        "/*\n",
        " * MAIN: convolution 1D host & device\n",
        " */\n",
        "int main(int argc, char **argv) {\n",
        "\t// set up device\n",
        "\tint dev = 0;\n",
        "\tcudaDeviceProp deviceProp;\n",
        "\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "\tprintf(\"starting conv1D at device %d: %s\\n\", dev, deviceProp.name);\n",
        "\tCHECK(cudaSetDevice(dev));\n",
        "\n",
        "\t// set up array size\n",
        "\tint n = 1 << 24;\n",
        "\tint N = MASK_SIZE;\n",
        "\n",
        "\tprintf(\"Array of size = %.1f MB\\n\", n/(1024.0*1024.0));\n",
        "\n",
        "\t// mem sizes\n",
        "\tsize_t nBytes = n * sizeof(float);\n",
        "\tsize_t nBytes_mask = N * sizeof(float);\n",
        "\n",
        "\t// grid configuration\n",
        "\tdim3 block(BLOCK_SIZE);\n",
        "\tdim3 grid((n + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "\t// allocate host memory\n",
        "\tfloat *h_data = (float *) malloc(nBytes);\n",
        "\tfloat *h_result = (float *) malloc(nBytes);\n",
        "\tfloat *result = (float *) malloc(nBytes);\n",
        "\tfloat *h_mask = (float *) malloc(nBytes_mask);\n",
        "\n",
        "\t//  initialize host array\n",
        "\tmovingAverage(h_mask, N);\n",
        "\tinitialData(h_data, n);\n",
        "\n",
        "\t// convolution on host\n",
        "\tdouble start = seconds();\n",
        "\tconvolutionHost(h_data, result, h_mask, n);\n",
        "\tdouble hostElaps = seconds() - start;\n",
        "\n",
        "\t// allocate device memory\n",
        "\tfloat *d_data, *d_result;\n",
        "\tCHECK(cudaMalloc((void**)&d_data, nBytes));\n",
        "\tCHECK(cudaMalloc((void**)&d_result, nBytes));\n",
        "\n",
        "\t// copy data from host to device\n",
        "\tCHECK(cudaMemcpy(d_data, h_data, nBytes, cudaMemcpyHostToDevice));\n",
        "\tCHECK(cudaMemcpyToSymbol(d_mask, h_mask, nBytes_mask));\n",
        "\n",
        "\tstart = seconds();\n",
        "\tconvolution1D<<<grid, block>>>(d_result, d_data, n);\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tdouble devElaps = seconds() - start;\n",
        "  printf(\"Times:\\n\");\n",
        "\tprintf(\"   - CPU elapsed time = %f\\n\", hostElaps);\n",
        "  printf(\"   - GPU elapsed time = %f\\n\", devElaps);\n",
        "  printf(\"   - Speed-up (ratio) = %f\\n\", hostElaps / devElaps);\n",
        "\n",
        "\tCHECK(cudaMemcpy(h_result, d_result, nBytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "\t// check result\n",
        "\tcheckResult(h_result, result, n);\n",
        "\n",
        "\t// free host and device memory\n",
        "\tCHECK(cudaFree(d_result));\n",
        "\tCHECK(cudaFree(d_data));\n",
        "\tfree(h_data);\n",
        "\tfree(h_mask);\n",
        "\tfree(h_result);\n",
        "\tfree(result);\n",
        "\n",
        "\t// reset device\n",
        "\tCHECK(cudaDeviceReset());\n",
        "\treturn EXIT_SUCCESS;\n",
        "}\n",
        "\n",
        "void initialData(float *h_data, int n) {\n",
        "\t// initialize the data\n",
        "\tfor (int i = 0; i < n; i++)\n",
        "\t\th_data[i] = 10.0;\n",
        "}\n",
        "\n",
        "void movingAverage(float *h_mask, int n) {\n",
        "\t// initialize mask moving average\n",
        "\tfor (int i = 0; i < n; i++)\n",
        "\t\th_mask[i] = 1.0 / ((float) n);\n",
        "\treturn;\n",
        "}\n",
        "\n",
        "void printData(float *a, const int size) {\n",
        "\tprintf(\"\\n\");\n",
        "\tfor (int i = 0; i < size; i++)\n",
        "\t\tprintf(\"%.2f \", a[i]);\n",
        "\tprintf(\"\\n\");\n",
        "\treturn;\n",
        "}\n",
        "\n",
        "void convolutionHost(float *data, float *result, float *mask, const int n) {\n",
        "\tfor (int i = 0; i < n; i++) {\n",
        "\t\tfloat sum = 0;\n",
        "\t\tfor (int j = 0; j < MASK_SIZE; j++) {\n",
        "\t\t\tint idx = i - MASK_RADIUS + j;\n",
        "\t\t\tif (idx >= 0 && idx < n)\n",
        "\t\t\t\tsum += data[idx] * mask[j];\n",
        "\t\t}\n",
        "\t\tresult[i] = sum;\n",
        "\t}\n",
        "}\n",
        "\n",
        "void checkResult(float *d_result, float *h_result, int n) {\n",
        "\tdouble epsilon = 1.0E-8;\n",
        "\n",
        "\tfor (int i = 0; i < n; i++)\n",
        "\t\tif (abs(h_result[i] - d_result[i]) > epsilon) {\n",
        "\t\t\tprintf(\"different on entry (%d) |h_result - d_result| >  %f\\n\", i,\n",
        "\t\t\t\t\tepsilon);\n",
        "\t\t\tbreak;\n",
        "\t\t}\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLxZjCx8bT3s"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "!nvcc -arch=sm_37  convolutionSMEM/conv1D.cu -o conv1D\n",
        "!./conv1D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 TODO"
      ],
      "metadata": {
        "id": "3jMZU51M8beJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwcTDn6ehJr_"
      },
      "source": [
        "%%writefile convolutionSMEM/conv2D.cu\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "#define DATA_WIDTH   (20*1024)\n",
        "#define DATA_HEIGHT  (20*1024)\n",
        "#define BLOCK_SIZE   8\n",
        "#define MASK_RADIUS  2\n",
        "#define MASK_SIZE    (2 * MASK_RADIUS + 1)\n",
        "#define TILE_WIDTH   (BLOCK_SIZE + MASK_SIZE - 1)\n",
        "#define DEBUG 0\n",
        "\n",
        "// constant mem\n",
        "__constant__ float M_dev[MASK_SIZE*MASK_SIZE];\n",
        "\n",
        "/*\n",
        " * kernel for convolution 2D (it holds only if MASK_RADIUS < BLOCK_SIZE)\n",
        " */\n",
        "__global__ void conv2D(float *A, float *B) {\n",
        "\t \n",
        "\t // TODO\n",
        "\t \n",
        "}\n",
        "\n",
        "/*\n",
        " * Average filter\n",
        " */\n",
        "void Avg_mask(float *mask) {\n",
        "\tint n = MASK_SIZE;\n",
        "\tfor (int i = 0; i < n*n; i++)\n",
        "\t\tmask[i] = (float) 1.0 / (n * n);\n",
        "}\n",
        "\n",
        "\n",
        "/*\n",
        " * main\n",
        " */\n",
        "int main(void) {\n",
        "\n",
        "  // check params\n",
        "  if (MASK_RADIUS >= BLOCK_SIZE) {\n",
        "    printf(\"ERROR: it holds only if MASK_RADIUS < BLOCK_SIZE!\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "\tint nW = DATA_WIDTH;\n",
        "  int nH = DATA_HEIGHT;\n",
        "\tint b = BLOCK_SIZE;\n",
        "\n",
        "\tfloat M[MASK_SIZE*MASK_SIZE]; // const size\n",
        "\tfloat *A, *B, *A_dev, *B_dev;\n",
        "\tint datasize = nW * nH * sizeof(float);\n",
        "  int masksize = MASK_SIZE*MASK_SIZE * sizeof(float);\n",
        "\n",
        "  printf(\"Data size: %.2f (MB)\\n\", (float)datasize/(1024.0*1024.0));\n",
        "\tprintf(\"Initializing data...\\n\");\n",
        "\tA = (float *) malloc(datasize);\n",
        "\tB = (float *) malloc(datasize);\n",
        "\n",
        "\t// initialize data\n",
        "\tfor (int i = 0; i < nH; i++)\n",
        "\t\tfor (int j = 0; j < nW; j++)\n",
        "\t\t\tA[i*nW+j] = rand()%10;\n",
        "\n",
        "  // initialize mask \n",
        "\tAvg_mask(M);\n",
        "\n",
        "#if DEBUG\n",
        "\t// print data\n",
        "\tprintf(\"Print matrix A...\\n\");\n",
        "\tfor (int i = 0; i < nH; i++) {\n",
        "    if (i%8 == 0 && i>0)\n",
        "      printf(\"\\n\");\n",
        "\n",
        "\t\tfor (int j = 0; j < nW; j++)\n",
        "      if (j%8 == 0 && j>0)\n",
        "\t\t\t  printf(\" %0.0f \", A[i*nW+j]);\n",
        "      else\n",
        "        printf(\"%0.0f \", A[i*nW+j]);\n",
        "\t\tprintf(\"\\n\");\n",
        "\t}\n",
        "\n",
        "\tprintf(\"Print matrix M ...\\n\");\n",
        "\tfor (int i = 0; i < MASK_SIZE; i++) {\n",
        "\t\tfor (int j = 0; j < MASK_SIZE; j++)\n",
        "\t\t\t  printf(\" %1.2f \", M[i * MASK_SIZE + j]);\n",
        "\t\tprintf(\"\\n\");\n",
        "\t}\n",
        "#endif\n",
        "\n",
        "\t// cuda allocation \n",
        "\tCHECK(cudaMemcpyToSymbol(M_dev, M, masksize));\n",
        "\tCHECK(cudaMalloc((void **) &A_dev, datasize));\n",
        "\tCHECK(cudaMalloc((void **) &B_dev, datasize));\n",
        "\tCHECK(cudaMemcpy(A_dev, A, datasize, cudaMemcpyHostToDevice));\n",
        "\t\n",
        "\t// block, grid dims, kernel\n",
        "\tdim3 block(b, b);\n",
        "\tdim3 grid((nW+b-1)/b, (nH+b-1)/b);\n",
        "  double iStart, iElaps;\n",
        "\tiStart = seconds();\n",
        "\tconv2D<<<grid, block>>>(A_dev, B_dev);\n",
        "  cudaDeviceSynchronize();\n",
        "  iElaps = seconds() - iStart;\n",
        "\tprintf(\"\\nconv2D<<<(%d,%d), (%d,%d)>>> elapsed time %f sec \\n\\n\", grid.x, grid.y, block.x, block.y, iElaps);\n",
        "\tCHECK(cudaGetLastError());\n",
        "\n",
        "\tCHECK(cudaMemcpy(B, B_dev, datasize, cudaMemcpyDeviceToHost));\n",
        "\n",
        "#if DEBUG\n",
        "\t// print out data\n",
        "\tprintf(\"Print results...\\n\");\n",
        "\tfor (int i = 0; i < nH; i++) {\n",
        "    if (i%8 == 0 && i>0)\n",
        "      printf(\"\\n\");\n",
        "\t\tfor (int j = 0; j < nW; j++)\n",
        "      if (j%8 == 0 && j>0)\n",
        "\t\t\t  printf(\" %0.2f \", B[i*nW+j]);\n",
        "      else\n",
        "        printf(\"%0.2f \", B[i*nW+j]);\n",
        "\t\tprintf(\"\\n\");\n",
        "\t}\n",
        "#endif\n",
        "\n",
        "\tcudaFree(A_dev);\n",
        "\tcudaFree(B_dev);\n",
        "  cudaDeviceReset();\n",
        "\tfree(A);\n",
        "\tfree(B);\n",
        "\treturn 0;\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiun00TE2wcE"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "!nvcc -arch=sm_37  convolutionSMEM/conv2D.cu -o conv2D\n",
        "!./conv2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ9BuU3ZW_zL"
      },
      "source": [
        "!nvprof conv2D"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}