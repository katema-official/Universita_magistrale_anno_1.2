{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUDA_lab7_TODO.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "WoJbB3T5Vkw-",
        "zs_a5Vuimily",
        "iUYP4kCJhEIx",
        "SbQlRthtJTQE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **LAB 7 - Global memory (GMEM)**\n",
        "---"
      ],
      "metadata": {
        "id": "fZYqN0UwVLC_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoJbB3T5Vkw-"
      },
      "source": [
        "# ▶️ CUDA setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fht2Wy8wVkxJ"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jP2H_YJVkxJ"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [GPU Compute Capability](https://developer.nvidia.com/cuda-gpus)"
      ],
      "metadata": {
        "id": "VKbaxH9wWosO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cGSqZovVkxK"
      },
      "source": [
        "## NVCC Plugin for Jupyter notebook\n",
        "\n",
        "*Usage*:\n",
        "\n",
        "\n",
        "*   Load Extension `%load_ext nvcc_plugin`\n",
        "*   Mark a cell to be treated as cuda cell\n",
        "`%%cuda --name example.cu --compile false`\n",
        "\n",
        "**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n",
        "\n",
        "*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCVhMkqYVkxK"
      },
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6PDOytTVkxK"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bash and data setup"
      ],
      "metadata": {
        "id": "cReFlD-VRfZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Bash setup\n",
        "%%writefile /root/.bashrc\n",
        "\n",
        "# If not running interactively, don't do anything\n",
        "[ -z \"$PS1\" ] && return\n",
        "\n",
        "# don't put duplicate lines in the history. See bash(1) for more options\n",
        "# ... or force ignoredups and ignorespace\n",
        "HISTCONTROL=ignoredups:ignorespace\n",
        "\n",
        "# append to the history file, don't overwrite it\n",
        "shopt -s histappend\n",
        "\n",
        "# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)\n",
        "HISTSIZE=10000\n",
        "HISTFILESIZE=20000\n",
        "\n",
        "# check the window size after each command and, if necessary,\n",
        "# update the values of LINES and COLUMNS.\n",
        "shopt -s checkwinsize\n",
        "\n",
        "# make less more friendly for non-text input files, see lesspipe(1)\n",
        "[ -x /usr/bin/lesspipe ] && eval \"$(SHELL=/bin/sh lesspipe)\"\n",
        "\n",
        "PS1='\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ '\n",
        "\n",
        "# enable color support of ls and also add handy aliases\n",
        "if [ -x /usr/bin/dircolors ]; then\n",
        "    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\n",
        "    alias ls='ls --color=auto'\n",
        "    #alias dir='dir --color=auto'\n",
        "    #alias vdir='vdir --color=auto'\n",
        "\n",
        "    alias grep='grep --color=auto'\n",
        "    alias fgrep='fgrep --color=auto'\n",
        "    alias egrep='egrep --color=auto'\n",
        "fi\n",
        "\n",
        "# some more ls aliases\n",
        "alias ll='ls -lF'\n",
        "alias la='ls -A'\n",
        "alias l='ls -CF'\n",
        "\n",
        "# path setup\n",
        "export PATH=\"./:/usr/local/cuda/bin:$PATH\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "O8ICSyy8_GEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source /root/.bashrc"
      ],
      "metadata": {
        "id": "QxIfKO3Ghf7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GPUcomputing site on github..."
      ],
      "metadata": {
        "id": "IYG8Cv4bTzyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/giulianogrossi/GPUcomputing.git"
      ],
      "metadata": {
        "id": "E7jZmHjCT0vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some paths..."
      ],
      "metadata": {
        "id": "ZarLje6wR_Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path setup\n",
        "!mkdir -p /content/GPUcomputing/lab7\n",
        "%cd /content/GPUcomputing/lab7\n",
        "!mkdir -p mems\n",
        "!mkdir -p unified\n",
        "!mkdir -p transpose\n",
        "!mkdir -p struct"
      ],
      "metadata": {
        "id": "tC-AaOJlkLOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ▶️ VS Code on Colab"
      ],
      "metadata": {
        "id": "zs_a5Vuimily"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab-ssh tunnel\n",
        "#@markdown Execute this cell to open the ssh tunnel. Check [colab-ssh documentation](https://github.com/WassimBenzarti/colab-ssh) for more details.\n",
        "\n",
        "# Install colab_ssh on google colab\n",
        "!pip install colab_ssh --upgrade\n",
        "\n",
        "from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n",
        "ssh_tunnel_password = \"gpu\" #@param {type: \"string\"}\n",
        "launch_ssh_cloudflared(password=ssh_tunnel_password)\n",
        "\n",
        "# Optional: if you want to clone a Github or Gitlab repository\n",
        "repository_url=\"https://github.com/giulianogrossi/GPUcomputing\" #@param {type: \"string\"}\n",
        "init_git_cloudflared(repository_url)"
      ],
      "metadata": {
        "id": "BCf9JxqphHAp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUYP4kCJhEIx"
      },
      "source": [
        "# ▶️ DeviceQuery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW9b_Yuxi7id"
      },
      "source": [
        "# DeviceQuery dell'attuale device (su Colab!)\n",
        "!nvcc /content/GPUcomputing/utils/deviceQuery.cu -o deviceQuery\n",
        "!./deviceQuery"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Static, pinned, zero-copy  memory"
      ],
      "metadata": {
        "id": "DvmApCF76YD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pinned memory**\n",
        "\n",
        "An example of using CUDA's memory copy API to transfer data to and from the device. In this case, `cudaMalloc` is used to allocate memory on the GPU and `cudaMemcpy` is used to transfer the contents of host memory to an array allocated using `cudaMalloc`. Host memory is allocated using `cudaMallocHost` to create a page-locked host array."
      ],
      "metadata": {
        "id": "4ApFq6kF6Olh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mems/pinMemTransfer.cu\n",
        "\n",
        "#include \"../../utils/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "  // set up device\n",
        "  int dev = 0;\n",
        "  CHECK(cudaSetDevice(dev));\n",
        "\n",
        "  // memory size\n",
        "  unsigned int isize = 1 << 24;\n",
        "  unsigned int nbytes = isize * sizeof(float);\n",
        "\n",
        "  // get device information\n",
        "  cudaDeviceProp deviceProp;\n",
        "  CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "\n",
        "  if (!deviceProp.canMapHostMemory) {\n",
        "      printf(\"Device %d does not support mapping CPU host memory!\\n\", dev);\n",
        "      CHECK(cudaDeviceReset());\n",
        "      exit(EXIT_SUCCESS);\n",
        "  }\n",
        "\n",
        "  printf(\"%s starting at \", argv[0]);\n",
        "  printf(\"device %d: %s memory size %d nbyte %5.2fMB canMap %d\\n\", dev,\n",
        "          deviceProp.name, isize, nbytes / (1024.0f * 1024.0f),\n",
        "          deviceProp.canMapHostMemory);\n",
        "\n",
        "  float *h_a;\n",
        "  // allocate the host memory   \n",
        "  //h_a = (float *)malloc(nbytes);\n",
        "\n",
        "  // allocate pinned host memory\n",
        "  CHECK(cudaMallocHost ((float **)&h_a, nbytes));\n",
        "\n",
        "  // allocate device memory\n",
        "  float *d_a;\n",
        "  CHECK(cudaMalloc((float **)&d_a, nbytes));\n",
        "\n",
        "  for (int i = 0; i < isize; i++) \n",
        "    h_a[i] = 100.10f;\n",
        "\n",
        "  // transfer data from the host to the device\n",
        "  CHECK(cudaMemcpy(d_a, h_a, nbytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "  // transfer data from the device to the host\n",
        "  CHECK(cudaMemcpy(h_a, d_a, nbytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // free memory\n",
        "  CHECK(cudaFree(d_a));\n",
        "  CHECK(cudaFreeHost(h_a));\n",
        "\n",
        "  // reset device\n",
        "  CHECK(cudaDeviceReset());\n",
        "  return EXIT_SUCCESS;\n",
        "}\n"
      ],
      "metadata": {
        "id": "XObjkBa16Wx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -arch=sm_37 mems/pinMemTransfer.cu -o pinMemTransfer\n",
        "!nvprof ./pinMemTransfer\n"
      ],
      "metadata": {
        "id": "arU-6PkD7V4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXUIQkZLCTcG"
      },
      "source": [
        "# ✅ Unified memory\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile unified/sumMatrix.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "void initialData(float *ip, const int size) {\n",
        "  int i;\n",
        "\n",
        "  for (i = 0; i < size; i++)\n",
        "    ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n",
        "  return;\n",
        "}\n",
        "\n",
        "void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny) {\n",
        "  float *ia = A;\n",
        "  float *ib = B;\n",
        "  float *ic = C;\n",
        "\n",
        "  for (int iy = 0; iy < ny; iy++) {\n",
        "    for (int ix = 0; ix < nx; ix++)\n",
        "      ic[ix] = ia[ix] + ib[ix];\n",
        "\n",
        "    ia += nx;\n",
        "    ib += nx;\n",
        "    ic += nx;\n",
        "  }\n",
        "  return;\n",
        "}\n",
        "\n",
        "void checkResult(float *hostRef, float *gpuRef, const int N) {\n",
        "  double epsilon = 1.0E-8;\n",
        "  bool match = 1;\n",
        "\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n",
        "      match = 0;\n",
        "      printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n",
        "      break;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (!match)\n",
        "    printf(\"Arrays do not match.\\n\\n\");\n",
        "}\n",
        "\n",
        "// grid 2D block 2D\n",
        "__global__ void sumMatrixGPU(float *MatA, float *MatB, float *MatC, int nx, int ny) {\n",
        "  unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "  unsigned int idx = iy * nx + ix;\n",
        "\n",
        "  if (ix < nx && iy < ny)\n",
        "    MatC[idx] = MatA[idx] + MatB[idx];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "    printf(\"%s Starting \", argv[0]);\n",
        "\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"using Device %d: %s\\n\", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // set up data size of matrix\n",
        "    int nx, ny;\n",
        "    int ishift = 12;\n",
        "\n",
        "    if  (argc > 1) ishift = atoi(argv[1]);\n",
        "\n",
        "    nx = ny = 1 << ishift;\n",
        "\n",
        "    int nxy = nx * ny;\n",
        "    int nBytes = nxy * sizeof(float);\n",
        "    printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n",
        "\n",
        "    // malloc host memory\n",
        "    float *h_A, *h_B, *hostRef, *gpuRef;\n",
        "    h_A = (float *)malloc(nBytes);\n",
        "    h_B = (float *)malloc(nBytes);\n",
        "    hostRef = (float *)malloc(nBytes);\n",
        "    gpuRef = (float *)malloc(nBytes);\n",
        "\n",
        "    // initialize data at host side\n",
        "    double iStart = seconds();\n",
        "    initialData(h_A, nxy);\n",
        "    initialData(h_B, nxy);\n",
        "    double iElaps = seconds() - iStart;\n",
        "\n",
        "    printf(\"initialization: \\t %f sec\\n\", iElaps);\n",
        "\n",
        "    memset(hostRef, 0, nBytes);\n",
        "    memset(gpuRef, 0, nBytes);\n",
        "\n",
        "    // add matrix at host side for result checks\n",
        "    iStart = seconds();\n",
        "    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"sumMatrix on host:\\t %f sec\\n\", iElaps);\n",
        "\n",
        "    // malloc device global memory\n",
        "    float *d_MatA, *d_MatB, *d_MatC;\n",
        "    CHECK(cudaMalloc((void **)&d_MatA, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatB, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatC, nBytes));\n",
        "\n",
        "    // invoke kernel at host side\n",
        "    int dimx = 32;\n",
        "    int dimy = 32;\n",
        "    dim3 block(dimx, dimy);\n",
        "    dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n",
        "\n",
        "    // init device data to 0.0f, then warm-up kernel to obtain accurate timing\n",
        "    // result\n",
        "    CHECK(cudaMemset(d_MatA, 0.0f, nBytes));\n",
        "    CHECK(cudaMemset(d_MatB, 0.0f, nBytes));\n",
        "    sumMatrixGPU<<<grid, block>>>(d_MatA, d_MatB, d_MatC, 1, 1);\n",
        "\n",
        "\n",
        "    // transfer data from host to device\n",
        "    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    iStart =  seconds();\n",
        "    sumMatrixGPU<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);\n",
        "\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"sumMatrix on gpu :\\t %f sec <<<(%d,%d), (%d,%d)>>> \\n\", iElaps,\n",
        "            grid.x, grid.y, block.x, block.y);\n",
        "\n",
        "    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // check kernel error\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // check device results\n",
        "    checkResult(hostRef, gpuRef, nxy);\n",
        "\n",
        "    // free device global memory\n",
        "    CHECK(cudaFree(d_MatA));\n",
        "    CHECK(cudaFree(d_MatB));\n",
        "    CHECK(cudaFree(d_MatC));\n",
        "\n",
        "    // free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(hostRef);\n",
        "    free(gpuRef);\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    return (0);\n",
        "}\n"
      ],
      "metadata": {
        "id": "Stv28L2PU-Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilazione ed esecuzione\n",
        "\n",
        "!nvcc -arch=sm_37 unified/sumMatrix.cu  -o sumMatrix\n",
        "!./sumMatrix 14"
      ],
      "metadata": {
        "id": "LezrfqF5VPEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# profilazione\n",
        "\n",
        "!nvprof ./sumMatrix 14"
      ],
      "metadata": {
        "id": "iRqExSDZViHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 TODO"
      ],
      "metadata": {
        "id": "R1V3b-a3RjL3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y52R0d3CA50"
      },
      "source": [
        "%%writefile unified/sumMatrixUni.cu\n",
        "\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "void initialData(float *ip, const int size) {\n",
        "  int i;\n",
        "  for (i = 0; i < size; i++){\n",
        "    ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n",
        "  }\n",
        "  return;\n",
        "}\n",
        "\n",
        "void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny) {\n",
        "  float *ia = A;\n",
        "  float *ib = B;\n",
        "  float *ic = C;\n",
        "\n",
        "  for (int iy = 0; iy < ny; iy++) {\n",
        "    for (int ix = 0; ix < nx; ix++)\n",
        "      ic[ix] = ia[ix] + ib[ix];\n",
        "\n",
        "    ia += nx;\n",
        "    ib += nx;\n",
        "    ic += nx;\n",
        "  }\n",
        "  return;\n",
        "}\n",
        "\n",
        "void checkResult(float *hostRef, float *gpuRef, const int N) {\n",
        "  double epsilon = 1.0E-8;\n",
        "  bool match = 1;\n",
        "\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n",
        "      match = 0;\n",
        "      printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n",
        "      break;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (!match)\n",
        "    printf(\"Arrays do not match.\\n\\n\");\n",
        "}\n",
        "\n",
        "// grid 2D block 2D\n",
        "__global__ void sumMatrixGPU(float *MatA, float *MatB, float *MatC, int nx, int ny) {\n",
        "    \n",
        "    unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    unsigned int idx = iy * nx + ix;\n",
        "\n",
        "    if (ix < nx && iy < ny)\n",
        "      MatC[idx] = MatA[idx] + MatB[idx];\n",
        "  \n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "  printf(\"%s Starting \", argv[0]);\n",
        "\n",
        "  // set up device\n",
        "  int dev = 0;\n",
        "  cudaDeviceProp deviceProp;\n",
        "  CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "  printf(\"using Device %d: %s\\n\", dev, deviceProp.name);\n",
        "  CHECK(cudaSetDevice(dev));\n",
        "\n",
        "  // set up data size of matrix\n",
        "  int nx, ny;\n",
        "  int ishift = 14;\n",
        "\n",
        "  if  (argc > 1) ishift = atoi(argv[1]);\n",
        "\n",
        "  nx = ny = 1 << ishift;\n",
        "\n",
        "  int nxy = nx * ny;\n",
        "  int nBytes = nxy * sizeof(float);\n",
        "  printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n",
        "\n",
        "  // malloc host memory\n",
        "  float* h_A, *h_B, *hostRef, *gpuRef;\n",
        "  cudaMallocManaged(&h_A, nBytes);\n",
        "  cudaMallocManaged(&h_B, nBytes);\n",
        "  cudaMallocManaged(&hostRef, nBytes);\n",
        "  cudaMallocManaged(&gpuRef, nBytes);\n",
        "\n",
        "\n",
        "\n",
        "  // initialize data at host side\n",
        "  initialData(h_A, nxy);\n",
        "  initialData(h_B, nxy);\n",
        "\n",
        "  memset(hostRef, 0, nBytes);\n",
        "  memset(gpuRef, 0, nBytes);\n",
        "\n",
        "  // add matrix at host side for result checks\n",
        "  double iStart = seconds();\n",
        "  sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);\n",
        "  double iElaps = seconds() - iStart;\n",
        "  printf(\"sumMatrix on host:\\t %f sec\\n\", iElaps);\n",
        "\n",
        "  // invoke kernel at host side\n",
        "  int dimx = 32;\n",
        "  int dimy = 32;\n",
        "  dim3 block(dimx, dimy);\n",
        "  dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n",
        "\n",
        "  // warm-up kernel, with unified memory all pages will migrate from host to device\n",
        "  //(Si arrabbia se lo faccio) CHECK(cudaMemset(h_A, 0.0f, nBytes));\n",
        "  //NO CHECK(cudaMemset(h_B, 0.0f, nBytes));\n",
        "  //memset(h_A, 0.0f, nBytes);\n",
        "  //memset(h_B, 0.0f, nBytes);\n",
        "  \n",
        "  sumMatrixGPU<<<grid, block>>>(h_A, h_B, gpuRef, 1, 1);\n",
        "\n",
        "  // initialize data at host side AGAIN (prova mia (si arrabbia se lo faccio))\n",
        "  //initialData(h_A, nxy);\n",
        "  //initialData(h_B, nxy);\n",
        "\n",
        "  // after warm-up, time with unified memory\n",
        "  iStart = seconds();\n",
        "\n",
        "  sumMatrixGPU<<<grid, block>>>(h_A, h_B, gpuRef, nx, ny);\n",
        "\n",
        "  CHECK(cudaDeviceSynchronize());\n",
        "  iElaps = seconds() - iStart;\n",
        "  printf(\"sumMatrix on gpu :\\t %f sec <<<(%d,%d), (%d,%d)>>> \\n\", iElaps,\n",
        "          grid.x, grid.y, block.x, block.y);\n",
        "\n",
        "  // check kernel error\n",
        "  CHECK(cudaGetLastError());\n",
        "\n",
        "  // check device results\n",
        "  checkResult(hostRef, gpuRef, nxy);\n",
        "\n",
        "  // free device global memory\n",
        "  CHECK(cudaFree(h_A));\n",
        "  CHECK(cudaFree(h_B));\n",
        "  CHECK(cudaFree(hostRef));\n",
        "  CHECK(cudaFree(gpuRef));\n",
        "\n",
        "  // reset device\n",
        "  CHECK(cudaDeviceReset());\n",
        "\n",
        "  return (0);\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PSc9B9PDTWt"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "\n",
        "!nvcc -arch=sm_37 unified/sumMatrixUni.cu  -o sumMatrixUni\n",
        "!./sumMatrixUni 14"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfR471rze2p-"
      },
      "source": [
        "# profilazione\n",
        "\n",
        "!nvprof ./sumMatrixUni"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mykWw4i6-PTq"
      },
      "source": [
        "%%writefile unified/sumMatrixGPUManual.cu\n",
        "\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "void initialData(float *ip, const int size) {\n",
        "  int i;\n",
        "\n",
        "  for (i = 0; i < size; i++)\n",
        "    ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n",
        "  return;\n",
        "}\n",
        "\n",
        "void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny) {\n",
        "  float *ia = A;\n",
        "  float *ib = B;\n",
        "  float *ic = C;\n",
        "\n",
        "  for (int iy = 0; iy < ny; iy++) {\n",
        "    for (int ix = 0; ix < nx; ix++)\n",
        "      ic[ix] = ia[ix] + ib[ix];\n",
        "\n",
        "    ia += nx;\n",
        "    ib += nx;\n",
        "    ic += nx;\n",
        "  }\n",
        "  return;\n",
        "}\n",
        "\n",
        "void checkResult(float *hostRef, float *gpuRef, const int N) {\n",
        "  double epsilon = 1.0E-8;\n",
        "  bool match = 1;\n",
        "\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n",
        "      match = 0;\n",
        "      printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n",
        "      break;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (!match)\n",
        "    printf(\"Arrays do not match.\\n\\n\");\n",
        "}\n",
        "\n",
        "// grid 2D block 2D\n",
        "__global__ void sumMatrixGPU(float *MatA, float *MatB, float *MatC, int nx, int ny) {\n",
        "  unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "  unsigned int idx = iy * nx + ix;\n",
        "\n",
        "  if (ix < nx && iy < ny)\n",
        "    MatC[idx] = MatA[idx] + MatB[idx];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "    printf(\"%s Starting \", argv[0]);\n",
        "\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"using Device %d: %s\\n\", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // set up data size of matrix\n",
        "    int nx, ny;\n",
        "    int ishift = 12;\n",
        "\n",
        "    if  (argc > 1) ishift = atoi(argv[1]);\n",
        "\n",
        "    nx = ny = 1 << ishift;\n",
        "\n",
        "    int nxy = nx * ny;\n",
        "    int nBytes = nxy * sizeof(float);\n",
        "    printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n",
        "\n",
        "    // malloc host memory\n",
        "    float *h_A, *h_B, *hostRef, *gpuRef;\n",
        "    h_A = (float *)malloc(nBytes);\n",
        "    h_B = (float *)malloc(nBytes);\n",
        "    hostRef = (float *)malloc(nBytes);\n",
        "    gpuRef = (float *)malloc(nBytes);\n",
        "\n",
        "    // initialize data at host side\n",
        "    double iStart = seconds();\n",
        "    initialData(h_A, nxy);\n",
        "    initialData(h_B, nxy);\n",
        "    double iElaps = seconds() - iStart;\n",
        "\n",
        "    printf(\"initialization: \\t %f sec\\n\", iElaps);\n",
        "\n",
        "    memset(hostRef, 0, nBytes);\n",
        "    memset(gpuRef, 0, nBytes);\n",
        "\n",
        "    // add matrix at host side for result checks\n",
        "    iStart = seconds();\n",
        "    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"sumMatrix on host:\\t %f sec\\n\", iElaps);\n",
        "\n",
        "    // malloc device global memory\n",
        "    float *d_MatA, *d_MatB, *d_MatC;\n",
        "    CHECK(cudaMalloc((void **)&d_MatA, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatB, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatC, nBytes));\n",
        "\n",
        "    // invoke kernel at host side\n",
        "    int dimx = 32;\n",
        "    int dimy = 32;\n",
        "    dim3 block(dimx, dimy);\n",
        "    dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n",
        "\n",
        "    // init device data to 0.0f, then warm-up kernel to obtain accurate timing\n",
        "    // result\n",
        "    CHECK(cudaMemset(d_MatA, 0.0f, nBytes));\n",
        "    CHECK(cudaMemset(d_MatB, 0.0f, nBytes));\n",
        "    sumMatrixGPU<<<grid, block>>>(d_MatA, d_MatB, d_MatC, 1, 1);\n",
        "\n",
        "\n",
        "    // transfer data from host to device\n",
        "    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    iStart =  seconds();\n",
        "    sumMatrixGPU<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);\n",
        "\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"sumMatrix on gpu :\\t %f sec <<<(%d,%d), (%d,%d)>>> \\n\", iElaps,\n",
        "            grid.x, grid.y, block.x, block.y);\n",
        "\n",
        "    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // check kernel error\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // check device results\n",
        "    checkResult(hostRef, gpuRef, nxy);\n",
        "\n",
        "    // free device global memory\n",
        "    CHECK(cudaFree(d_MatA));\n",
        "    CHECK(cudaFree(d_MatB));\n",
        "    CHECK(cudaFree(d_MatC));\n",
        "\n",
        "    // free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(hostRef);\n",
        "    free(gpuRef);\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    return (0);\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJcJtD6f_IXK"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "\n",
        "!nvcc -arch=sm_60 unified/sumMatrixGPUManual.cu  -o sumMatrixGPUManual\n",
        "!./sumMatrixGPUManual 14"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5JMDp3t_kEI"
      },
      "source": [
        "# profilazione (senza unified memory - dà errore)\n",
        "\n",
        "!nvprof --unified-memory-profiling off ./sumMatrixGPUManual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOFMQZAkjlLW"
      },
      "source": [
        "# ✅ SoA vs AoS structs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9nRkLgeB10A"
      },
      "source": [
        "%%writefile struct/SoA.cu\n",
        "\n",
        "#include <stdint.h>\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "#define N 1<<24\n",
        "#define blocksize 1<<7\n",
        "\n",
        "struct SoA {\n",
        "\tuint8_t r[N];\n",
        "\tuint8_t g[N];\n",
        "\tuint8_t b[N];\n",
        "};\n",
        "\n",
        "\n",
        "void initialize(SoA*, int);\n",
        "void checkResult(SoA*, SoA*, int);\n",
        "\n",
        "/*\n",
        " * Riscala l'immagine al valore massimo [max] fissato\n",
        " */\n",
        "__global__ void rescaleImg(SoA *img, const int max, const int n) {\n",
        "\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tif (i < n) {\n",
        "\t\tfloat r,g,b;\n",
        "\t\tSoA *tmp = img;\n",
        "\t\tr = max * (float)tmp->r[i]/255.0f;\n",
        "\t\timg->r[i] = (uint8_t)r;\n",
        "\t\tg = max * (float)tmp->g[i]/255.0f;\n",
        "\t\timg->g[i] = (uint8_t)g;\n",
        "\t\tb = max * (float)tmp->b[i]/255.0f;\n",
        "\t\timg->b[i] = (uint8_t)b;\n",
        "\t}\n",
        "}\n",
        "\n",
        "/*\n",
        " * cancella un piano dell'immagine [plane = 'r' o 'g' o 'b'] fissato\n",
        " */\n",
        "__global__ void deletePlane(SoA *img, const char plane, const int n) {\n",
        "\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tif (i < n) {\n",
        "\t\tswitch (plane) {\n",
        "\t\tcase 'r':\n",
        "\t\t\timg->r[i] = 0;\n",
        "\t\t\tbreak;\n",
        "\t\tcase 'g':\n",
        "\t\t\timg->g[i] = 0;\n",
        "\t\t\tbreak;\n",
        "\t\tcase 'b':\n",
        "\t\t\timg->b[i] = 0;\n",
        "\t\t\tbreak;\n",
        "\t\t}\n",
        "\t}\n",
        "}\n",
        "\n",
        "/*\n",
        " * setup device\n",
        " */\n",
        "__global__ void warmup(SoA *img, const int max, const int n) {\n",
        "\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tif (i < n) {\n",
        "\t\tfloat r,g,b;\n",
        "\t\tSoA *tmp = img;\n",
        "\t\tr = max * (float)tmp->r[i]/255.0f;\n",
        "\t\timg->r[i] = (uint8_t)r;\n",
        "\t\tg = max * (float)tmp->g[i]/255.0f;\n",
        "\t\timg->g[i] = (uint8_t)g;\n",
        "\t\tb = max * (float)tmp->b[i]/255.0f;\n",
        "\t\timg->b[i] = (uint8_t)b;\n",
        "\t}\n",
        "}\n",
        "\n",
        "/*\n",
        " * Legge da stdin quale kernel eseguire: 0 per rescaleImg, 1 per deletePlane\n",
        " */\n",
        "int main(int argc, char **argv) {\n",
        "\t// set up device\n",
        "\tint dev = 0;\n",
        "\tcudaDeviceProp deviceProp;\n",
        "\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "\tprintf(\"%s test SoA at \", argv[0]);\n",
        "\tprintf(\"device %d: %s \\n\", dev, deviceProp.name);\n",
        "\tCHECK(cudaSetDevice(dev));\n",
        "\n",
        "\t// scelta del kernel da eseguire\n",
        "\tint kernel = 0;\n",
        "\tif (argc > 1) kernel = atoi(argv[1]);\n",
        "\n",
        "\t// allocate host memory\n",
        "\tsize_t nBytes = sizeof(SoA);\n",
        "\tSoA *img = (SoA *)malloc(nBytes);\n",
        "\tSoA *new_img = (SoA *)malloc(nBytes);\n",
        "\n",
        "\t// initialize host array\n",
        "\tinitialize(img, N);\n",
        "\n",
        "\t// allocate device memory\n",
        "\tint n_elem = N;\n",
        "\tSoA *d_img;\n",
        "\tCHECK(cudaMalloc((void**)&d_img, nBytes));\n",
        "\n",
        "\t// copy data from host to device\n",
        "\tCHECK(cudaMemcpy(d_img, img, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "\t// definizione max\n",
        "\tint max = 128;\n",
        "\tif (argc > 2) max = atoi(argv[2]);\n",
        "\n",
        "\t// configurazione per esecuzione\n",
        "\tdim3 block (blocksize, 1);\n",
        "\tdim3 grid  ((n_elem + block.x - 1) / block.x, 1);\n",
        "\n",
        "\t// kernel 1: warmup\n",
        "\tdouble iStart = seconds();\n",
        "\twarmup<<<1, 32>>>(d_img, max, 32);\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tdouble iElaps = seconds() - iStart;\n",
        "\tprintf(\"warmup<<< 1, 32 >>> elapsed %f sec\\n\",iElaps);\n",
        "\tCHECK(cudaGetLastError());\n",
        "\n",
        "\t// kernel 2 rescaleImg o deletePlane\n",
        "\tiStart = seconds();\n",
        "\tif (kernel == 0)\n",
        "\t\trescaleImg<<<grid, block>>>(d_img, max, n_elem);\n",
        "\telse\n",
        "\t\tdeletePlane<<<grid, block>>>(d_img, 'r', n_elem);\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tiElaps = seconds() - iStart;\n",
        "\tprintf(\"rescaleImg <<< %3d, %3d >>> elapsed %f sec\\n\", grid.x, block.x, iElaps);\n",
        "\tCHECK(cudaMemcpy(new_img, d_img, nBytes, cudaMemcpyDeviceToHost));\n",
        "\tCHECK(cudaGetLastError());\n",
        "\n",
        "\t//checkResult(img, new_img, n_elem);\n",
        "\n",
        "\t// free memories both host and device\n",
        "\tCHECK(cudaFree(d_img));\n",
        "\tfree(img);\n",
        "\tfree(new_img);\n",
        "\n",
        "\t// reset device\n",
        "\tCHECK(cudaDeviceReset());\n",
        "\treturn EXIT_SUCCESS;\n",
        "}\n",
        "\n",
        "void initialize(SoA *img,  int size) {\n",
        "\tfor (int i = 0; i < size; i++) {\n",
        "\t\timg->r[i] = rand() % 256;\n",
        "\t\timg->g[i] = rand() % 256;\n",
        "\t\timg->b[i] = rand() % 256;\n",
        "\t}\n",
        "\treturn;\n",
        "}\n",
        "\n",
        "void checkResult(SoA *img, SoA *new_img, int n_elem) {\n",
        "\tfor (int i = 0; i < n_elem; i+=1000)\n",
        "\t\tprintf(\"img[%d] = (%d,%d,%d) -- new_img[%d] = (%d,%d,%d)\\n\",\n",
        "\t\t\t\ti,img->r[i],img->g[i],img->b[i],i,new_img->r[i],new_img->g[i],new_img->b[i]);\n",
        "\treturn;\n",
        "}\n",
        "\n",
        "\n",
        "void transposeHost(float *out, float *in, const int nx, const int ny) {\n",
        "\tfor (int iy = 0; iy < ny; ++iy) {\n",
        "\t\tfor (int ix = 0; ix < nx; ++ix) {\n",
        "\t\t\tout[ix * ny + iy] = in[iy * nx + ix];\n",
        "\t\t}\n",
        "\t}\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLxZjCx8bT3s"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "!nvcc -arch=sm_37  struct/SoA.cu -o SoA\n",
        "!./SoA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 TODO"
      ],
      "metadata": {
        "id": "cqQnULvlE2fu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwcTDn6ehJr_"
      },
      "source": [
        "%%writefile struct/AoS.cu\n",
        "\n",
        "#include <stdint.h>\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "#define N 1<<24\n",
        "#define blocksize 128\n",
        "\n",
        "struct AoS {\n",
        "\tuint8_t r;\n",
        "\tuint8_t g;\n",
        "\tuint8_t b;\n",
        "};\n",
        "\n",
        "void initialize(AoS *, int);\n",
        "void checkResult(AoS *, AoS *, int);\n",
        "\n",
        "/*\n",
        " * Riscala l'immagine al valore massimo [max] fissato\n",
        " */\n",
        "__global__ void rescaleImg(AoS* d_img, const int max, const int n_elem) {\n",
        "\tint idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "\tif(idx < n_elem){\n",
        "\t\t\tAoS tmp;\n",
        "\t\t\ttmp.r = max * d_img[idx].r / 255.0f;\n",
        "\t\t\ttmp.g = max * d_img[idx].g / 255.0f;\n",
        "\t\t\ttmp.b = max * d_img[idx].b / 255.0f;\n",
        "\t\t\td_img[idx] = tmp;\n",
        "\t}\t\n",
        "\n",
        "}\n",
        "\n",
        "/*\n",
        " * cancella un piano dell'immagine [plane = 'r' o 'g' o 'b'] fissato\n",
        " */\n",
        "__global__ void deletePlane() {\n",
        "\t\n",
        "\t//TODO\n",
        "\n",
        "}\n",
        "\n",
        "/*\n",
        " * setup device\n",
        " */\n",
        "__global__ void warmup(AoS *img, const int max, const int n) {\n",
        "\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tif (i < n) {\n",
        "\t\tAoS tmp;\n",
        "\t\ttmp.r = max * img[i].r / 255.0f;\n",
        "\t\ttmp.g = max * img[i].g / 255.0f;\n",
        "\t\ttmp.b = max * img[i].b / 255.0f;\n",
        "\t\timg[i] = tmp;\n",
        "\t}\n",
        "}\n",
        "\n",
        "/*\n",
        " * Legge da stdin quale kernel eseguire: 0 per rescaleImg, 1 per deletePlane\n",
        " */\n",
        "int main(int argc, char **argv) {\n",
        "\t// set up device\n",
        "\tint dev = 0;\n",
        "\tcudaDeviceProp deviceProp;\n",
        "\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "\tprintf(\"%s test AoS at \", argv[0]);\n",
        "\tprintf(\"device %d: %s \\n\", dev, deviceProp.name);\n",
        "\tCHECK(cudaSetDevice(dev));\n",
        "\n",
        "\t// scelta del kernel da eseguire\n",
        "\tint kernel = 0;\n",
        "\tif (argc > 1) kernel = atoi(argv[1]);\n",
        "\n",
        "\t// allocate host memory\n",
        "\tint n_elem = N;\n",
        "\tint nBytes = n_elem*sizeof(AoS);\n",
        "\tAoS *img = (AoS*)malloc(nBytes);\n",
        "\tAoS *new_img = (AoS*)malloc(nBytes);\n",
        "\t\n",
        "\n",
        "\t// initialize host array\n",
        "\tinitialize(img, N);\n",
        "\n",
        "\t// allocate device memory\n",
        "\tAoS* d_img;\n",
        "\tCHECK(cudaMalloc((AoS**) &d_img, nBytes));\n",
        "\n",
        "\n",
        "\t// copy data from host to device\n",
        "\tCHECK(cudaMemcpy(d_img, img, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "\t// definizione max\n",
        "\tint max = 128;\n",
        "\tif (argc > 2) max = atoi(argv[2]);\n",
        "\n",
        "\t// configurazione per esecuzione\n",
        "\tdim3 block(blocksize);\n",
        "\tdim3 grid((n_elem + block.x - 1)/block.x);\n",
        "\n",
        "\t// kernel 1: warmup\n",
        "\tdouble iStart = seconds();\n",
        "\tprintf(\"ciao 0\");\n",
        "\twarmup<<<1, 32>>>(d_img, max, 32);\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tprintf(\"ciao 1\");\n",
        "\tdouble iElaps = seconds() - iStart;\n",
        "\tprintf(\"warmup<<< 1, 32 >>> elapsed %f sec\\n\",iElaps);\n",
        "\tCHECK(cudaGetLastError());\n",
        "\t// kernel 2 rescaleImg o deletePlane\n",
        "\tiStart = seconds();\n",
        "\tif (kernel == 0)\n",
        "\t\trescaleImg<<<grid, block>>>(d_img, max, n_elem);\n",
        "\telse\n",
        "\t\tint k = -1;\n",
        "\t\t//deletePlane<<<grid, block>>>(d_img, 'r', n_elem);\n",
        "\tCHECK(cudaDeviceSynchronize());\n",
        "\tiElaps = seconds() - iStart;\n",
        "\tprintf(\"rescaleImg <<< %3d, %3d >>> elapsed %f sec\\n\", grid.x, block.x, iElaps);\n",
        "\tCHECK(cudaMemcpy(new_img, d_img, nBytes, cudaMemcpyDeviceToHost));\n",
        "\tCHECK(cudaGetLastError());\n",
        "\t//checkResult(img, new_img, n_elem);\n",
        "\n",
        "\t// free memories both host and device\n",
        "\tCHECK(cudaFree(d_img));\n",
        "\tfree(img);\n",
        "\tfree(new_img);\n",
        "\n",
        "\t// reset device\n",
        "\tCHECK(cudaDeviceReset());\n",
        "\treturn EXIT_SUCCESS;\n",
        "}\n",
        "\n",
        "void initialize(AoS *img,  int size) {\n",
        "\tfor (int i = 0; i < size; i++) {\n",
        "\t\timg[i].r = rand() % 256;\n",
        "\t\timg[i].g = rand() % 256;\n",
        "\t\timg[i].b = rand() % 256;\n",
        "\t}\n",
        "\treturn;\n",
        "}\n",
        "\n",
        "void checkResult(AoS *img, AoS *new_img, int n_elem) {\n",
        "\tfor (int i = 0; i < n_elem; i+=1000)\n",
        "\t\tprintf(\"img[%d] = (%d,%d,%d) -- new_img[%d] = (%d,%d,%d)\\n\",\n",
        "\t\t\t\ti,img[i].r,img[i].g,img[i].b,i,new_img[i].r,new_img[i].g,new_img[i].b);\n",
        "\treturn;\n",
        "}\n",
        "\n",
        "\n",
        "//ATTN\n",
        "//L'AoS, in questo caso, mi restituisce risultati migliori della SoA, come invece\n",
        "//ci si poteva aspettare. La mia ipotesi del perché è questa:\n",
        "//Supponendo che la GPU su cui ho eseguito il codice (una Tesla mi sembra) abiliti\n",
        "//di default la cache L1, abbiamo la seguente situazione.\n",
        "//1) Nella Struct of Array, i 32 thread dello stesso warp prima leggono 32 byte\n",
        "//(ricordiamoci che il tipo dei dati rgb qui non è un intero, ma un byte) per la r,\n",
        "//poi 32 per la g e infine altri 32 per la b. Leggono questi dati e li portano\n",
        "//in cache L1, ma di fatto farlo è inutile, perché vengono letti e poi dimenticati.\n",
        "//2) Nella Array of Struct, invece, come prima cosa ogni thread legge 96 byte:\n",
        "//32 per r, 32 per g e 32 per b (dato che stanno tutti nella stessa struct), e li\n",
        "//portano nella L1. A quel punto, quando vanno effettivamente a leggere i 32 byte\n",
        "//della g e i 32 della b, li leggono dalla L1, che è molto più vicina della global.\n",
        "//Lo speedup è dato da questo, secondo me, ovvero dall'utilizzo della L1.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiun00TE2wcE"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "!nvcc -arch=sm_37  struct/AoS.cu -o AoS\n",
        "!./AoS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbQlRthtJTQE"
      },
      "source": [
        "# ✅ Transpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmsEg15mJRLU"
      },
      "source": [
        "%%writefile transpose/transpose.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include \"../../utils/common.h\"\n",
        "\n",
        "/*\n",
        " * Various memory access pattern optimizations applied to a matrix transpose kernel.\n",
        " */\n",
        "\n",
        "#define BDIMX 16\n",
        "#define BDIMY 16\n",
        "\n",
        "void initialData(float *in,  const int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    in[i] = (float)( rand() & 0xFF ) / 10.0f; //100.0f;\n",
        "  return;\n",
        "}\n",
        "\n",
        "void printData(float *in,  const int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    printf(\"%dth element: %f\\n\", i, in[i]);\n",
        "  return;\n",
        "}\n",
        "\n",
        "void checkResult(float *hostRef, float *gpuRef, const int size, int showme) {\n",
        "    double epsilon = 1.0E-8;\n",
        "    bool match = 1;\n",
        "\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n",
        "            match = 0;\n",
        "            printf(\"different on %dth element: host %f gpu %f\\n\", i, hostRef[i],\n",
        "                    gpuRef[i]);\n",
        "            break;\n",
        "        }\n",
        "\n",
        "        if (showme && i > size / 2 && i < size / 2 + 5)\n",
        "          printf(\"%dth element: host %f gpu %f\\n\",i,hostRef[i],gpuRef[i]);\n",
        "    }\n",
        "\n",
        "    if (!match)  printf(\"Arrays do not match.\\n\\n\");\n",
        "}\n",
        "\n",
        "void transposeHost(float *out, float *in, const int nx, const int ny) {\n",
        "  for( int iy = 0; iy < ny; ++iy)\n",
        "    for( int ix = 0; ix < nx; ++ix)\n",
        "      out[ix * ny + iy] = in[iy * nx + ix];\n",
        "}\n",
        "\n",
        "__global__ void warmup(float *out, float *in, const int nx, const int ny)\n",
        "{\n",
        "    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
        "\n",
        "    if (ix < nx && iy < ny)\n",
        "    {\n",
        "        out[iy * nx + ix] = in[iy * nx + ix];\n",
        "    }\n",
        "}\n",
        "\n",
        "// case 0 copy kernel: access data in rows\n",
        "__global__ void copyRow(float *out, float *in, const int nx, const int ny)\n",
        "{\n",
        "    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
        "\n",
        "    if (ix < nx && iy < ny)\n",
        "    {\n",
        "        out[iy * nx + ix] = in[iy * nx + ix];\n",
        "    }\n",
        "}\n",
        "\n",
        "// case 1 copy kernel: access data in columns\n",
        "__global__ void copyCol(float *out, float *in, const int nx, const int ny)\n",
        "{\n",
        "    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
        "\n",
        "    if (ix < nx && iy < ny)\n",
        "    {\n",
        "        out[ix * ny + iy] = in[ix * ny + iy];\n",
        "    }\n",
        "}\n",
        "\n",
        "// case 2 transpose kernel: read in rows and write in columns\n",
        "__global__ void transposeNaiveRow(float *out, float *in, const int nx, const int ny) {\n",
        "    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
        "\n",
        "    if (ix < nx && iy < ny)\n",
        "        out[ix * ny + iy] = in[iy * nx + ix];\n",
        "}\n",
        "\n",
        "// case 3 transpose kernel: read in columns and write in rows\n",
        "__global__ void transposeNaiveCol(float *out, float *in, const int nx,\n",
        "                                  const int ny)\n",
        "{\n",
        "    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
        "\n",
        "    if (ix < nx && iy < ny)\n",
        "    {\n",
        "        out[iy * nx + ix] = in[ix * ny + iy];\n",
        "    }\n",
        "}\n",
        "\n",
        "// case 4 transpose kernel: read in rows and write in columns + unroll 4 blocks\n",
        "__global__ void transposeUnroll4Row(float *out, float *in, const int nx,\n",
        "                                    const int ny)\n",
        "{\n",
        "    unsigned int ix = blockDim.x * blockIdx.x * 4 + threadIdx.x;\n",
        "    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
        "\n",
        "    unsigned int ti = iy * nx + ix; // access in rows\n",
        "    unsigned int to = ix * ny + iy; // access in columns\n",
        "\n",
        "    if (ix + 3 * blockDim.x < nx && iy < ny)\n",
        "    {\n",
        "        out[to]                   = in[ti];\n",
        "        out[to + ny * blockDim.x]   = in[ti + blockDim.x];\n",
        "        out[to + ny * 2 * blockDim.x] = in[ti + 2 * blockDim.x];\n",
        "        out[to + ny * 3 * blockDim.x] = in[ti + 3 * blockDim.x];\n",
        "    }\n",
        "}\n",
        "\n",
        "// case 5 transpose kernel: read in columns and write in rows + unroll 4 blocks\n",
        "__global__ void transposeUnroll4Col(float *out, float *in, const int nx,\n",
        "                                    const int ny)\n",
        "{\n",
        "    unsigned int ix = blockDim.x * blockIdx.x * 4 + threadIdx.x;\n",
        "    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
        "\n",
        "    unsigned int ti = iy * nx + ix; // access in rows\n",
        "    unsigned int to = ix * ny + iy; // access in columns\n",
        "\n",
        "    if (ix + 3 * blockDim.x < nx && iy < ny)\n",
        "    {\n",
        "        out[ti]                = in[to];\n",
        "        out[ti +   blockDim.x] = in[to +   blockDim.x * ny];\n",
        "        out[ti + 2 * blockDim.x] = in[to + 2 * blockDim.x * ny];\n",
        "        out[ti + 3 * blockDim.x] = in[to + 3 * blockDim.x * ny];\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        " * case 6 :  transpose kernel: read in rows and write in colunms + diagonal\n",
        " * coordinate transform\n",
        " */\n",
        "__global__ void transposeDiagonalRow(float *out, float *in, const int nx,\n",
        "                                     const int ny)\n",
        "{\n",
        "    unsigned int blk_y = blockIdx.x;\n",
        "    unsigned int blk_x = (blockIdx.x + blockIdx.y) % gridDim.x;\n",
        "\n",
        "    unsigned int ix = blockDim.x * blk_x + threadIdx.x;\n",
        "    unsigned int iy = blockDim.y * blk_y + threadIdx.y;\n",
        "\n",
        "    if (ix < nx && iy < ny)\n",
        "    {\n",
        "        out[ix * ny + iy] = in[iy * nx + ix];\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        " * case 7 :  transpose kernel: read in columns and write in row + diagonal\n",
        " * coordinate transform.\n",
        " */\n",
        "__global__ void transposeDiagonalCol(float *out, float *in, const int nx,\n",
        "                                     const int ny)\n",
        "{\n",
        "    unsigned int blk_y = blockIdx.x;\n",
        "    unsigned int blk_x = (blockIdx.x + blockIdx.y) % gridDim.x;\n",
        "\n",
        "    unsigned int ix = blockDim.x * blk_x + threadIdx.x;\n",
        "    unsigned int iy = blockDim.y * blk_y + threadIdx.y;\n",
        "\n",
        "    if (ix < nx && iy < ny)\n",
        "    {\n",
        "        out[iy * nx + ix] = in[ix * ny + iy];\n",
        "    }\n",
        "}\n",
        "\n",
        "// main functions\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"%s starting transpose at \", argv[0]);\n",
        "    printf(\"device %d: %s \", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // set up array size 2048\n",
        "    int nx = 1 << 12;\n",
        "    int ny = 1 << 12;\n",
        "\n",
        "    // select a kernel and block size\n",
        "    int iKernel = 0;\n",
        "    int blockx = 16;\n",
        "    int blocky = 16;\n",
        "\n",
        "    if (argc > 1) iKernel = atoi(argv[1]);\n",
        "\n",
        "    if (argc > 2) blockx  = atoi(argv[2]);\n",
        "\n",
        "    if (argc > 3) blocky  = atoi(argv[3]);\n",
        "\n",
        "    if (argc > 4) nx  = atoi(argv[4]);\n",
        "\n",
        "    if (argc > 5) ny  = atoi(argv[5]);\n",
        "\n",
        "    printf(\" with matrix nx %d ny %d with kernel %d\\n\", nx, ny, iKernel);\n",
        "    size_t nBytes = nx * ny * sizeof(float);\n",
        "\n",
        "    // execution configuration\n",
        "    dim3 block (blockx, blocky);\n",
        "    dim3 grid  ((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n",
        "\n",
        "    // allocate host memory\n",
        "    float *h_A = (float *)malloc(nBytes);\n",
        "    float *hostRef = (float *)malloc(nBytes);\n",
        "    float *gpuRef  = (float *)malloc(nBytes);\n",
        "\n",
        "    // initialize host array\n",
        "    initialData(h_A, nx * ny);\n",
        "\n",
        "    // transpose at host side\n",
        "    transposeHost(hostRef, h_A, nx, ny);\n",
        "\n",
        "    // allocate device memory\n",
        "    float *d_A, *d_C;\n",
        "    CHECK(cudaMalloc((float**)&d_A, nBytes));\n",
        "    CHECK(cudaMalloc((float**)&d_C, nBytes));\n",
        "\n",
        "    // copy data from host to device\n",
        "    CHECK(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // warmup to avoide startup overhead\n",
        "    double iStart = seconds();\n",
        "    warmup<<<grid, block>>>(d_C, d_A, nx, ny);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    double iElaps = seconds() - iStart;\n",
        "    printf(\"warmup         elapsed %f sec\\n\", iElaps);\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // kernel pointer and descriptor\n",
        "    void (*kernel)(float *, float *, int, int);\n",
        "    const char *kernelName;\n",
        "\n",
        "    // set up kernel\n",
        "    switch (iKernel)\n",
        "    {\n",
        "    case 0:\n",
        "        kernel = &copyRow;\n",
        "        kernelName = \"CopyRow       \";\n",
        "        break;\n",
        "\n",
        "    case 1:\n",
        "        kernel = &copyCol;\n",
        "        kernelName = \"CopyCol       \";\n",
        "        break;\n",
        "\n",
        "    case 2:\n",
        "        kernel = &transposeNaiveRow;\n",
        "        kernelName = \"NaiveRow      \";\n",
        "        break;\n",
        "\n",
        "    case 3:\n",
        "        kernel = &transposeNaiveCol;\n",
        "        kernelName = \"NaiveCol      \";\n",
        "        break;\n",
        "\n",
        "    case 4:\n",
        "        kernel = &transposeUnroll4Row;\n",
        "        kernelName = \"Unroll4Row    \";\n",
        "        grid.x = (nx + block.x * 4 - 1) / (block.x * 4);\n",
        "        break;\n",
        "\n",
        "    case 5:\n",
        "        kernel = &transposeUnroll4Col;\n",
        "        kernelName = \"Unroll4Col    \";\n",
        "        grid.x = (nx + block.x * 4 - 1) / (block.x * 4);\n",
        "        break;\n",
        "\n",
        "    case 6:\n",
        "        kernel = &transposeDiagonalRow;\n",
        "        kernelName = \"DiagonalRow   \";\n",
        "        break;\n",
        "\n",
        "    case 7:\n",
        "        kernel = &transposeDiagonalCol;\n",
        "        kernelName = \"DiagonalCol   \";\n",
        "        break;\n",
        "    }\n",
        "\n",
        "    // run kernel\n",
        "    iStart = seconds();\n",
        "    kernel<<<grid, block>>>(d_C, d_A, nx, ny);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "\n",
        "    // calculate effective_bandwidth\n",
        "    float ibnd = 2 * nx * ny * sizeof(float) / 1e9 / iElaps;\n",
        "    printf(\"%s elapsed %f sec <<< grid (%d,%d) block (%d,%d)>>> effective \"\n",
        "           \"bandwidth %f GB\\n\", kernelName, iElaps, grid.x, grid.y, block.x,\n",
        "           block.y, ibnd);\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // check kernel results\n",
        "    if (iKernel > 1)\n",
        "    {\n",
        "        CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n",
        "        checkResult(hostRef, gpuRef, nx * ny, 1);\n",
        "    }\n",
        "\n",
        "    // free host and device memory\n",
        "    CHECK(cudaFree(d_A));\n",
        "    CHECK(cudaFree(d_C));\n",
        "    free(h_A);\n",
        "    free(hostRef);\n",
        "    free(gpuRef);\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "    return EXIT_SUCCESS;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQrHMFU_KXF5"
      },
      "source": [
        "# Compilazione ed esecuzione\n",
        "!nvcc -arch=sm_37  transpose/transpose.cu -o transp\n",
        "!./transp 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbefiJYuKk0X"
      },
      "source": [
        "!./transp 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTqFftynMeSd"
      },
      "source": [
        "!./transp 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiH6ehEyMffv"
      },
      "source": [
        "!./transp 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbVGCa8vMjZZ"
      },
      "source": [
        "!./transp 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wrYsvLeMpQD"
      },
      "source": [
        "!./transp 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A42KS45eMqNM"
      },
      "source": [
        "!./transp 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHumN5kGMtOx"
      },
      "source": [
        "!./transp 7"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}